What is weight decay?	L2 regularization on the weights.<br>\(E_j = E_{j-1} + \lambda \sum_i^n w_i^2\)
What is <b>bagging</b>?	Ensemble models trained on subsets with overlap of the dataset
What is the bias-variance tradeoff?	Balancing the current accuracy (bias) with the generalization power (variance)
What is the formula of the cross-entropy?	\(H(p, q) = - \sum_x p(x) \log q(x)\)
What is the formula of the <b>Kullback-Leibler</b> divergence?	\(D_{KL}(p, q) = - \sum_x p(x) \log \frac{q(x)}{p(x)}\)
What is the Kullback-Leibler divergence?	A measure of how one distribution diverges from a second, expected distribution.
What is the recall?	The % of positive elements detected<br>\(\text{recall} = \frac{TP}{P}\)
What is the precision?	The % of detected elements that are positive<br>\(\text{precision} = \frac{TP}{TP+FP}\)
What is the f1-score?	Harmonic average of the recall &amp; the precision
What is the harmonic average?	Average for rates<br><br>\(H = \frac{n}{\sum_{i=1}^n \frac{1}{x_i}}\)
What is the <b>Exponential Moving Average?</b>	Moving average applying a weight to the current element<br><br>\(S_t = \alpha \cdot Y_t + (1 - \alpha) \cdot S_{t-1}\)
What is the l1 distance?	\(\sum_{i=1}^n |x_i - y_i|\)
What is the l2 distance?	\((\sum_{i=1}^n |x_i - y_i|^2)^{\frac{1}{2}}\)
What improvements did the Fast-RCNN?	Removed the SVM to add RoI pooling &amp; fully connected layers
What improvements did the Faster-RCNN?	Replaced the Selective Search by a Region Proposal Network
What is the characteristic of <b>2-stage detection</b> algorithms?	They have a stage to classify objects between background &amp; foreground
What is the sigmoid formula?	\(\sigma(x) = \frac{1}{1 + e^{-x}}\)
What is the sigmoid derivative?	\(\sigma'(x) = \sigma(x)(1 - \sigma(x))\)
What is a Triplet Network?	A network computing a pair-wise distance between one sample and two labeled samples
In lowshots learning, what is <b>n-way</b>?	The number n of previously unseen classes
In lowshot learning, what is k-shots?	The number of classified samples per class
How does <b>Matching Network</b> encode the support set?	ConvNet on each sample, then Bi-LSTM of the whole set
What is a depthwise convolution?	"A convolution that maps one unique kernel per input channels<br><br><img src=""1LVyPaSrFFKkjmktzH1cYzg.png"">"
What is the computational cost of a depthwise convolution?	"\(D_f^2 \cdot M \cdot D_k^2\)<br><br><img src=""1LVyPaSrFFKkjmktzH1cYzg.png"">"
What is the computational cost of a convolution?	"\(D_f^2 * M * N * D_k^2\)<br><img src=""1ZbeQ1-KHpegpYbz7ODXZYw.png"">"
What is a pointwise convolution?	"Convolution with a 1x1 kernel that maps input channels with output channels<br><img src=""1KiirgK6_dilzs8-muumnIw.png"">"
What is the computational cost of a pointwise convolution?	"\(M * N * D_f^2\)<br><img src=""1KiirgK6_dilzs8-muumnIw.png"">"
What does <b>MobileNet</b> use?	Depthwise separable convolutions =<br>depthwise convolution +<br>pointwise convolution
What is MobileNet's <b>width multiplier</b>?	A factor between 0 and 1 to thin the number of channels at each convolution
What is a group convolution in ShuffleNet?	"Several convolutions each taking as input a portion of the input channels<br><br><img src=""1Du6kevcjT2Gj1w55xYFymQ.png"">"
What is the <b>computational cost</b> of a group convolution in ShuffleNet?	\((D_f^2 * D_k^2 * \frac{M}{G} * \frac{N}{G}) * G\)
What does ShuffleNet use?	"<div>Group convolution + channel shuffle + Depthwise convolution. And residual branch</div><div><br></div><img src=""paste-4e381d07a2ba7ae73d35001e4736329bd4fe8db6.jpg"">"
What is InfoGAN?	A GAN that disentangle the representation so we can modify the latent vector to produce wanted modification on the generated image
What is a Feature Pyramid Network?	"Network that feature maps at multiple scales<br><br><img src=""1Q1cHeGjHpe73wdlUsb1sjA.png"">"
In a LSTM unit, what does the <b>forget gate</b> do?	"It multiplies (i.e. ~forget) the cell state by a sigmoid result<br><br><img src=""paste-a1032ae525544a1158728fe0e335b752a4bec3b6.jpg"">"
In a LSTM unit, what does the <b>input gate</b> do?	"A sigmoid decides which values to update &amp; a tanh to create new candidate values to add to the cell state<br><br><img src=""paste-aac2366c228704c4950de3f9eca514c86a89d135.jpg"">"
In a LSTM unit, what does the <b>output gate</b> do?	"Apply a sigmoid on the concatenation of the input and the hidden state, to select the tanh result of the cell state<br><br><img src=""paste-70fd315b145080c056f4d41a3d06b34049c13bc2.jpg"">"
In a LSTM unit, what are the <b>three gates</b>?	<b>Forget</b> gate, <b>input</b> gate, and <b>output</b> gate
In a LSTM unit, what are the <b>peephole connections</b>?	"Connections between the cell state and the gates<br><br><img src=""paste-7b0eeb4119b89a37728fd45a9280ff7d88abc52b.jpg"">"
How does <b>GRU</b> change the LSTM unit?	"It combines the <b>forget</b> &amp; <b>input</b> gate<br>It merges the <b>hidden</b> &amp; <b>cell</b> state<br><br><img src=""paste-79044a80806580c083895abc07adb7b690f16474.jpg"">"
What is a function K <b>Lipschitz continuous</b>?	"\(|f(x_1) - f(x_2)| \le K |x_1 - x_2|\)<br><br><img src=""paste-eb38e1438c0a7b73cd6dbf06f937f8134b24579e.jpg"">"
On what is applied the <b>batch norm</b>?	"On the <b>whole batch</b> for <b>one channel<br><br></b><img src=""paste-460c627729ddb05567780752cd19d92dc77ed1f4.jpg""><b><br></b>"
On what is applied the <b>layer norm</b>?	"On <b>one image</b> for <b>all channels<br><br></b><img src=""paste-07a3998c9ce59d56fb48f28b65cde17062e226dc.jpg""><b><br></b>"
On what is applied the <b>instance norm</b>?	"On <b>one image</b> for <b>one channel<br><br></b><img src=""paste-213fc2bbea421b98885e3626b265749cbc89e5d8.jpg""><b><br></b>"
On what is applied the <b>group norm</b>?	"On <b>one image</b> for a <b>group of channels<br><br></b><img src=""paste-6d56d614bde3536fb658bc1d6e2893e9162b72fb.jpg""><b><br></b>"
What is the goal of the <b>focal loss</b>?	To give less importance to well classified examples
What is the formula of the <b>focal loss</b>?	\(\text{FL}(p_t) = - (1 - p_t)^\gamma \log p_t\)
What is the formula of softmax?	\(\sigma_j(z) = \frac{e^{z_j}}{\sum_{k=1}^K e_{z_k}}\)
What is <b>hard negative mining</b>?	Explicit sampling of hard examples<br><br>These can be previous false positives
Worst case time complexity of <b>Quicksort</b>?	\(\mathcal{O}(n^2)\)
Worst case time complexity of <b>Merge Sort</b>?	\(\mathcal{O}(n \log n)\)
Worst case time complexity of <b>Bubble Sort</b>?	\(\mathcal{O}(n^2)\)
Worst case time complexity of <b>Heapsort</b>?	\(\mathcal{O}(n \log n)\)
Worst case time complexity of <b>Insertion Sort</b>?	\(\mathcal{O}(n^2)\)
What is <b>Bayes' rule</b>?	\(P(A | B) = \frac{P(B | A) P(A)}{P(B)}\)
<div>What is a matrix product?<br></div>	"A line with a column:<br><img src=""paste-cea1b10ec744705a0fa2d839148e6482a0f41e6f.jpg"">"
What is the <b>variance </b>formula?	\(\sigma^2 = \frac{\sum_{i=1}^n (x_i - \mu)^2}{n}\)
What is <b>dropout</b>?	Randomly drop neurons during training<br><br>Avoid overfitting<br><br>Can be seen as an ensemble
What does a NALU do?	Multiplication, division, &amp; power
What is RoI pooling?	"The Region of Interest pooling extracts a box from the feature maps &amp; standardizes its shape<br><br><img src=""roi_pooling.svg"">&nbsp;<div><br></div><div><img src=""roi_pooling2.svg""><br></div>"
What is a <b>Region Proposal Network </b>(RPN)?	"A mini-network:<div>- applying anchors of different sizes &amp; ratios<br>- classifying between background &amp; foreground<br>- regressing deltas for the bboxes<br><br><img src=""paste-28856b63168cfa84462216b8da11cb01c6829d0d.jpg""></div>"
CBOW predicts the ... based on the ...	CBOW predicts the <b>center word</b> based on the <b>context</b>
What is the Switchable Normalization?	Normalization where the mean \(\mu\) and the variance \(\sigma^2\) are a weighted linear combination between the stats of:<br>- Instance norm<br>- Layer norm<br>- Batch norm
What is the <b>Instance Norm</b> used for?	Normalize the contrast of the content image in style transfer
What is the layer norm used for?	Normalize the input of each RNN's cell independently
Why <b>Group Normalization</b> normalizes channels per group?	It is assumed that channels specialize in tasks (frequency, shapes, illumination, textures). One group's channels would have the same task
What is <b>weight normalization</b> formula?	<div>\(y = \phi(W \cdot x + b)\)<br></div><div>with:</div><div>\(W = \frac{g}{\Vert V \Vert}V\)<br></div>
What is <b>cosine normalization</b> formula?	Replacing the dot product by the cosine similarity:<div><br></div><div>\(y = \phi(\frac{W \cdot X}{\Vert W \Vert \Vert X \Vert})\)</div>
What is DenseNet?	"A CNN where each block has residuals to all subsequent layers<div><br></div><div><img src=""paste-fb7d96bd41c3fd227fc20c14661df7eb766af98f.jpg""><br></div>"
What is the meta-learning algo <b>Reptile</b>?	1. Starts with parameter \(W_1\)<div>2. Train on task \(T_1\) to get new weights \(W_2\)</div><div>3. \(T_1\) Gradient is interpolation between \(W_1\) and \(W_2\)</div>
What is the formula of a <b>neuron</b> from a fully connected layer?	"\(\sum_i^n w_i x_i + b = w \cdot x + b = w^Tx + b\)<br><br><img src=""paste-c64521dae7877c8160468a630e0914bc0099d724.jpg"">"
What is the <b>Chain Rule</b> formula?	\(\frac{d}{dx} f(g(x)) = \frac{df(g(x))}{dg(x)} \frac{dg(x)}{dx}\)<br><br>\(\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}\)
What is a <b>gradient</b> of a function?	The vector of its partial derivatives
What is the <b>Jacobian</b> matrix?	"<div>The collection of \(m\) x \(n\) possible partial derivates, which is the stack of \(m\) gradients:<br><br><img src=""paste-07ac55fbdfb0aa3c393eb93446ebe82b8e110009.jpg""><br></div><div><br></div>"
What is the <b>Hadamard product</b>?	The element-wise multiplication \(\odot\)
"What is the ""<i>single-variable total-derivative chain rule</i>""?"	The total derivative assumes that potentially all variables are codependent:<br><br>\(\frac{\partial f(u_1, ..., u_n)}{\partial x} = \sum_i^n \frac{\partial f}{\partial u_i} \frac{\partial u_i}{\partial x}\)
What is <b>gradient checkpointing</b>?	"Saving node value during forward only at some checkpoints. Recompute the non-saved node value for backprop.<div><br></div><div><img src=""0VEYowymIqvNc2HzB._""><br><br><img src=""0s7U1QDfSXuVd1LrF._""><br></div>"
What is the difference between <b>batch gradient descent</b> &amp; <b>stochastic gradient descent</b>?	Batch: SG on the whole dataset<br>Stochastic: SG on a single sample<br><br>Minibatch training is between those two
What is the <b>standard error of the mean</b>?	\(\sigma^{-}_x = \frac{\sigma}{\sqrt{n}}\)
What is the advantages of <b>large batches</b>?	- More accurate estimation of the mean of the gradient (yet with less than linear returns): \(\text{std error mean} = \frac{\sigma}{\sqrt{n}}\)<br>- Take more advantages of the parallelized operations
What is the advantages of <b>small batches</b>?	- Fit in memory<div>- Add a regularizing effect due to the noise</div>
What is the <b>Hessian</b> matrix?	"The matrix of the second partial derivates of \(f\):<div><br></div><div><img src=""paste-7370f1f84047862606bb69f1a9c765867ac4ac98.jpg""><br></div><div><br></div><div>Can be seen as the Jacobian of \(\nabla f(x)\)</div>"
What is the <b>ill-conditioning</b> of the Hessian?	When the condition number of the Hessian is high, the function' second derivatives are very dissimilar while the step size is constant for all.<br>It slows down the training.<div><br></div><div>The Hessian is ill-conditioned on a saddle point.</div>
What is the <b>SGD</b> update rule?	\(\theta_i = \theta_{i-1} - \eta \nabla L(f(x; \theta), y)\)
What is the momentum in SGD?	We add the weighted value of the previous gradients to the current gradient:<div><br></div><div>\(g_i = \alpha g_{i-1} + \eta \nabla L(f(x; \theta), y)\)<br></div><div>\(\theta_i = \theta_{i-1} - g\)<br></div>
What is the <b>Nesterov momentum</b>?	Like the momentum SGD but with the weights updated with the velocity before the evaluation of the current gradient:<br><br>&nbsp;<div><div>\(g_i = \alpha g_{i-1} + \eta \nabla L(f(x; \theta - \alpha g_{i-1}), y)\)<br></div><div>\(\theta_i = \theta_{i-1} - g\)</div></div>
What is the RMSProp algorithm?	A modified version of SGD where the gradient is weighted by the inverse of a EMA of the squared gradient:<br><br><div>\(r_i = \alpha r_{i-1} + (1 - \alpha)g \odot g\)<br></div><div>\(\theta_i = \theta_{i-1} - \frac{\eta}{\sqrt{\epsilon + r}} g\)</div><div><br></div>
What is the <b>Adam</b> algorithm?	A SGD where the learning rate is modulated by an estimation of the first &amp; second moment:<div><br></div><div>\(s_i = \alpha_1 s_{i-1} + (1 - \alpha_1)g\)<br></div><div><div>\(r_i = \alpha_2 r_{i-1} + (1 - \alpha_2)g \odot g\)</div><div><br></div>\(\hat{s_i} = \frac{s_i}{1 - \alpha_1^t}\)</div><div>\(\hat{r_i} = \frac{r_i}{1 - \alpha_2^t}\)</div><div><div><br></div><div>\(\theta_i = \theta_{i-1} - \eta \frac{\hat{s_i}}{\sqrt{\hat{r_i}}\epsilon} g\)</div></div>
In C, how to get the address of a variable?	&amp;x
In C, what is dereferencing a pointer?	Accessing the value stored in the memory location pointed by the pointer
In C, how to dereference a pointer?	*ptr
What is the <b>constrastive loss</b> formula?	\(L = (1 - Y) D^2 + Y (\max (0, m - D))^2\)<br><br>With \(Y = 1\) when images are dissimilar, \(0\) else.
What is the <b>Triplet Loss</b> formula?	\(L = \max(D(X, X^+) - D(X, X^-) + m, 0)\)
What is the <b>Triplet Ranking Loss</b> formula?	\(L = [m + D(X, X^+) - D(X, X^-)]_+\)<div>\(+ [m + D(X, X^+) - D(X^+, X^-)]_+\)</div>
What is <b>view-manifold learning</b>?	<div>Lowshot learning for multiple points of view:</div><div><br></div>- Sample same objects from different points of view + one different object.<br>- Embed them with a convnet.<br>- Pairwise distance with a cosine distance.<br>- Triplet loss.
What is a <b>Bi-RNN</b>?	RNN is applied both from the start of the sequence to its end, and in the opposite direction.<br>At each time step both forward &amp; backward outputs are concatenated.<br><br>Useful to gain context (step before &amp; after the current step).
What is <b>Prototypical Network</b>?	"Lowshot model that creates <i>prototype, </i>i.e. mean of the embedding of supports of the same class.<div><br></div><div>Inference is done by a softmax of the negated distance between the test image &amp; each prototype.</div><div><br></div><div><img src=""Screen Shot 2019-07-21 at 15.26.10.png""><br></div>"
"What is the <b>Siamese Network</b> used by (<i>Koch et al, 2015</i>) in ""<u>Siamese Networks for One-shot Image Recognition</u>"" ?"	Abs difference between 1024 embeddings of the images, then FC layer with a sigmoid. Optimizes the binary cross-entropy.
\(P(\cup_{i=1}^n E_i) =\) ...	\(P(\cup_{i=1}^n E_i) = \sum_{i=1}^n P(E_i)\)
What is a <b>combination</b>?	It's an arrangement of k objects from a pool of n objects where the <b>order does not matter</b><div><b><br></b></div><div><b><br></b></div>
What's the <b>permutation</b> formula of \(r\) objects from a pool of \(n\) objects?	\(P(n, r) = \frac{n!}{(n - r)!}\)<div><br></div><div>The order matter.</div>
What's the <b>combination</b> formula of \(r\) objects from a pool of \(n\) objects?	\(C(n, r) = \frac{P(n, r)}{r!} = \frac{n!}{r!(n - r)!}\)<div><br></div><div>The order doesn't matter.</div>
When two events are <b>independent</b>?	\(P(A \cap B) = P(A)P(B)\)
What is the <b>Chebyhev's inequality</b> formula?	\(P(|X-\mu| \ge k\sigma) \le \frac{1}{k^2}\)
What is a probability density function (<b>PDF</b>)?	The probability that a random continuous variable falls into a particular range of values
What is a cumulative distribution function (<b>CDF</b>)?	The probability that a random continuous variable X takes a value less than or equal to x
What is the <b>Central Limit Theorem</b>?	Given a sample size \(n \to \infty\) of a distribution with a mean \(\mu\) and a variance \(\sigma^2\), it approximates a normal distribution of mean \(\mu\) and variance \(\frac{\sigma}{\sqrt n}\)
What is the <b>null hypothesis</b>?	The default position that there is no relationship between two measured phenomena
What is the <b>p-value</b>?	The probability that, if the null-hypothesis is true, the expected events are verified.<div><br></div><div>If it's too low, we consider that the null-hypothesis is not significant</div>
How to find the <b>eigenvalues</b> of a matrix?	Finding all \(\lambda\) that satisfy the equation:<div><br></div><div>\(det(A - \lambda I) = 0\)</div>
How to find the <b>eigenvectors</b> of a matrix?	For each eigenvalue \(\lambda_i\), resolve the linear system:<div><br></div><div>\((A - \lambda_i I)x = 0\)</div>
What is the <b>adjugate matrix</b> <i>(fr: comatrix)</i>?	"The transpose of the cofactor matrix:<div><img src=""paste-c6fc7644dd87f489aa8f50b88337bc5b1f979993.jpg""><br></div><div><br></div><div><img src=""paste-cd6548f7b0aaabfc865eae5a85d729c1089fbe0e.jpg""><br></div><div><img src=""paste-c7e605cabcca3d266e46c659631e12da34a3eb35.jpg""><br></div>"
What does the <b>determinant</b> of a matrix represent?	The <b>scaling factor</b> of the linear transformation described by the matrix
How to find the <b>determinant</b> of a 3x3 square matrix?	"<img src=""paste-e57804dc37ccb872ba5ce2c7ed3dd1d3259dd8e5.jpg"">"
What are the steps of the <b>PCA</b>?	- substract mean<div>- Compute covariance matrix</div><div>- Calculate eigen{values/vectors} of the covariance matrix</div><div>- Normalize to unit vector the eigenvectors</div><div>- Keep the \(k\) eigenvectors that correspond to the \(k\) largest eigenvalues</div><div>- Concatenate as several columns in a matrix the chosen eigenvectors (<i>that produces the projection matrix&nbsp;</i>\(W\))</div><div>- Transform the data \(X\): \(Y = X &nbsp;W\)</div>
What is the <b>covariance</b> <b>matrix</b> formula?	\(\Sigma = \frac{1}{n - 1} ((X - \mu)^T(X - \mu))\)
What are the 4 conditions of a <b>metric</b>?	\(d(x, y) \ge 0\)<div>\(d(x, y) = 0 \iff x = y\)</div><div>\(d(x, y) = d(y, x)\)</div><div>\(d(x, y) \le d(x, z) + d(z, y)\)</div>
What is the <b>entropy formula</b>?	\(H(X) = - \Sigma_{i=1}^n P(x_i) \log P(x_i)\)
What does the <b>entropy</b> represents?	The amount of randomness / disorder
What is <b>curriculum learning</b>?	Feeding easy examples at first, and augment the proportion of difficult examples through time&nbsp;
What is the formula of <b>RNN</b>?	\(h^{(t)} = tanh(b_1 + W h^{(t - 1)} + U x^{(t)})\)<div><br></div><div>\(o^{(t)} = b_2 + V h^{(t)}\)</div>
What is a <b>permutation</b>?	It's an arrangement of k objects from a pool of n objects where the <b>order matter</b>
What is <b>teacher forcing</b>?	Recurrent models where the output of one time step is fed to next time step<div><br></div><div>Used in language modeling for example&nbsp;</div>
What is <b>sequence-to-sequence</b> (Seq2Seq)?	Model to convert seq from one domain (ex. english) &nbsp;to another (ex. french).<div><br></div><div>Uses a <b>RNN encoder</b> &amp; a <b>RNN decoder</b>.</div><div>The RNN decoder can use teacher forcing.</div>
What is <b>data parallelism</b>?	"When several machines run <b>a different input example</b> in parallel<div><br></div><div><img src=""paste-ad33ba32eafe8a70e8bc5b5c5ae16b5378814e00.jpg""><br></div>"
What is <b>model parallelism</b>?	"When for a same input example, several machines run <b>different parts of the model</b><div><b><br></b></div><div>Like AlexNet did with group convolution over two machines</div><div><br></div><div><img src=""paste-4935756d4da6b1641de5a7a44f9a4b1804a7d9ff.jpg""><br></div>"
What is a <b>Neural Turing Machine</b>?	"A <b>differentiable</b> Turing Machine that can <b>read &amp; write</b>&nbsp;with a <b>controller</b> to an <b>external memory</b><div><b><br></b></div><div><img src=""paste-87fcf0fda8d3ad50b6be38664e1a8c46d37cab30.jpg""><b><br></b></div>"
What is the <b>read formula</b> in a Neural Turing Machine?	\(M_t \in I\!R^{N \times M}\) the <b>memory matrix</b>, and \(w_t\) a <b>vector of weightings</b> over the \(N\) locations emitted by a read head at time \(t\).<div><br></div><div>\(r_t \leftarrow \Sigma_i w_t(i)M_t(i)\)</div>
What are the <b>two parts of a write operation</b> in a Neural Turing Machine?	<b>Erase</b> operation followed by an <b>add</b> operation
What is the <b>erase formula</b> in a <b>write operation</b> of a Neural Turing Machine?	\(M_t \in I\!R^{N \times M}\) &nbsp;the <b>memory matrix</b>, \(w_t \in I\!R^{N}\) the <b>vector of weightings</b>, and \(e_t \in I\!R^{M}\) the <b>erase vector</b> whose elements lie in the range (0, 1).<div><br></div><div>\(\tilde{M_t}(i) \leftarrow M_{t - 1}(i)[\pmb{1} - w_t(i)e_t] \)</div>
When do the <b>elements of a memory location are reset to zero</b> in an<b> erase</b> operation of a Neural Turing Machine?	<div>When both the <b>weighting at the location</b> and the <b>erase element</b> are one.</div><br><div>\(\tilde{M_t}(i) \leftarrow M_{t - 1}(i)[\pmb{1} - w_t(i)e_t] \)<br></div>
What is the <b>add formula</b> in a <b>write operation</b> in a Neural Turing Machine?	\(M_t \in I\!R^{N \times M}\) &nbsp;the&nbsp;<b>memory matrix</b>, \(w_t \in I\!R^{N}\) the&nbsp;<b>vector of weightings</b>, and \(a_t \in I\!R^{M}\) the&nbsp;<b>add vector</b>.<div><br></div><div>\(M_t(i) \leftarrow \tilde{M_t}(i) + w_t(i)a_t\)</div>
What are the <b>four steps of the addressing mechanism</b> in a Neural Turing Machine?<div><br></div><div><span class=cloze>[]</span></div><div>Interpolation</div><div>Convolutional shift</div><div>Sharpening<br></div>	"What are the <b>four steps of the addressing mechanism</b> in a Neural Turing Machine?<div><br></div><div><span class=cloze>Content addressing</span></div><div>Interpolation</div><div>Convolutional shift</div><div>Sharpening<br></div><br><br> <div><img src=""paste-536dc95442abedc4ae1c024eab7c4784a128c298.jpg""><br></div>"
What is the <b>content-base addressing</b> in a Neural Turing Machine?	It focuses attention on locations based on the <b>cosine similarity</b> between their current values &amp; values emitted by the controller.<div><br></div><div>\(M_t \in I\!R^{N \times M}\) &nbsp;the&nbsp;<b>memory matrix</b>, \(k_t \in I\!R^M\) the <b>key vector</b>, \(\beta_t\) the <b>key strenght </b>which modulate the precision of the focus and \(K\) the <b>cosine similarity</b>:<br></div><div><br></div><div>\(w_t^c(i) \leftarrow \frac{\exp(\beta_t K[k_t, M_t(i)])}{\Sigma_j \exp(\beta_t K[k_t, M_t(j)])}\)<br></div>
What is the <b>controller</b> in a Neural Turing Machine?	A <b>LSTM</b> or a <b>feedforward network</b>.<div><br></div><div>It interacts with the <b>external memory</b> using <b>read &amp; write head</b>.</div>
What is <b>Stochastic Weight Averaging</b> (SWA)?	<b>Average of the weights</b> every time the <b>cyclical learning rate</b> is at its minimum<div><br></div><div>\(w_{\text{SWA}} \leftarrow \frac{w_{\text{SWA}} \cdot t + w}{t + 1}\)</div>
What is the <b>LR Range Test</b>?	Method to <b>find optimal range of learning rates</b>:<div><br></div><div>- Start with small learning rates</div><div>- Increase it at each iterations for one epoch</div><div>- Record LR &amp; loss</div><div>- Upper bound of the learning rate is when loss was the lowest</div><div>- Lower bound is 0.25 or 0.75 of the upper bound</div>
What is <b>Born Again Neural Networks</b>?	"A <b>ensembling</b> &amp; <b>distillation</b> method, where the student learns from the teacher logits, then the student became a teacher for a new student.<div><br></div><div>Finally a vote is done between all students:<br><div><br></div><div><img src=""paste-3d47348b269db2f4e3318e478d38174072516449.jpg""><br></div></div>"
"What is the architecture of ""<u>Show and Tell: A Neural Image Caption Generato</u>r""?"	A <b>CNN</b> to embed the image followed by an <b>LSTM</b> producing the description.
What is <b>Beam Search</b> in Language Modeling?	We iteratively consider the set of the \(k\) best sentences up to a time \(t\) as candidates to generate sentences of of size \(t + 1\).
What is the message of the <b>sampling theorem</b> in the case of neural networks?	You should always blur (<b>pooling</b>) before subsampling (<b>stride</b>).
What is the <b>architecture</b> of the original <b>VQA</b>?	<b>CNN</b> to embed the image, <b>Word embeding</b> + <b>LSTM</b> for the question, <b>point-wise multiplication</b> between the two vectors &amp; fully connected classifier.
What is a <b>U-Net</b>?	"A <b>CNN</b> architecture used for <b>segmentation </b>where the image is downsampled to a bottleneck then upsampled to the original size producing a segmentation <b>mask</b>. There are <b>shortcut</b>&nbsp;connections between the downsampling &amp; upsampling phases.<div><br></div><div><img src=""paste-6c656fd79462b06f4549a14a2151ce8886e7e436.jpg""><br></div>"
"What is the <b>architecture</b> of <b>FashionNet</b> from the paper ""<u>DeepFashion</u>""?"	"<img src=""paste-ff7d92c11441939e1061e75b08a75550e62d059d.jpg""><br><div><img src=""paste-45bc05a253155315f36c65c4009fb4f92890a32d.jpg""><br></div>"
"What is ""<u>Distilling the Knowledge in a Neural Network</u>"" by (<i>Hinton et al, 2015</i>)?"	Distill a large trained model into a smaller model.<div><br></div><div>The smaller model try to <b>match the logits</b> of the bigger model with a <b>mean squared error</b>.</div>
What does <b>FreezeOut</b> do?	<b>Freeze gradually the layers</b>, starting from the firsts.<div><br></div><div>The reasonning is that the first layers quickly converge.</div>
What is <b>Snapshot Ensembles</b>?	<b>Save on disk the snapshot</b> of the model at the lowest point of the <b>cyclical learning rate</b> schedule. Then vote between all snapshots.<div><br></div><div>The reasonning is that a the lowest learning rate, the model falls in a local minimum, and an <b>ensemble of local minima is better</b>.</div>
What is the architecture of <b>Mask R-CNN</b>?	"At first like <b>Faster R-CNN</b>, with <b>CNN</b> and <b>RPN</b>.<div><br></div><div>Then add a <b>RoI Align</b>, followed by a <b>FCN</b> network to produce all the masks.</div><div><br></div><div><img src=""paste-e67d4f320eb10c49ea0555e1db92aa19bc797677.jpg""><br></div>"
How <b>RoI Align</b> is different from <b>RoI Pooling</b>?	"RoI Pooling is discrete, and samples the RoI in not equal surfaces.<div><br></div><div>RoI Align is <b>continuous</b> by doing a <b>bilinear interpolation</b>.</div><div><br></div><div><img src=""paste-073a94d0f71fe6f76cfeb452578400c99a4fc459.jpg""><br></div>"
What is a <b>Parametrized ReLU</b>?	Like a <b>Leaky ReLU</b> but with a <b>learned scalar </b>\(\alpha\):<div><br></div><div>\(\begin{cases} x_i &amp; \text{if}\ x_i \ge 0\\ \alpha x_i &amp; otherwise \end{cases}\)</div>
What is the architecture of <b>Stacked Attention Networks</b> (SAN)?	<b>CNN</b> to embed the image, <b>LSTM</b> to embed the question.<div><br></div><div>The image vector is formated so to be a matrix \(v_I \in I\!R^{d \times m}&nbsp;\) with the \(d\) the channels and \(m\) the spatial dimension.</div><div><br></div><div>We <b>refine iteratively</b> the image matrix \(v_I\) with the <b>attention map</b> \(p_I\):</div><div><br></div>\(h = \text{tanh}(W_I v_I + (W_Q v_Q + b_Q))\)<div>\(p_I = \text{softmax}(W_P h + b_P)\)</div><div>\(\tilde{v_I} = p_I + v_I\)</div><div><br></div><div>Finally we <b>combine the refined image matrix &amp; the question matrix with an addition</b> followed by a FC + softmax.</div>
What is the goal of <b>Histogram Loss</b>?	"Making the distributions of the similarities of the positive and negative pairs <b>less overlapping</b>.<div><br></div><div><img src=""Screen Shot 2019-09-21 at 11.47.24.png""><br></div>"
How does the <b>Histogram Loss</b> work?	"<b>Compute the histograms</b>&nbsp;of similarity of postive and negative pairs. The similarity function is the <b>negative cosine similarity</b>.<div><br><div>Estimate the probability of the similarity in a random negative pair to be more than the similarity in a random positive pair, aka the <b>probability of reverse</b>.</div></div><div><br></div><div>That can be approximated by:</div><div><br></div><div>\(L(X, \theta) = \sum_{r = 1}^R (h_r^{-} \sum_{q = 1}^r h_q^{+})\)</div><div><br></div><div>With \(\sum_{q = 1}^r h_q^{+}\) being the <b>CDF</b>&nbsp;up to \(r\).</div><div><br></div><div><img src=""Screen Shot 2019-09-21 at 11.46.52.png""><br></div><div><img src=""Screen Shot 2019-09-21 at 11.47.24.png""><br></div>"
What distance is used before the <b>Histogram Loss</b>?	Negative cosine distance
Why Triple-based losses are <b>less constrained </b>than pairwise losses?	Because the characteristic distance separating positive &amp; negative pairs <b>can vary across the embedding space</b>, i.e. depending on the location of \(x_0\).
\(P(a \le X \le b) =\) ... ? Using the PDF \(f\).	\(P(a \le X \le b) = \int_a^b f(x)dx\)
What is the<b> formula of the CDF</b> using the PDF \(f\)?	\(F(x) = \int_{-\infty}^x f(u) du\)
\(P(a \le X \le b) =\) ... ? Using the CDF \(F\).	\(P(a \le X \le b) = F(b) - F(a)\)
What is the <b>Empirical CDF</b> formula?	"<img src=""paste-c92847793b0c974a9355ec2d56b1ee4f35c91ff7.jpg"">"
What is the formula of the <b>Linear Regression</b>?	\(y = W \cdot X + b = \sum_{i=0}^n W_i X_i + b= W^T X + b \)
What is the Moore-Penrose <b>pseudo-inverse formula</b>?	\(A^{+} = (A^T A)^{-1} A^T\)
What is a <b>complete graph</b>?	A graph where <b>each pair of vertices is joined by an edge</b>.
What is a <b>connected graph</b>?	A graph where there is a <b>path in at least one direction between connecting every vertices </b>together.<b>&nbsp;</b>
What is a <b>strongly connected graph</b>?	A directed graph where there is a <b>path in both directions connecting all vertices </b>together.
What is a <b>vertice degree</b>&nbsp;in graph theory?	The <b>number of neighbours</b> of the vertice.
When is a graph <b>acyclic</b>?	When it has <b>no cycle</b>.
In a program memory, <b>where are disposed the .data, .text, .bss, stack, &amp; heap</b>?	"<img src=""page1-149px-Program_memory_layout.pdf.jpg"">"
What is stored in the <b>.bss</b> of a program memory?	The <b>uninitialized data</b>.
What is stored in the <b>.rodata</b>&nbsp;of a program memory?	The <b>constant data</b>.
What is stored in the <b>.data</b>&nbsp;of a program memory?	The <b>Global &amp; Static data</b>.
What is stored in the <b>.text </b>of a program memory?	The <b>executable instructions </b>of the program.
What is the formula of the <b>Pearson Correlation Coefficient</b>?	\(\rho_{x,y} = \frac{cov(x, y)}{\sigma_x \sigma_y}\)<div><br></div><div>With \(cov(x, y)\) the covariance.</div>
"What are the two contributions of ""<u>Dynamic Few-Short Visual Learning without Forgetting</u>"" (<i>Gidaris &amp; Komodakis, 2018</i>)?"	<b>Cosine similarity </b>between feature representations.<div><br></div><div>Attention-based few-shot <b>classification weight generator</b>.</div>
"What is the <b>cosine-similarity based classifier</b>&nbsp;in ""<u>Dynamic Few-Shot Visual Learning without Forgetting</u>"" (<i>Gidaris &amp; Komodakis, 2018</i>)?"	<b>L2 normalization </b>of the features &amp; the classifier weights:<div><br></div><div>\(o_k = \alpha \cdot \frac{Z_k}{\Vert Z_k \Vert }\frac{W_k}{\Vert W_k \Vert}\)</div><div><br></div><div>Followed by a <b>softmax</b>.</div><div><br></div><div>It's like the <b>cosine normalization</b>.</div>
"What is the <b>last activation of the convnet</b> in ""<u>Dynamic Few-Short Visual Learning without Forgetting</u>"" (<i>Gidaris &amp; Komodakis, 2018</i>)?"	None (aka <b>linear activation</b>) to allow the features vector to take positive &amp; negative values.
"What is the <b>classification weight generator </b>in ""<u>Dynamic Few-Short Visual Learning without Forgetting</u>"" (<i>Gidaris &amp; Komodakis, 2018</i>)?"	A method to generate weight of a classifier neuron to recognize a novel class.<div><br></div><div>\(w_{avg} = \frac{1}{N} \sum_{i=1}^N z_i\)</div><div>\(w_{att} = \frac{1}{N} \sum_{i=1}^N \sum_{b=1}^{K_{base}} \text{Att}(\phi_q z_i, k_b) \cdot w_b\)</div><div><br></div><div>\(k_b\) being a vector of the same dimension as \(w_b\) and \(\phi_*\) used for <i>indexing the memory</i>.</div><div><br></div><div>\(w = \phi_{avg} \odot w_{avg} + \phi_{att} \odot w_{att}\)</div>
"What are the <b>contributions</b> of ""<u>Low-Shot Learning from Imaginary Data</u>"" (<i>Wang et al., 2018)</i>?"	<b>Prototype matching networks</b>, a combination of Protypical Network &amp; Matching Network.<div><br></div><div><b>Hallucinator</b> / GAN to add new supports elements.</div><div><br></div><div>A <b>prior </b>between base classes &amp; novel classes.</div>
"What is the <b>hallucinator</b>&nbsp;in ""<u>Low-Shot Learning from Imaginary Data</u>"" (<i>Wang et al., 2018)?</i>"	A three layers <b>MLP</b> with ReLU: \(x' = G(x, z; W_G)\) with \(z\) a random vector.<div><br></div><div>The weights are initialized as <b>block diagonal identity matrices</b>.</div>
"What is the <b>prior</b>&nbsp;introduced in ""<u>Low-Shot Learning from Imaginary Data</u>"" (<i>Wang et al., 2018)?</i>"	"A <b>prior </b>\(\mu\)<b> between base &amp; novel classes</b>.<div><br></div><div><img src=""paste-a1c6b8ec526ff14e9d7b18b89fa798b71084d30a.jpg""><br></div>"
What is <b>Prototype Matching Network</b>?	A <b>combination</b> between Protypical Network &amp; Matching Network.<div><br></div><div>Images of the same class are <b>collapsed into their mean </b>then fed as usual to the <b>Bi-LSTM </b>of the Matching Network.</div>
What are the <b>advantages </b>of Prototype Matching Network over Matching Network?	By <b>collapsing to the mean </b>several images of a same class, it avoid to be swamped by <b>common classes</b>&nbsp;while seeing very few <b>rare classes</b>.
What is a <b>block diagonal matrix</b>?	"<img src=""paste-a179a3c3618def264ed05ac99caf46fc526dd947.jpg""><br><div>\(A\) being a <b>square matrix</b>&nbsp;and every \(A_k\) are also square matrices embedded into A.</div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Change user email to &lt;dummy&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git config --global user.email dummy </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Display the git configuration"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git config --list </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Initialize git repo"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git init </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Start tracking all &lt;.py&gt; files"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git add *.py </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Commit with a message &lt;msg&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git commit -m <span style=""color: #BA2121"">""msg""</span> </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Stage all and commit"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git commit -a </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>See the state of the repo"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git status </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>See all the changes of a repo"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git diff </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Delete a file &lt;file&gt; from the file system and the git repo"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git rm file </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Delete an unstaged file &lt;file&gt; from the file system and the git repo"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git rm -f file </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Remove a file &lt;file&gt; from the stagging area without deleting it from the file system"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git rm --cached file </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Rename a file &lt;file&gt; to &lt;file2&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git mv file file2 </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>See all the past commits"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git log </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>See a summary of all past commits"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git log --pretty<span style=""color: #666666"">=</span>oneline </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Change last commit message"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git commit --amend </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Undo the last stagging for a file &lt;file&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git reset HEAD file </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Add origin url &lt;url&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git remote add origin url </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Display remote urls"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git remote -v </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Get remote files without changing local files"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git fetch origin </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Get remotes files overwritting local ones"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git pull origin </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Upload local files to the remote on the master branch"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git push origin master </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Rename the remote name from ""origin"" to ""sf"""	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git remote rename origin sf </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Display all tags"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git tag </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Add a tag name &lt;v1&gt; with a long description &lt;description&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git tag -a v1 -m <span style=""color: #BA2121"">""description""</span> </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>See details for a tag &lt;tagname&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git show tagname </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Push a tag &lt;tagname&gt; to the remote branch &lt;branchname&gt;<div><br></div>"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git push branchname tagname </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Push all the tags to the remote branch &lt;branchname&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git push branchname --tags </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Create a branch &lt;branchName&gt; and switch to it"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git checkout -b branchName </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Switch to another branch &lt;branchName&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git checkout branchName </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Display all branches"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git branch </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Push a branch &lt;branchName&gt; to a remote &lt;origin&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git push origin branchName </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Get a branch &lt;branchName&gt; from a remote &lt;origin&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git checkout -b branchName origin/branchName </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Display all merged branches"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git branch --merged </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Display all unmerged branches"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git branch --no-merged </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Display all branches and their last commit"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git branch -v </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Delete a branch &lt;branchName&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git branch -d branchName </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Delete an unmerged branch &lt;branchName&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git branch -D branchName </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Delete on remote &lt;origin&gt; a locally deleted branch &lt;branchName&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git push origin :branchName </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Rebase a branch &lt;branchName&gt; to &lt;master&gt;"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1 2</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git checkout branchName git rebase master </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Go to the commit &lt;hash&gt; and discard present ones"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git reset <span style=""color: #008000"">hash</span> </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div><u>Git</u>:</div>Reset and modify the file system"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">git reset --hard </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"What is a <b>Hourglass </b>module from ""<u>Stacked Hourglass Networks for&nbsp;</u><u>Human Pose Estimation</u>""?"	"A <b>U-Net </b>like network to do <b>segmentation</b> that downsample the image using <b>max-pooling &amp; strided convs</b>, then upsample the bottleneck with <b>nearest-neighbours upsampling</b>.<div><br></div><div>There are shortcuts with <b>pointwise convolutions</b>.</div><div><br></div><div>It is ended by two pointwise convolutions to produce the <b>prediction heatmap</b>.</div><div><br></div><div><img src=""paste-88c175dbf094ed1cc6e9a0259221e94391cd929e.jpg""><br></div>"
"What are the <b>stacked Hourglass </b>networks from ""<u>Stacked Hourglass Networks for Human Pose Estimation</u>""?"	"<b>Consecutive</b> hourglass networks with <b>non-shared weights.&nbsp;</b><div><br></div><div><b>Each intermediary hourglass network has a loss</b> comparing to the ground-truth.</div><div><br></div><div>Each <b>refines the prediction heatmap </b>of the previous hourglass network.</div><div><br></div><div><img src=""paste-d1735c7cf6d1a48d4b63bb594012df4af27fe2e5.jpg""><br></div>"
"What is the <b>loss of the hourglass networks</b> in ""<u>Stacked Hourglass Networks for Human Pose Estimation</u>""?"	<b>Mean Squared Error </b>(MSE) comparing to the ground-truth heamap.
"What is the ground-truth heatmap in ""<u>Stacked Hourglass Networks for Human Pose Estimation</u>""?"	Heatmap of zeros everywhere, but with <b>2D gaussian</b>&nbsp;(with std of 1px) <b>centered on the joint location</b>.
What are the <b>2 drawbacks of Anchor Boxes</b> in object detection?	Anchors&nbsp;<b>rarely fit perfectly</b>&nbsp;ground-truth boxes, thus a <b>very large set of anchors is needed </b>(40k in DSSD, 100k in RetinaNet).<div><br></div><div>It has <b>many hyperparameters</b>&nbsp;to be configured, such as grid size &amp; aspect ratio.</div>
What is the architecture of <b>RetinaNet</b>?	"A <b>ResNet </b>backbone, a <b>Feature Pyramid Network</b>&nbsp;(FPN) for scale invariance, <b>anchors</b>&nbsp;for each FPN level, and two <b>Fully Convolutional Networks </b>(FCN) for classification &amp; regression.<div><br></div><div><img src=""paste-4626bbc5063cb4085a6e3fac5e3d80d309b6b641.jpg""><br></div>"
What is the <b>Smooth L1 loss</b>&nbsp;used for?	Loss for <b>box regression</b>&nbsp;in object detection algorithms.&nbsp;
What is the <b>Smooth L1 loss </b>formula?	"With \(x\) the L1 distance between two vectors:<div><br></div><div>\(\begin{cases} 0.5 x^2 &amp; \left|x\right| \lt 1 \\ \left|x\right|&nbsp;&amp; \text{otherwise} \\ \end{cases}\)</div><div><br></div><div><img src=""paste-3f1a68d1b4d3a11a446a2047a9309412dd08e679.jpg""><br></div>"
What is <b>CornerNet</b>?	An&nbsp;<b>object detection</b>&nbsp;architecture, <b>one-stage</b>, that predict the <b>top-left &amp; bottom-right corners</b>&nbsp;of objects <b>without anchors</b>.
What is the <b>backbone of CornerNet</b>?	A<b> stacked Hourglass</b>&nbsp;network, made of <b>2 modified</b>&nbsp;hourglasses.
What is the <b>architecture </b>of <b>CornerNet </b>after its stacked hourglass backbone?	"For each corner (<i>top-left</i>, <i>bottom-right</i>):<div><br></div><div><img src=""paste-d6942906ff28ce7fba99553fbec135453f9c5103.jpg""><br></div>"
How does <b>CornerNet</b>&nbsp;<b>group</b> the top-left &amp; bottom-right corners <b>together</b>?	It produces a <b>1d embedding</b>&nbsp;for each corner.<div><br></div><div>A <b>pull loss</b>&nbsp;forces the network to <b>group corners pairs.</b></div><div><b><br></b></div><div>A <b>push loss</b>&nbsp;forces the network to <b>separate</b>&nbsp;the other <b>corners</b>.</div>
Why does <b>CornerNet</b>&nbsp;produces <b>offsets</b>?	Its backbone outputs a feature maps of <b>smaller</b>&nbsp;than the image. There is thus a <b>loss of precision</b>&nbsp;when <b>rescaling the corners locations to the original image size</b>.
What is the <b>ground-truth offsets</b>&nbsp;for <b>CornerNet</b>?	"<img src=""paste-f36a5fa330ca2e2b8edefec15c98939ad29546b2.jpg"">"
What is the <b>offsets loss </b>for <b>CornerNet</b>?	A <b>Smooth L1 </b>loss.
What is the goal of&nbsp;<b>Corner Pooling</b>&nbsp;in <b>CornerNet</b>?	"Because <b>a corner cannot be based on local evidence</b>:<div><br></div><div><img src=""paste-87374e2991e4a5837fd81553748f2c849d8a0c17.jpg""><br></div><div><br></div><div>Thus we need to <b>see between the corners</b>.</div>"
What is <b>Corner Pooling</b>?	"Set pixels to <b>their max in the orthogonal directions</b>, <b>max-pool </b>them, and <b>add</b> the two directions together:<div><br></div><div><img src=""paste-7c78a6d2dc8d9e5e3bbf8318f3c236e12ce789ea.jpg""><br></div>"
What is <b>Gradual Tuning</b>?	For <b>transfer learning</b>, retrain only the <b>last layers</b>, then <b>gradually retrain</b>&nbsp;<b>earlier layers</b>.
What is <b>Gradient Reversal Layer</b>?	"A layer is <b>identity during forward </b>and <b>identity times a negative constant during backpropagation</b>.<div><br></div><div>It forces the features extractor to <b>maximize </b>the loss of the domain classifier.<br><div><br></div><div><img src=""paste-b3cb70ed405b26e7b4aed5fc779cbfe68f22ac75.jpg""><br></div></div>"
What is <b>Gradient Reversal Layer</b>&nbsp;for?	To train a <b>features extractor </b>to be <b>domain invariant</b>.
What is the <b>Lottery Ticket Hypothesis</b>?	That large networks train better than smaller networks, because they <b>contains small sub-networks</b>&nbsp;that have <b>good connections &amp; initialization weights</b>.
What is the <b>Weakly supervised ranking loss </b>for?	A <b>modification of the triplet loss</b>, with <b>definite negatives</b>&nbsp;and <b>potential positives</b>.
What is the <b>Weakly supervised ranking loss</b>&nbsp;formula?	Given a <b>query image </b>\(q\), several <b>potential positives </b>\(p^q_i\) and <b>definite negatives </b>\(n^q_j\):<div><br></div><div>\(L = \sum_j \max(0, \min_i d^2(q, p^q_i) + m - d^2(q, n^q_j))\)</div>
"What is the goal of ""<u>Open Set Domain Adaptation by Backpropagation</u>""?"	"<b>Domain adaptation </b>to a target set after training on a source set. The target set can contains <b>unknown classes</b>.<div><br></div><div><img src=""paste-c71feb4b0ea4fc6026951860ae5aaacaf9e45685.jpg""><br></div>"
"What is the <b>loss formula </b>in ""<u>Open Set Domain Adaptation by Backpropagation</u>""?&nbsp;"	"<div>Classifier predicts <b>K+1 classes</b>&nbsp;including the <b>unknown class</b>.</div><div><br></div><div><b>Binary cross-entropy </b>on the unknown class softmax probability that must match a fixed target.</div><div><br></div><div>Flip of the gradient with a&nbsp;<b>Gradient Reversal Layer</b>&nbsp;for the target set.</div><div><br></div><img src=""paste-2eb68089cc225f02335446117a6b34bcba7d13cd.jpg"">"
What is <b>least square </b>method?	\(\text{min} \, \lVert Ax - b \rVert^2_2\)<div><br></div><div>\(A \in R^{mxn}, b \in R^m, x \in R^n\)</div>
What is a <b>linear program</b>?	\(\text{min} \, c^Tx\)<div>\(\text{with} \, a_i^Tx \le b_i \, i = 1, ..., m\)</div><div><br></div><div>\(c, a_i, x \in R^n\)</div><div><br></div><div>Note that \(c^Tx\) can be seen as \(c_1 x_1 + ... + c_n x_n\).</div>
What is a <b>quadratic program</b>?	\(\text{min} \, \lVert Fx - g \rVert^2_2\)<div>\(\text{with} \, a_i^Tx \le b_i \, i = 1, ..., m\)</div>
What is a <b>affine set</b>?	Elements that can be defined with the <b>affine combination</b>&nbsp;of two distincts points of the set:<div><br></div><div>\(x = \theta x_1 + (1 - \theta) x_2\)</div>
What is a <b>convex set</b>?	"A set of elements where there is <b>always a line segment between any two points</b>&nbsp;of the set.<div><br></div><div><img src=""paste-6e475d17b59c4a3308fe509e42ea8667ef18856e.jpg""><br></div>"
What is <b>convex combination</b>?	\(x = \theta_1 x_1 + ... + \theta_k x_k\)<div>\(\text{with} \, \theta_1 + ... + \theta_k = 1 \, \text{and} \, \theta_i \ge 0\)</div>
What is a <b>convex hull</b>?	"It is the <b>smallex convex set</b>&nbsp;that contains a set of points C.<div><br></div><div>\(\text{conv}C = \{\theta_1 x_1 + ... + \theta_k x_k \, | \, x_i \in C, \theta_i \ge 0, \sum_k \theta_k = 1\}\)</div><div><br></div><div><img src=""paste-8562ae9bf873e03280db251e5ebfdd87d3a19551.jpg""><br></div>"
What is a <b>convex cone</b>?	"The set that contains the <b>conic combinations </b>of \(x_1\) and \(x_2\):<div><br></div><div>\(x = \theta_1 x_1 + \theta_2 x_2\)</div><div>\(\text{with} \, \theta_1 \ge 0, \theta_2 \ge 0\)</div><div><br></div><div><img src=""paste-1d362be57e5fed59de88c4e754bb508738d21984.jpg""><br></div>"
What is an <b>hyperplane</b>?	"It is the set of the form:<div><br></div><div>\(\{x \, | \, a^tx = b\} \, a \ne 0 \)</div><div><br></div><div>It is <b>affine</b>&nbsp;and <b>convex</b>.</div><div><br></div><div><img src=""paste-ce79089d4031a15826a889075ff121afe9daa0f3.jpg""><br></div>"
What is a <b>conic hull</b>?	"It is the <b>smallest convex cone</b>&nbsp;that contains \(C\).<div><br></div><div><img src=""paste-ca61a2c4349256532c354294c6f9ba48ca985807.jpg""><br></div>"
What is a <b>halfspace</b>?	"It is a set of the form:<div><br></div><div>\(\{x \, | \, a^tx \le b \} \, a \ne 0\)</div><div><br></div><div>It is <b>convex</b>.</div><div><br></div><div><img src=""paste-596a1c283add91f4053b0572eee485f3601ca286.jpg""><br></div>"
What is an <b>ellipsoid</b>&nbsp;formula?	"\(\{x \, | \, (x - x_c)^T P^{-1} (x - x_c) \le 1\}\)<div><br></div><div>With \(P\) <b>symetric positive definite</b>.</div><div><br></div><div><img src=""paste-ff188b1af3b3a026c0402ad5f20b2d689018c70d.jpg""><br></div>"
What is a <b>norm ball </b>formula?	\(\{x \, | \, \lVert x - x_c \rVert \le r\}\)<div><br></div><div>It is <b>convex</b>.</div>
What is a <b>norm cone </b>formula?	"\(\{(x, t) \, | \, \lVert x &nbsp;\rVert \le t\}\)<br><div><br></div><div>It is <b>convex</b>.</div><div><br></div><div><img src=""paste-26e33d82706e407e4d217a0b215ed856436a2447.jpg""><br></div>"
What is a <b>polyhedra</b>?	"A solution set of finitely many linear equalities &amp; inequalities.<div><br></div><div>\(Ax \le b \,\, Cx = d\)</div><div><br></div><div><b>Intersection of a finite number of halfspaces &amp; hyperplanes</b>.</div><div><br></div><div><img src=""paste-aeaca9a58d02cc5a0bdc315dd7044ec12482a176.jpg""><br></div>"
What is a <b>proper cone</b>?	"It is a <b>convex cone</b>&nbsp;\(K \in R^n\) where:<div><br></div><div>K is <b>closed</b>&nbsp;(contains its boundary).</div><div>K is <b>solid </b>(has no empty interior).</div><div>K is <b>pointed </b>(contains no line, i.e. \(x \in K, -x \in K \implies x = 0\))</div><div><br></div><div>Example:</div><div><br></div><div>Cone not pointed:</div><div><br></div><div><img src=""1024px-DoubleCone.png""><br></div><div><br></div><div><br></div><div>Cone pointed:</div><div><br></div><div><img src=""Circular-pyramid.png""><br></div>"
What is the <b>separating hyperplane theorem</b>?	"There is always a <b>separating hyperplane</b>&nbsp;between <b>two disjoint convex sets</b>.<div><br></div><div>\(\{x \, | \, a^Tx = b\}\)</div><div><br></div><div><img src=""paste-76a480eca9664d19be4408f5edc108984ac1304b.jpg""><br></div>"
What is the <b>supporting hyperplane theorem</b>?	"If a set is <b>convex</b>&nbsp;there is a <b>supporting hyperplane</b>&nbsp;at every <b>boundary point</b>&nbsp;of the set.<div><br></div><div><br></div><div>On the following example, there are no supporting hyperplanes at several boundary points.</div><div><img src=""paste-66b22376d7eae17bdff7e4a86721a185d06e875e.jpg""><br></div>"
What is a <b>dual cone</b>?	"The <b>dual of a cone</b>&nbsp;\(K\) is:<div><br></div><div>\(K^* = \{y \, | \, y^Tx \ge 0 \, \text{for all} \, x \in K\}\)</div><div><br></div><div>\(K^*\) is <b>always convex</b>&nbsp;even if \(K\) is not.</div><div><br></div><div><i>It can be seen as the ensemble of the vectors that have an angle inferior to 90deg.</i></div><div><i><br></i></div><div><img src=""paste-3f606da8221ad211f075ce4ebd7b248c33d004e5.jpg""><i><br></i></div>"
In ocvx, what is a <b>minimum element</b>?	"\(x\) is a <b>minimum element</b>&nbsp;of \(S\) iif for all \(\lambda &gt; 0\), \(x\) is a the <b>unique </b>minimizer of \(\lambda^Tz\) over \(S\).<div><br></div><div><img src=""paste-288fbf63f93ed731c65cdde8b091533d981dce3c.jpg""><br></div>"
In ocvx, what is a <b>minimal element</b>?	"If \(x\) minimizes \(\lambda^Tz\) over \(S\) for some \(\lambda &gt; 0\), the \(x\) is minimal.<div><br></div><div><img src=""paste-b8ea1be022c85dc00701fd481a10c89749eb3ced.jpg""><br></div>"
What is <b>Lowshot with imprinted weights</b>?	"Convnet + classifier.<div><br></div><div>The <b>weights</b> of the classifier corresponding to <b>new classes</b> are the <b>features extracted by the convnet</b> on the samples from the new classes.&nbsp;</div><div><br></div><div><img src=""paste-b5d94c620dc8c6657ec50c7b58bd04462bf7fc2d.jpg""><br></div>"
Why the <b>extracted features</b>&nbsp;can act as <b>weights</b>&nbsp;in <b>Lowshot with imprinted weights</b>?	"<div>The existing weights \(w_i\) &amp; the extracted features \(\phi(x)\) are <b>forced to be similar</b> using <b>inner product</b>.</div><div><br></div><img src=""paste-189f62adc73636df02174723dd9935e0390cfff8.jpg""><br><div><br></div><div>Because both \(w_i\) and \(\phi(x)\) are <b>L2 normalized</b> (like in <i>cosine normalization</i>), \(w_i^T \phi(x)\) will be <b>maximised to 1 when their angle is 0</b> (\(cos(0) = 1\)), i.e. when the weights &amp; features are the <b>most similar</b>.</div>"
What is the <b>orthogonal projection</b>&nbsp;onto a line?	"<div>The orthogonal projection of the vector \(x\) onto \(y\) results in \(z\):</div><div><br></div><div>\(z = \frac{x \cdot y}{y \cdot y}y\)</div><div><br></div><img src=""paste-bfecf1b9283cbd459b8aa305f4e7ead36ddaeffc.jpg"">"
What is a <b>symmetric matrix</b>?	"A matrix<b> equal to its transpose</b>.<div><br></div><div><img src=""paste-6cc73dbf479de0b8fe31d020b5b5d109025b0ece.jpg""><br></div>"
What is a <b>minor</b>&nbsp;in linear algebra?	"A&nbsp;<b>minor</b> of a matrix A is the <b>determinant of some smaller square matrix</b>, cut down from A by removing one or more of its rows or columns.<br><div><br></div><div><img src=""paste-7a3af0df0e3fe66a0f92a26f91df4833af31de2c.jpg""><br></div><div><br></div><div>They are required to compute the matrix <b>cofactors</b>&nbsp;used in the <b>inverse</b>&nbsp;of a matrix.</div>"
What is a matrix <b>positive definite</b>?	Equivalently:<div><br></div><div>A matrix \(A\) where for all \(z \in R^n\): \(z^TAz \gt 0\)</div><div>All <b>eigen values</b> of the matrix are positive.</div><div>All <b>leading principal</b>&nbsp;<b>minors</b> of the matrix are positive.</div>
What are the&nbsp;<b>principal minors </b>of a matrix?	The minors where \(i = j\).<div><br></div><div>Note that a minor \(M_{12}\) is the determinant of the matrix without the first row and the second column.</div>
What is a <b>concave function</b>?	"A function \(f\) is <b>concave</b> is \(-f\) is <b>convex.</b><div><b><br></b></div><div><img src=""paste-f2e6be9ce9e0beda6617103654c36b19363b8f90.jpg""><b><br></b></div>"
What is a matrix <b>positive semidefinite</b>?	Equivalently:<div><br></div><div>A matrix \(A\) where for all \(z \in R^n\): \(z^TAz \ge 0\)</div><div>All&nbsp;<b>eigen values</b>&nbsp;of the matrix are non-negative.</div><div>All<b>&nbsp;principal</b>&nbsp;<b>minors</b>&nbsp;of the matrix are non-negative.</div>
What is the <b>duality </b>in optimization theory?	"<strong>Duality</strong> means that optimization problems may be viewed from either of two perspectives:<div><br></div><div>The <b>primal</b> problem or the <b>dual</b> problem.</div><div><br></div><div>The solution to the dual problem provides a <b>lower bound</b> to the solution of the primal (minimization) problem.<br></div><div><br></div><div><i>Example</i>: The primal problem is to minimize a function \(f\), then the dual problem is to maximize a function \(-f\).</div><div><br></div><div><img src=""paste-80b7f0114a524725699bdf47ababc3091d23a497.jpg""><br></div>"
How to use the <b>method of Lagrange multipliers</b>&nbsp;to solve a optimization problem of a function with an equality constraint?	"<div>Suppose we want to minimise a function \(f\) with an equality constraint \(g(x) = 0\).</div><div><br></div><div>We create the function \(L(x, \lambda) = f(x) + \lambda g(x)\).<br><br>We then search for all solutions \(x\), \(\lambda\) satisfying \(\nabla L(x, \lambda) = 0\).</div><div><br></div><div>Among those solutions are maximums, and minimums. We keep only the minimums.</div><div><br></div><img src=""paste-6ada68d50766fdb1bb807bba9683555ccbff6b02.jpg"">"
How to use the&nbsp;<b>method of Lagrange multipliers</b>&nbsp;to solve a optimization problem of a function with an inequality constraint?	"Same as with <b>equality constraints</b>, but there is an additonal constraint on the Lagrange multiplier \(\lambda\):<div><br></div><div><img src=""paste-c9541890462e05cc56074c4406ee2d757f58271b.jpg""><br></div>"
What is the <b>analytical </b>version of the <b>SVM optimization</b>&nbsp;problem?	"We want to maximize the margin \(m = \frac{2}{\Vert w \Vert}\).<div><br></div><div><img src=""paste-fbfb96d5b50b6462bc15bc45e1e622916a265f9b.jpg""><br></div><div><br></div><div>With the <b>lagrangian</b>:</div><div><br></div><div><img src=""paste-0ded75465155958419318f5f03389e2cfb705c8e.jpg""><br></div>"
Explain how to <b>maximize the margin \(m\) </b>in a <b>SVM</b>.	"<img src=""paste-c220a0278272949c809c20af0b708d223597a973.jpg""><br><div><img src=""paste-7aba671a7d33dde607b11466f09e923c7072f448.jpg""><br></div><div><img src=""paste-ea272172ed74a03b90b596753bc2008a9661bc2f.jpg""><br></div>"
What are <b>disjoint sets</b>&nbsp;in set theory?	\(A \cap B = \emptyset\)
What is a <b>partition</b>&nbsp;in set theory?	A grouping of the set's elements into non-empty subsets, in such a way that every element is included in one and only one of the subsets.<br><br><i>Example</i>:<br>\(A = \{1, 2, 3, 4\}\)<div>\(\text{Partition of} \, A = \{\{1, 2\}, 3, 4\}\)</div>
What is a <b>cartesian product </b>in set theory?	"<img src=""cartesian1.svg""><div><br></div><div><img src=""paste-9adb2f92579ca5b4e95b57c1b23c810b5bf52f70.jpg""><br></div>"
What is a <b>power set</b>&nbsp;in set theory?	"<b>The power set</b>&nbsp;of any set <i>S</i> is the <b>set of all subsets of <i>S</i></b>, including the empty set and S itself.<br><div><br></div><div><i>Example</i>:</div><div><br></div><div>\(A = \{1, 2, 3\}\)</div><div><img src=""paste-24d6ab518f7b499c3e11cde72ef6ce1ee35b1d60.jpg""><br></div><div><br></div>"
What is <b>cardinality</b>&nbsp;in set theory?	"<img src=""paste-850055804b97ba172c9c422ed346fe1ab8b45d8a.jpg"">"
What is the <b>output dimension</b>&nbsp;of a 2d <b>convolution</b>?	\(W_o = (W_i - K + 2P) / S + 1\)<div><br></div><div>With \(W_o\) and \(W_i\) the output and input dimension.</div><div>\(K\) the kernel size, \(P\) the padding, and \(S\) the stride.</div>
What is the <b>Hinge Loss</b>&nbsp;formula used by SVMs?	\(H(t, y) = \text{max}(0, 1 - t \cdot y)\)<div><br></div><div>Where \(t\) is the target, either -1 or 1.</div><div>And \(y\) the result of \(w \cdot x + b\).</div><div><br></div><div>It can be seen that when <b>\(t\) and \(y\) have the same sign</b> (meaning \(y\) predicts the right class) the <b>hinge loss is minimal</b>. With opposite sign it is maximal.</div>
"What is the goal of ""<u>Multi-Adversarial Domain Adaptation</u>""?"	"Make the <b>features extractor indiscriminative</b> to the source &amp; target domain.<div><br></div><div>But only <b>pairwise by categories</b>.</div><div><br></div><div><i>i.e. ""dog"" of target should be close to ""dog"" of source, and ""cat"" of source should be close to ""cat"" of target.</i></div><div><i><br></i></div><div><img src=""paste-1a0790715c431b5c680414ee8640b9fb8e546b2c.jpg""><i><br></i></div>"
"What is the architecture described in ""<u>Multi-Adversarial Domain Adaptation</u>""?"	"<div>Shared features extractor.</div><div>A classifier for the source domain.</div><div><br></div><div>One <b>domain classifier</b> (<i>source vs target</i>) <b>for each category</b>.</div><div>The features given to the domain classifiers is <b>weighted by their probability</b>.</div><div><br></div><div><b>Gradient reversal layer</b> for all domain classifiers.</div><div><br></div><img src=""paste-d124560b6a6948486c1cf8821d3111e3e4fdf75d.jpg"">"
"What is the goal of ""<u>Omnia Faster-RCNN</u>""?"	"Being able to <b>exploit several datasets by</b> <b>merging them</b>.<div><br></div><div>It improves the definition of what <b>background</b>&nbsp;is, since categories from one dataset can be only background for another.</div><div><br></div><div><i>Here a Modanet Faster-RCNN (top row) will see clothing everywhere and not be able to see ""person"" while Omnia (bottom row) is improved by its merging with OpenImages.</i></div><div><img src=""paste-b6eb1ac789f58a5393402fea5a46a85ff1969ed0.jpg""><br></div>"
"What is the goal of ""<u>Adaptive Semantic Segmentation with a strategic curriculum of proxy labels</u>""?"	To learn a <b>segmentation</b> on <b>unlabeled target domain</b> using <b>labeled source domain</b>.
"What is the <b>architecture</b> of ""<u>Adaptive Semantic Segmentation with a strategic curriculum of proxy labels</u>""?"	"<div><b>Shared convnet encoder</b>.</div><div>Two differents branches predict a segmentation.</div><div>If <b>they disagree</b>, it means the at least <b>one of them is wrong</b>, and the <b>gradient won't be backpropagated</b>.</div><div><br></div><img src=""paste-1b1a9fc2e4041f3a11d1181cb3cc9bac4c9d5810.jpg"">"
"What is the <b>curriculum </b>in ""<u>Adaptive Semantic Segmentation with a strategic curriculum of proxy labels</u>""?"	"Source &amp; target images are sampled randomly.<div><br><div>A segmentation map is produced by each branch, <b>the gradient is not backpropagated when the branches disagree on a pixel</b>.</div><div><br></div><div>It uses the concept of <b>self-training</b>, since we don't have labels on the target domain.</div><div><br></div><div>At first, most target images are not used as their are harder (<b>target easy mining</b>).</div><div><br></div><div><img src=""paste-1b1a9fc2e4041f3a11d1181cb3cc9bac4c9d5810.jpg""><br></div><div><br></div><div>At the end, very-well classified pixels on the source domain are not used to avoid being overwhelmed from them (<b>source hard mining</b>).</div></div>"
"How does the authors in ""<u>Adaptive Semantic Segmentation with a strategic curriculum of proxy labels</u>"" <b>avoid the two prediction branches to learn similar weights</b>?"	Each branch is <b>initialized with a different random seed</b>.<div><br></div><div>A <b>cosine similarity</b> (\(w_1 \cdot w_2\)) between the two branches is used as a <b>regularization factor</b>.</div>
"What is the <b>goal</b>&nbsp;of ""<u>WELDON</u>""?"	To predicts <b>relevant image regions</b> (like an object detector) using only <b>image-level annotations</b>.<div><br></div><div><i>i.e. by learning that in an image, there is a cat, it may find the most probable image's regions where the cat could be located.</i></div>
"What is the <b>architecture </b>of ""<u>WELDON</u>""?"	"<div>A features extractor.</div><div>A <b>transfer layer</b>&nbsp;where the feature maps spatial dimensions are reduced, each new spatial positive is a <i>region<b>.</b></i></div><div>A <b>pointwise convolution </b>that produces a channel dimensions for each class \(C\).</div><div><br></div><div>A <b>spatial pooling</b>&nbsp;that <b>sums the top \(k\) regions and the low \(k\) regions</b>.</div><div><br></div><img src=""paste-d15fc7f5db7cf778e3980f9ee0cb8f1fd0cf6614.jpg"">"
What is a&nbsp;<b>mode</b>?	"The mode of a set of data values is the <b>value that appears most often</b>.<br><div><br></div><div><img src=""paste-b5d472d4567d83c8f343908c97fdf1fdd9605dc2.jpg""><br></div>"
<b><span class=cloze>[...]</span></b>&nbsp;is a decision rule used in artificial intelligence for <i>mini</i>mizing the possible loss for a worst case (<i>max</i>imum loss) scenario.	"<b><span class=cloze>Minimax</span></b>&nbsp;is a decision rule used in artificial intelligence for <i>mini</i>mizing the possible loss for a worst case (<i>max</i>imum loss) scenario.<br><br> <img src=""paste-17de5929206b447202535ae3b8909475acd872bd.jpg"">"
<b>Minimax</b>&nbsp;is a decision rule used in artificial intelligence for <span class=cloze>[...]</span>.	"<b>Minimax</b>&nbsp;is a decision rule used in artificial intelligence for <span class=cloze><i>mini</i>mizing the possible loss for a worst case (<i>max</i>imum loss) scenario</span>.<br><br> <img src=""paste-17de5929206b447202535ae3b8909475acd872bd.jpg"">"
What is the goal of the <b>ROCK</b>&nbsp;algorithm?	Using <b>auxiliary tasks</b>&nbsp;to improve the <b>primary task</b>&nbsp;using a <b>multi-task learning </b>scheme.
What is the general <b>architecture </b>of the <b>ROCK </b>algorithm?	"<img src=""paste-9b6136fd26e20a527844cee15ee5398e925af989.jpg"">"
How are the different tasks <b>fusionned</b>&nbsp;together in the <b>ROCK </b>algorithm?	"Using <b>element-wise addition</b> between the auxiliary features maps and the residual from the convnet.<div><br></div><div><img src=""paste-b386f902949f0abd58cce2e097019e240d3fcd33.jpg""><br></div>"
What is the goal of <b>Domain Separation Networks</b>?	To bridge the <b>low-level gap</b>&nbsp;(noise, illuminance, color, etc.)&nbsp;between two domains.<div><br></div><div>To do so, it learn a features representation <b>invariant to the domains</b>&nbsp;and features representations <b>unique to each domain</b>.</div>
<b>Four kinds of losses </b>in <b>Domain Separation Networks</b>?<div><br></div><div><span class=cloze>[]</span><div>Feature representation&nbsp;<b>difference</b></div><div><b>Source classification</b></div><div><b>Image reconstruction</b></div></div>	"<b>Four kinds of losses </b>in <b>Domain Separation Networks</b>?<div><br></div><div><span class=cloze>Feature representation&nbsp;<b>similarity</b></span><div>Feature representation&nbsp;<b>difference</b></div><div><b>Source classification</b></div><div><b>Image reconstruction</b></div></div><br><br> <div><img src=""paste-efd0c19cb9b285181ec985e00d1c3f9fafd15b04.jpg""><br></div>"
What is the general <b>architecture</b>&nbsp;of <b>Domain Separation Networks</b>?	"<img src=""paste-efd0c19cb9b285181ec985e00d1c3f9fafd15b04.jpg"">"
\( \log_b a  = [...]\)	\( \log_b a  =  \frac{\log a}{\log b} \)<br><br> 
\([...] =  \frac{\log a}{\log b} \)	\( \log_b a  =  \frac{\log a}{\log b} \)<br><br> 
The <b>Gradient Noise Scale </b>measures&nbsp;the variation in the data as seen by the model.&nbsp;<div><br></div><div>When the noise scale is small, looking at a lot of data in parallel <span class=cloze>[...]</span>, whereas when it is large, we can <span class=cloze>[...]</span>.</div><div><br></div><div>Its formula is: \(B =  \frac{E[\Vert G - G_{true} \Vert_2^2]}{\Vert G_{true} \Vert_2^2} \)</div>	The <b>Gradient Noise Scale </b>measures&nbsp;the variation in the data as seen by the model.&nbsp;<div><br></div><div>When the noise scale is small, looking at a lot of data in parallel <span class=cloze>quickly becomes redundant</span>, whereas when it is large, we can <span class=cloze>still learn a lot from huge batches of data</span>.</div><div><br></div><div>Its formula is: \(B =  \frac{E[\Vert G - G_{true} \Vert_2^2]}{\Vert G_{true} \Vert_2^2} \)</div><br><br> 
The <b>Gradient Noise Scale </b>measures&nbsp;the <span class=cloze>[...]</span>.&nbsp;<div><br></div><div>When the noise scale is small, looking at a lot of data in parallel quickly becomes redundant, whereas when it is large, we can still learn a lot from huge batches of data.</div><div><br></div><div>Its formula is: \(B =  \frac{E[\Vert G - G_{true} \Vert_2^2]}{\Vert G_{true} \Vert_2^2} \)</div>	The <b>Gradient Noise Scale </b>measures&nbsp;the <span class=cloze>variation in the data as seen by the model</span>.&nbsp;<div><br></div><div>When the noise scale is small, looking at a lot of data in parallel quickly becomes redundant, whereas when it is large, we can still learn a lot from huge batches of data.</div><div><br></div><div>Its formula is: \(B =  \frac{E[\Vert G - G_{true} \Vert_2^2]}{\Vert G_{true} \Vert_2^2} \)</div><br><br> 
The <b>Gradient Noise Scale </b>measures&nbsp;the variation in the data as seen by the model.&nbsp;<div><br></div><div>When the noise scale is small, looking at a lot of data in parallel quickly becomes redundant, whereas when it is large, we can still learn a lot from huge batches of data.</div><div><br></div><div>Its formula is: \(B = [...]\)</div>	The <b>Gradient Noise Scale </b>measures&nbsp;the variation in the data as seen by the model.&nbsp;<div><br></div><div>When the noise scale is small, looking at a lot of data in parallel quickly becomes redundant, whereas when it is large, we can still learn a lot from huge batches of data.</div><div><br></div><div>Its formula is: \(B =  \frac{E[\Vert G - G_{true} \Vert_2^2]}{\Vert G_{true} \Vert_2^2} \)</div><br><br> 
What is&nbsp;<b>CutOut</b>?	"A <b>data augmentation</b> method that places randomly grey <b>squares</b> on the image.<div><br></div><div><img src=""paste-c56789faf37b3c086fc951ab1f29015c9d06c4cd.jpg""><br></div>"
What is <b>SamplePairing</b>?	"A <b>data augmentation</b> method that <b>averages</b> two images together, but uses only the labels of one.<div><br></div><div><img src=""paste-dce429eebfa2dc51ebf426cc4ae605ebdf49b920.jpg""><br></div>"
What is <b>MixUp</b>?	A <b>data augmentation</b> method that <b>mix two images and their labels</b> together based on a <b>beta</b>-distribution ponderation.
What is <b>Shake-shake regularization</b>?	"<b>Two branch residuals ponderated</b> by a random variable.<div><br></div><div><img src=""paste-76641a83b60735b3533545f879a6f1e9f8eb5d85.jpg""><br></div>"
Out-of-Distribution (OoD) problem 	When the model is totally unqualified to make&nbsp;<b>predictions on data generated via a different procedure</b>&nbsp;than the one used to create the training set.&nbsp;  
When the model is totally unqualified to make&nbsp;<b>predictions on data generated via a different procedure</b>&nbsp;than the one used to create the training set.&nbsp;	Out-of-Distribution (OoD) problem
<span class=cloze>[...]</span> uncertainty is representative of the random noise in an experiments.	<span class=cloze>Aleatoric</span> uncertainty is representative of the random noise in an experiments.<br><br> <i>Example: Imperfect sensor</i>
Aleatoric uncertainty is <span class=cloze>[...]</span>.	Aleatoric uncertainty is <span class=cloze>representative of the random noise in an experiments</span>.<br><br> <i>Example: Imperfect sensor</i>
<span class=cloze>[...]</span> uncertainty is tied to the negligence of certain effects.	<span class=cloze>Epistemic</span> uncertainty is tied to the negligence of certain effects.<br><br> <div><i>Example: Neglecting the air resistance</i><br></div>
Epistemic uncertainty is <span class=cloze>[...]</span>.	Epistemic uncertainty is <span class=cloze>tied to the negligence of certain effects</span>.<br><br> <div><i>Example: Neglecting the air resistance</i><br></div>
<div> <div> <div> <div>A distributed algorithm is <span class=cloze>[...]</span>&nbsp;(Blanchard et al., 2017) if its convergence is robust when up to 50% of workers behave <b>adversarially</b>.</div> </div> </div></div>	<div> <div> <div> <div>A distributed algorithm is <span class=cloze><b>Byzantine fault tolerant</b></span>&nbsp;(Blanchard et al., 2017) if its convergence is robust when up to 50% of workers behave <b>adversarially</b>.</div> </div> </div></div><br><br> 
<div> <div> <div> <div>A distributed algorithm is <b>Byzantine fault tolerant</b>&nbsp;(Blanchard et al., 2017) if its convergence is robust when up to <span class=cloze>[...]</span> of workers behave <b>adversarially</b>.</div> </div> </div></div>	<div> <div> <div> <div>A distributed algorithm is <b>Byzantine fault tolerant</b>&nbsp;(Blanchard et al., 2017) if its convergence is robust when up to <span class=cloze>50%</span> of workers behave <b>adversarially</b>.</div> </div> </div></div><br><br> 
What are the&nbsp;<b>derivative&nbsp;w.r.t \(X\)</b>&nbsp;<b>of a linear layer</b>&nbsp;\(Z = XW + B\) with the matrix form?<div><br></div><div>\(X\) = (batch size, dimension in)</div><div>\(W\) = (dimension in, dimension out)</div><div>\(B\) = (dimension out)</div>	<div>\(\frac{\partial Z}{\partial X} = \text{grad} \,@\, W^T\)</div><div><br></div><div>With \(\text{grad}\) being \(\frac{\partial Z}{\partial X}\) of the following layer.<br></div>
What are the&nbsp;<b>derivative&nbsp;w.r.t \(W\)</b>&nbsp;<b>of a linear layer</b>&nbsp;\(Z = XW + B\) with the matrix form?<div><br></div><div>\(X\) = (batch size, dimension in)</div><div>\(W\) = (dimension in, dimension out)</div><div>\(B\) = (dimension out)</div>	<div>\(\frac{\partial Z}{\partial W} = X^T \, @\, \text{grad}\)</div><div><br></div><div>With \(\text{grad}\) being \(\frac{\partial Z}{\partial X}\) of the following layer.<br></div>
What are the&nbsp;<b>derivative&nbsp;w.r.t \(B\)</b>&nbsp;<b>of a linear layer</b>&nbsp;\(Z = XW + B\) with the matrix form?<div><br></div><div>\(X\) = (batch size, dimension in)</div><div>\(W\) = (dimension in, dimension out)</div><div>\(B\) = (dimension out)</div>	\(\frac{\partial Z}{\partial B} = \text{grad}\)<br><div><br></div><div>With \(\text{grad}\) being \(\frac{\partial Z}{\partial X}\) of the following layer.<br></div>
What is the task of <b>Natural Language Inference</b>?	"The task of classifying couples of sentences as either <b>entailment</b><em> </em>(i.e. logical implication), <b>contradiction</b>, or <b>neutral</b> (neither of the previous classes).<br><div><br></div><div><img src=""1BH2LrrmrKsiuvmg3iE4FgQ.png""><br></div>"
What is&nbsp;<strong>Deep Averaging Networks</strong>?	NLP model where the <b>sentence's characters embeddings are averaged</b> &amp; then fed to a multi-layers classifier.
What is the <b>brievety penalty</b>&nbsp;in the BLEU score?	Factor multiplied to the BLEU score to <b>penalize translations shorter than the reference.</b><div><br></div><div>If translation is \(\ge\) than reference than 1.</div><div>Else go progressively to 0.</div>
What does the acronyme&nbsp;<b>BLEU </b>means?	<b>B</b>ilingua<b>l</b> <b>e</b>valuation <b>u</b>nderstudy
What is the main idea to compute <b>BLEU</b> for translation?	Compute an <b>average of the intersection (1 if found else 0)</b> of <b>unigram, bigram, trigram, and 4-gram</b> between reference sentence &amp; translation.
What is the <b>originaly-design level</b> (word, sentence, corpus, etc.) of <b>BLEU</b> score?	corpus level
reharsal incremental learning	<b>Mixing old &amp; new</b> classes during the sessions to simulate iid conditions.
pseudo-reharsal incremental learning	Mixing old &amp; new classes, by <b>generating the wanted data</b>.<div><br></div><div>It avoids storing all the dataset as reharsal does.</div>
incremental learning: <b>herding</b>	<b>Keeping the most useful samples</b> in the external memory.<div><br></div><div>Often can be the closest images to the class mean.</div><div><br></div><div>Also called <b>memory selection</b>.</div>
<b>iCarl</b>: Examplars<b> </b>selection in bounded memory	Computes features <b>mean</b> per class.<div><br></div><div><b>Select iteratively</b> sample whose <b>average w/ all selected</b> samples is closest to mean class.</div>
<b>iCarl</b>: memory bounded	<b>Fixed number of stored samples</b>.<div><br></div><div>Less samples per class as number of classes grows up.</div>
<b>iCarl</b>: distillation loss for previous classes	Force new model to <b>match previous model predictions</b> probabilities on previous classes
<b>iCarl</b>: inference prediction	Closest w/ L2 distance to the class mean
<b>iCarl</b>: training data for a session	<b>All new</b> classes' images + <b>examplars</b> of previous classes
<b>BERT</b>: architecture	A <b>Tranformer</b>&nbsp;encoder
<b>BERT</b>: inputs embeddings	"<b>Token </b>embeddings (<i>like word2vec</i>).<div><br></div><div><b>Segment </b>embeddings (<i>to know which part is question or paragraph</i>).</div><div><br></div><div><b>Positional </b>embeddings (<i>where is the word in the sentence</i>).</div><div><br></div><div><img src=""paste-19608f5f510d7187819ebfbee8b54accb6c35d2e.jpg""><br></div>"
<b>BERT</b>: masked language model	"<div>Has to encode-decode same sentence.</div><div><br></div><div>Some output words are masked, or replaced w/ random.</div><div><br></div><div><br></div><img src=""0ViwaI3Vvbnd-CJSQ.png"">"
<b>BERT</b>: next sentence prediction	"Give to <b>2 sentences</b> concatenated: try to predict if <b>one follows the others</b> or else <b>no relation</b>.<div><br></div><div><img src=""paste-d36b5ddd6aa90c991e4e33e23962c470e99a4467.jpg""><br></div>"
unsupervised-learning: <b>jigsaw</b>	<b>Split image in 9 patches</b> w/ some gaps between them.<div><br></div><div>Goal is to <b>reorder</b> them.</div>
unsupervised-learning: <b>relative patch location</b>	Make <b>two patches</b> from an image.<div><br></div><div>Learns <b>spatial relation</b> (top, left, right, bottom) between them.</div>
\(\lVert x \rVert \ge [...]\)<div><br></div><div>\(\lVert x \rVert = 0 \, \text{iif} \, x = [...]\)</div>	\(\lVert x \rVert \ge 0\)<div><br></div><div>\(\lVert x \rVert = 0 \, \text{iif} \, x = 0\)</div><br><br> 
<div>\(\lVert tx \rVert = [...]\)</div>	<div>\(\lVert tx \rVert = |t| \lVert x \rVert\)</div><br><br> 
\(\lVert x + y \rVert \le [...]\)	\(\lVert x + y \rVert \le \lVert x \rVert + \lVert y \rVert\)<br><br> 
what:&nbsp;<b>Mutual Information </b>between X and Y	<div> <div> <div> <div>Amount of information learned from knowledge of random variable Y about the other random variable X</div> </div> </div></div>
<div> <div> <div> <div>Mutual Information formula:</div><div><br></div><div>\(I(X; Y )\) = <span class=cloze>[...]</span></div> </div> </div></div>	<div> <div> <div> <div>Mutual Information formula:</div><div><br></div><div>\(I(X; Y )\) = <span class=cloze>\(H(X) − H(X|Y ) = H(Y ) − H(Y |X)\) with \(H(.)\) the entropy </span></div> </div> </div></div><br><br> 
what: <b>miscalibration</b> in neural networks	"<b>Discrepancy</b> between the <b>confidence &amp; the accuracy.</b><div><b><br></b></div><div><i>Ex: With perfect calibration, if 100 examples are classified with a confidence of 0.8, then 80% of them are right.</i></div><div><i><br></i></div><div><img src=""Screen Shot 2019-04-06 at 17.27.20.png""><i><br></i></div>"
what: <b>Expected Calibration Error</b> (ECE)&nbsp;	<b>Average difference between confidence &amp; accuracy</b><br><br>Computed per bins of equal size, and weighted by bins population size.
How does <b>model capacity</b> influences <b>miscalibration</b>?	"More model capacity <b>increases</b> the miscalibration.<div><br></div><div><img src=""Screen Shot 2019-04-06 at 17.36.09.png""><br></div>"
Why more <b>model capacity</b> increases&nbsp;<b>miscalibration</b> while reducing error?	Because it overfit the <b>Negative Likelihood</b>&nbsp;&amp; thus <b>increases incorrectly the confidence</b>.
How does <b>regularization</b> influences <b>miscalibration</b>?	"More <b>regularization </b>descreases <b>miscalibration</b>.<div><br></div><div><img src=""Screen Shot 2019-04-06 at 17.47.12.png""><br></div>"
what: <b>Temperature Scaling</b>, in miscalibration correction	"Learns a temperature \(T\) on the validation set to <b>soften the softmax.</b><div><b><br></b></div><div><img src=""Screen Shot 2019-04-06 at 17.48.52.png""><b><br></b></div><div><br></div><div>It doesn't change accuracy.</div>"
what: <b>Isotonic Regression</b>	"Learns a <b>piece-wise </b>function <b>monotonic</b>:<div><br></div><div><img src=""400px-Isotonic_regression.svg.png""><br></div>"
"What is algo name described in:<div><br></div><div><img src=""Screen Shot 2019-04-07 at 14.26.36.png""><br></div>"	Elastic Weight Consolidation (EWC)
What is main idea of <b>Elastic Weight Consolidation</b>?	"Weights of task \(T + 1\) are L2 <b>regularized to their counterpart</b> in task \(T\)<b> proportionaly to their importance</b> (\(F_i\)).<div><br></div><div><img src=""Screen Shot 2019-04-07 at 14.31.24.png""><br></div>"
What is the <b>importance factor</b>&nbsp;in <b>Elastic Weight Consolidation</b> (EWC)?	"The <b>Fisher information matrix </b>(\(F_i\))<b>.</b><div><b><br></b></div><div><img src=""Screen Shot 2019-04-07 at 14.31.24.png""><b><br></b></div><div><i>Basically the variance of the gradients.</i></div>"
"What is the main idea of this paper:<div><br></div><div><img src=""Screen Shot 2019-04-07 at 14.35.43.png""><br></div>"	"Learns <b>incremental tasks</b> while keeping a <b>fixed model capacity</b>.<div><br></div><div>Each tasks only uses a <b>subset of the model weights</b>.</div><div><br></div><div><br></div><div><i>See ""Lottery Ticket Hypothesis"".</i></div>"
"How does ""<u>Continual Learning with Neural Pruning</u>"" <b>re-use weights</b> between tasks?"	"Uses <b>sparsification </b>during training.<div><br></div><div>Only <b>very active neurons are kept &amp; frozen</b>.</div><div><br></div><div>The others are <b>reset</b>.</div><div><br></div><div><img src=""Screen Shot 2019-04-07 at 14.49.27.png""><br></div>"
"How does ""<u>Continual Learning with Neural Prunin</u>g"" can <b>re-use the same classification head </b>for all tasks?"	"Classification head <b>weights are shared</b>.<div><br></div><div>Some <b>input neurons are masked</b> to correspond to the task.</div><div><br></div><div><img src=""Screen Shot 2019-04-07 at 14.54.29.png""><br></div>"
"What is the goal of:<div><br></div><div><img src=""Screen Shot 2019-04-07 at 14.56.40.png""><br></div>"	"<b>Re-use previous models as residual</b> to be added to the new model for a new task.<div><br></div><div><img src=""Screen Shot 2019-04-07 at 14.58.28.png""><br></div>"
Mutual information \(I(X, Y) = 0\) means <span class=cloze>[...]</span>	Mutual information \(I(X, Y) = 0\) means <span class=cloze>\(X\) and \(Y\) are independent.</span><br><br> 
What is a<b>&nbsp;Separable convolution?</b>	<b>depthwise</b> convolution + <b>pointwise</b> convolution
"What does this paper introduces:<br><br><img src=""Screen Shot 2019-04-08 at 12.42.12.png"">"	"Creation of <b>randomly wired network </b>as an alternative to more complicated NAS method.<div><br></div><div>Called <b>RandWire</b>.<br><div><br></div><div>Reachs competitive results on ImageNet.</div><div><br></div><div><img src=""Screen Shot 2019-04-08 at 11.53.50.png""><br></div></div>"
what: <b>L</b>&nbsp;axis in the <b>LAB color space</b>?	<b>Lightness</b><div><br></div><div><i>0 is the dark, while 100 bright.</i></div>
what: <b>A</b>&nbsp;axis in the&nbsp;<b>LAB&nbsp;color space</b>?	<b>green</b>–<b>red</b> component.<br><div><br></div><div><i>Green in the negative direction and red in the positive direction.</i><br></div>
what: <b>B</b>&nbsp;axis in the&nbsp;<b>LAB&nbsp;color space</b>?	<b>Blue</b>–<b>yellow</b> component.<div><br></div><div><i>Blue in the negative direction and yellow in the positive direction</i><br></div>
What is the advantage of <b>LAB color space</b>&nbsp;over <b>RGB</b>?	<b>Distances</b> in LAB are more <b>similar to what human eye sees.</b>
<b>linear scaling rule</b>&nbsp;about learning rate	When the<b> minibatch size</b> is multiplied by k, multiply the <b>learning rate</b> by k
what: <b>time-dependent gaussian noise</b>	"Add a <b>gaussian noise to the gradients</b>, to regularize.<div><br></div><div><img src=""Screen Shot 2019-04-29 at 11.11.55.png""><br></div><div><br></div><div>The mean is null, and the <b>variance is high at the beginning then decreasing</b>.</div><div><br></div><div><img src=""Screen Shot 2019-04-29 at 11.12.00.png""><br></div>"
What is the use of <b>Laplacian filter</b>?	Detects <b>edges</b>
How can we <b>measure the blurriness</b> of an image?	By computing the <b>variance </b>of the <b>Laplacian filter</b>.<div><br></div><div>It should be <b>low for blurry images</b>.</div>
what is an <b>undercomplete autoencoder</b>?	An autoencoder where the <b>code dimension is smaller than the input dimension.</b><div><b><br></b></div><div>This makes a <b>bottleneck</b>&nbsp;allowing it to select only the relevant submanifold.</div>
how do we call an <b>autoencoder</b>&nbsp;that has a <b>bottleneck</b>?	An <b>undercomplete </b>autoencoder
what are <b>Adabound</b>'s critics of <b>Adam</b>?	"Adam has an <b>adaptative learning rate</b> per layer.<div><br></div><div>It can be sometimes <b>extreme</b>: too high or too low.</div><div><br></div><div><img src=""Screen Shot 2019-05-23 at 21.25.40.png""><br></div>"
how <b>Adabound </b>is different from <b>Adam</b>?	At first Adabound behaves like Adam.<div><br></div><div>Then gradually the <b>minimal &amp; maximal adaptative learning rate get closer</b> up to force a single learning for all layers.</div>
<b>Gradient Episodic Memory</b>: what is the initial <b>constraint</b>?	"<b>Loss</b> cannot be <b>higher</b> on <b>memory</b> examples:<div><br></div><div><img src=""Screen Shot 2019-05-23 at 23.19.12.png""><br></div>"
intuition of <b>Maximum Entropy Regularizer </b>(MER)	"Enforce the <b>predictions to be less certain</b>&nbsp;when deadling with <b>uncertain transferred knowledge</b>.<br><br><img src=""Screen Shot 2019-05-29 at 16.59.44.png"">"
intuition&nbsp;<b>DropOut Sampling </b>(DOS)	Sampling/curriculum method:<div><br></div><div>- that <b>alleviates the class imbalance </b>between new and old classes</div><div><br></div><div>- gradually <b>samples more high uncertainty examples later in the training</b></div>
first phase of <b>DropOut Sampling </b>(DOS)	<b>Randomly exclude samples from new classes </b>to have as much new classes as old classes.
second phase of&nbsp;<b>DropOut Sampling&nbsp;</b>(DOS)	Excludes <b>samples from new classes with high CE</b> values.<br><br>With <b>self-paced curriculum</b> where high uncertainty samples are learned later.
"herding method for <b>MEDIC </b>algorithm<br><br><img src=""Screen Shot 2019-05-29 at 17.08.19.png"">"	<b>Random sampling<br></b><br><i>End-to-End shows that it is almost equivalent to smarter herding.</i>
"2 improvements of <b>MEDIC </b>algorithm<br><br><img src=""Screen Shot 2019-05-29 at 17.08.19.png"">"	<b>Maximum Entropy Regularization </b>(MER)<br><br><b>DropOut Sampling </b>(DOS)
"how <b>Gradient Episodic Memory </b>(GEM) enforce their loss constraint in the gradients?<br><br><img src=""Screen Shot 2019-05-29 at 17.11.04.png"">"	"Enforce that the <b>gradients are in the same direction </b>by computing their angle:<br><br><img src=""Screen Shot 2019-05-29 at 17.10.55.png""><br><br><img src=""Screen Shot 2017-07-31 at 7.36.14 PM.png"">"
what <b>Gradient Episodic Memory </b>(GEM) does if gradient constraint is violated?	"<b>Project the gradient \(g\) to its closest gradient \(\tilde{g}\) satisfying all the constraints.<br><br></b><img src=""Screen Shot 2019-05-29 at 17.14.06.png""><br><b><br></b><img src=""Screen Shot 2017-07-31 at 7.36.14 PM.png""><b><br></b>"
3 naive types of <b>scaling to increase a neural network accuracy</b>	"<div><b>Width </b>scaling</div><div><b>Depth </b>scaling<br><b>Resolution </b>scaling</div><img src=""Screen Shot 2019-05-31 at 17.24.23.png"">"
"<b>compound </b>scaling introduced by:<br><img src=""Screen Shot 2019-05-31 at 17.26.08.png"">"	"<div>Scale the <b>width</b>, the <b>depth</b>, and the <b>resolution </b>under the constraints:<br><img src=""Screen Shot 2019-05-31 at 17.26.59.png""><br><br></div><img src=""Screen Shot 2019-05-31 at 17.25.47.png"">"
"<b>optimization </b>metrics to search for <b>EfficientNet</b>:<br><img src=""Screen Shot 2019-05-31 at 17.26.08.png"">"	"Accuracy &amp; FLOPS<br><br><img src=""Screen Shot 2019-05-31 at 17.28.07.png"">"
"main <b>building block </b>of <b>EfficientNet</b>:<br><img src=""Screen Shot 2019-05-31 at 17.26.08.png"">"	"Mobile Inverted Bottleneck<br><br><img src=""Screen Shot 2019-05-31 at 17.31.55.png"">"
"what does <b>Multi-Sample Dropout</b>:<div><br></div><div><img src=""Screen Shot 2019-05-31 at 17.38.38.png""><br></div>"	"<div><b>Forward twice </b>in the FC layers with <b>two different Dropout masks</b>.<br><br>Losses are then <b>averaged</b>.</div><img src=""Screen Shot 2019-05-31 at 17.38.30.png"">"
<b>Hard Parameter Sharing </b>in multi-tasks learning	"<img src=""mtl_images-001-2.png"">"
<b>Soft Parameter Sharing </b>in multi-tasks learning	"<div><b>Constrained</b> layers are forced to be <b>similar </b>but not necessarily equal.</div><img src=""mtl_images-002-1.png"">"
"How <b>Memory Aware Synapses</b>&nbsp;avoid catastrophic forgetting?<br><br><img src=""Screen Shot 2019-06-02 at 16.46.39.png"">"	"Computes an <b>importance metric for each weights</b> (as EWC).&nbsp;<div><br></div><div><img src=""Screen Shot 2019-06-02 at 16.48.11.png""><br></div>"
"what is the <b>importance metric </b>\(\Omega\) from <b>Memory Aware Synapses</b>?<br><img src=""Screen Shot 2019-06-02 at 16.46.39.png""><br><img src=""Screen Shot 2019-06-02 at 16.48.11.png"">"	"Measures how much a <b>small perturbation in the weights </b>can affect the network output \(F\):<br><br><img src=""Screen Shot 2019-06-02 at 16.50.42.png""><br><img src=""Screen Shot 2019-06-02 at 16.51.09.png"">"
"How does <b>Task-Free Continual Learning </b>know when to update the <b>weight importance metrics</b>?<br><br><img src=""Screen Shot 2019-06-02 at 16.51.35.png"">"	"When the <b>loss in on a plateau</b>. Aka when the model is confident with the current samples.<br><br><img src=""Screen Shot 2019-06-02 at 16.52.22.png"">"
What is <b>Cumulative Moving Average</b>?	Keep a <b>buffer of \(N\) previous values</b>, and make an <b>average</b>&nbsp;of it.
<b>Rademacher complexity</b> of a model 	The model capacity to fit <b>random noise</b>.<br><br>It is preferable that the model's <b>representativeness is way inferior to this complexity</b>.  
The model capacity to fit <b>random noise</b>.<br><br>It is preferable that the model's <b>representativeness is way inferior to this complexity</b>.	<b>Rademacher complexity</b> of a model
"Differences between <b>Gradient Episodic Memory</b>&nbsp;and <b>Online Continual Learning With No Task Boundaries</b>?<br><br><img src=""Screen Shot 2019-06-02 at 17.22.54.png"">"	"The latter <b>reduces the number of constraints</b> of GEM by selecting a more relax <b>feasible set.<br><br></b><img src=""Screen Shot 2019-06-02 at 17.23.09.png""><b><br></b>"
"<b>Online Continual Learning With No Task Boundaries: </b>constraints<br><br><img src=""Screen Shot 2019-06-02 at 17.22.54.png"">"	"Buffer of <b>constraints sampled randomly</b> as <b>GEM.</b><br><br>Among those, selects <b>active constraints </b>producing a more relax feasible set.<br><br><img src=""Screen Shot 2019-06-02 at 17.23.09.png"">"
"<b>Online Continual Learning With No Task Boundaries:&nbsp;</b>how to choose the <b>active constraints</b>?<br><img src=""Screen Shot 2019-06-02 at 17.22.54.png"">"	"Find among all previous gradients (<b>constraints)&nbsp;</b>in the buffer the <b>set of gradients whose angle is maximal</b>.<br><br><img src=""Screen Shot 2019-06-02 at 17.30.18.png""><br><br>It produces the&nbsp;<b>closest feasible set</b>&nbsp;to the original feasible set.<br>See intuition in 2D:<br><img src=""Screen Shot 2019-06-02 at 17.31.02.png"">"
"what is <b>M2KD's multi-model</b>&nbsp;distillation?<div><br></div><div><img src=""Screen Shot 2019-06-10 at 19.02.15.png""><br></div>"	"Instead of distilling simply from the last model, <b>every past models contribute</b>.<div><br></div><div><img src=""Screen Shot 2019-06-10 at 19.03.16.png""><br></div>"
"what is <b>M2KD's multi-level</b>&nbsp;distillation?<div><br></div><div><img src=""Screen Shot 2019-06-10 at 19.02.15.png""><br></div>"	"They distill the knowledge from the <b>usual classifiers </b>and from a <b>dummy discriminator</b> that is <b>plugged from intermediary features</b>.<div><br></div><div><img src=""Screen Shot 2019-06-10 at 19.04.21.png""><br></div>"
"how <b>M2KD</b>&nbsp;manages to have a <b>low memory overhead</b> while distilling knowledge from <b>all previous models</b>?<div><br></div><div><img src=""Screen Shot 2019-06-10 at 19.02.15.png""><br></div><div><img src=""Screen Shot 2019-06-10 at 19.06.30.png""><br></div><div><br></div>"	"They <b>saved only parts</b> of previous models using masks.<div><br></div><div>The <b>weights that had the biggest magnitude</b>.</div><div><br></div><div><img src=""Screen Shot 2019-06-10 at 19.07.39.png""><br></div>"
Design pattern:<div><br></div><div><span class=cloze>[]</span> ensures that resources are properly <b>released</b> by tying them to the <b>lifespan</b> of suitable objects.<br></div>	"Design pattern:<div><br></div><div><span class=cloze><b>Resource acquisition is initialization</b> (RAII) </span> ensures that resources are properly <b>released</b> by tying them to the <b>lifespan</b> of suitable objects.<br></div><br><br> <center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%""> 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;mutex&gt;</span><span style=""color: #BC7A00""></span> <span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;iostream&gt;</span><span style=""color: #BC7A00""></span> <span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;string&gt; </span><span style=""color: #BC7A00""></span> <span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;fstream&gt;</span><span style=""color: #BC7A00""></span> <span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;stdexcept&gt;</span><span style=""color: #BC7A00""></span>  <span style=""color: #B00040"">void</span> <span style=""color: #0000FF"">write_to_file</span> (<span style=""color: #008000; font-weight: bold"">const</span> std<span style=""color: #666666"">::</span>string <span style=""color: #666666"">&amp;</span> message) {     <span style=""color: #408080; font-style: italic"">// mutex to protect file access (shared across threads)</span>     <span style=""color: #008000; font-weight: bold"">static</span> std<span style=""color: #666666"">::</span>mutex mutex;      <span style=""color: #408080; font-style: italic"">// lock mutex before accessing file</span>     std<span style=""color: #666666"">::</span>lock_guard<span style=""color: #666666"">&lt;</span>std<span style=""color: #666666"">::</span>mutex<span style=""color: #666666"">&gt;</span> lock(mutex);      <span style=""color: #408080; font-style: italic"">// try to open file</span>     std<span style=""color: #666666"">::</span>ofstream file(<span style=""color: #BA2121"">""example.txt""</span>);     <span style=""color: #008000; font-weight: bold"">if</span> (<span style=""color: #666666"">!</span>file.is_open())         <span style=""color: #008000; font-weight: bold"">throw</span> std<span style=""color: #666666"">::</span>runtime_error(<span style=""color: #BA2121"">""unable to open file""</span>);          <span style=""color: #408080; font-style: italic"">// write message to file</span>     file <span style=""color: #666666"">&lt;&lt;</span> message <span style=""color: #666666"">&lt;&lt;</span> std<span style=""color: #666666"">::</span>endl;          <span style=""color: #408080; font-style: italic"">// file will be closed 1st when leaving scope (regardless of exception)</span>     <span style=""color: #408080; font-style: italic"">// mutex will be unlocked 2nd (from lock destructor) when leaving</span>     <span style=""color: #408080; font-style: italic"">// scope (regardless of exception)</span> } </pre></div> </td></tr></tbody></table></center>"
Design pattern:<div><br></div><div><b>Resource acquisition is initialization</b> (RAII)  <span class=cloze>[]</span>.<br></div>	"Design pattern:<div><br></div><div><b>Resource acquisition is initialization</b> (RAII)  <span class=cloze>ensures that resources are properly <b>released</b> by tying them to the <b>lifespan</b> of suitable objects</span>.<br></div><br><br> <center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%""> 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;mutex&gt;</span><span style=""color: #BC7A00""></span> <span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;iostream&gt;</span><span style=""color: #BC7A00""></span> <span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;string&gt; </span><span style=""color: #BC7A00""></span> <span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;fstream&gt;</span><span style=""color: #BC7A00""></span> <span style=""color: #BC7A00"">#include</span> <span style=""color: #408080; font-style: italic"">&lt;stdexcept&gt;</span><span style=""color: #BC7A00""></span>  <span style=""color: #B00040"">void</span> <span style=""color: #0000FF"">write_to_file</span> (<span style=""color: #008000; font-weight: bold"">const</span> std<span style=""color: #666666"">::</span>string <span style=""color: #666666"">&amp;</span> message) {     <span style=""color: #408080; font-style: italic"">// mutex to protect file access (shared across threads)</span>     <span style=""color: #008000; font-weight: bold"">static</span> std<span style=""color: #666666"">::</span>mutex mutex;      <span style=""color: #408080; font-style: italic"">// lock mutex before accessing file</span>     std<span style=""color: #666666"">::</span>lock_guard<span style=""color: #666666"">&lt;</span>std<span style=""color: #666666"">::</span>mutex<span style=""color: #666666"">&gt;</span> lock(mutex);      <span style=""color: #408080; font-style: italic"">// try to open file</span>     std<span style=""color: #666666"">::</span>ofstream file(<span style=""color: #BA2121"">""example.txt""</span>);     <span style=""color: #008000; font-weight: bold"">if</span> (<span style=""color: #666666"">!</span>file.is_open())         <span style=""color: #008000; font-weight: bold"">throw</span> std<span style=""color: #666666"">::</span>runtime_error(<span style=""color: #BA2121"">""unable to open file""</span>);          <span style=""color: #408080; font-style: italic"">// write message to file</span>     file <span style=""color: #666666"">&lt;&lt;</span> message <span style=""color: #666666"">&lt;&lt;</span> std<span style=""color: #666666"">::</span>endl;          <span style=""color: #408080; font-style: italic"">// file will be closed 1st when leaving scope (regardless of exception)</span>     <span style=""color: #408080; font-style: italic"">// mutex will be unlocked 2nd (from lock destructor) when leaving</span>     <span style=""color: #408080; font-style: italic"">// scope (regardless of exception)</span> } </pre></div> </td></tr></tbody></table></center>"
"what is <b>Model Consolidation</b>&nbsp;in the following paper:<div><br></div><div><img src=""Screen Shot 2019-06-11 at 16.10.00.png""><br></div>"	"A new random model is used as <b>student of the concatenation of old and new models</b> output.<div><br></div><div><img src=""Screen Shot 2019-06-11 at 16.10.06.png""><br></div><div><br></div><div><i>It must match both teachers predictions using an L2 loss.</i></div>"
"what is the <b>matching metric</b>&nbsp;when training the <b>Model Consolidation </b>of the student based on the two teacher?<div><br></div><div><img src=""Screen Shot 2019-06-11 at 16.10.00.png""><br></div><div><br></div><div><img src=""Screen Shot 2019-06-11 at 16.10.06.png""><br></div>"	"An <b>L2 loss</b>.<div><br></div><div><img src=""paste-e25ab6b8866be8cdf9541185fc0e181920906255.jpg""><br></div>"
"on <b>what data</b> is trained the student of <b>Model Consolidation </b>from the two teachers (old and new models) concatenation?<div><img src=""Screen Shot 2019-06-11 at 16.10.00.png""><br></div><div><br></div>"	"On <b>unlabeled data</b>.<div><br></div><div><img src=""Screen Shot 2019-06-11 at 16.10.06.png""><br></div><div><br></div><div><i>The student only have to match the two teachers predictions, not any actual ground-truth.</i></div>"
"what is <b>Large Scale Incremental Learning </b>innovation?<br><br><div><img src=""Screen Shot 2019-06-12 at 22.42.31.png""><br></div>"	"They add a <b>bias correction</b>&nbsp;for the logits of the <b>last classes</b>.<div><br></div><div><img src=""Screen Shot 2019-06-12 at 22.43.43.png""><br></div><div><br></div><div><img src=""Screen Shot 2019-06-12 at 22.44.01.png""><br></div>"
"what is Large Scale Incremental Learning's <b>bias correction</b>?<div><br></div><div><img src=""Screen Shot 2019-06-12 at 22.42.31.png""><br></div><div><img src=""Screen Shot 2019-06-12 at 22.43.43.png""><br></div>"	"A <b>two scalar parameters</b> that are learned on the validation.<div><br></div><div><img src=""Screen Shot 2019-06-12 at 22.44.01.png""><br></div><div><br></div><div><i>It actually correct a miscalibration problem.</i></div>"
"what is the <b>intuition </b>behing Large Scale Incremental Learning's <b>bias correction</b>?<br><br><div><img src=""Screen Shot 2019-06-12 at 22.42.31.png""><br></div><div><img src=""Screen Shot 2019-06-12 at 22.43.43.png""><br></div>"	"The last FC <b>classifier is biased to the new classes</b> as the weights are not shared across classes.<div><br></div><div><img src=""Screen Shot 2019-06-12 at 22.48.37.png""><br></div>"
what is in lifelong-learning the evaluation setting <b>Single-Head</b>?	The model is <b>evaluated on all seen tasks</b> but isn't informed from which tasks are in-coming samples.
what is in lifelong-learning the evaluation setting&nbsp;<b>Multi-Heads</b>?	The model is <b>evaluated on all tasks separately</b>. It is informed beforehand of the task of the in-coming sample.<div><br></div><div><i>This setting is less realist as we don't know which task the sample belongs to in real-life.</i>&nbsp;</div>
"what is the intuition behind <b>Learning to Grow</b>?<div><br></div><div> <div> <div> <div><img src=""Screen Shot 2019-06-15 at 16.23.03.png""><br></div> </div> </div></div>"	"<b>Each task should have its own architecture</b>.<div><br></div><div>To do so they have a base model, and modify it lightly for each task using <b>Neural Architecture Search</b>:</div><div><br></div><div><img src=""Screen Shot 2019-06-15 at 16.25.21.png""><br></div>"
"what is the key idea of <b>Born-Again Networks</b>?<div><br></div><div><img src=""Screen Shot 2019-06-16 at 17.52.51.png""><br></div>"	"Iterative use of <b>teacher/student training</b> where the <b>student become the next teacher</b>.<div><br></div><div>Then all models are used together as an <b>ensemble</b>.</div><div><br></div><div><img src=""Screen Shot 2019-06-16 at 17.52.57.png""><br></div><div><br></div><div><i>All models are of identical capacity.</i></div>"
"how does <b>Born-Again Networks </b>combine the models predictions?<div><img src=""paste-dbb97efae2a2f5910a3a444d78ea940dec3e8ef6.jpg""><br></div><div><img src=""Screen Shot 2019-06-16 at 17.52.57.png""><br></div><div><br></div>"	"Simply <b>average them</b>.<div><br></div><div><img src=""Screen Shot 2019-06-16 at 17.55.55.png""><br></div>"
"what is the <b>use-case </b>of <b>Residual Adapters</b>?<div><img src=""Screen Shot 2019-06-16 at 18.49.30.png""><br></div>"	"<b>Learning different datasets with different domains</b> while keeping most of the main model identical.<div><br></div><div><img src=""Screen Shot 2019-06-16 at 18.50.28.png""><br></div>"
"what is <b>Residual Adapters </b>that are used to learn several datasets/domains with a same base model?<div><br></div><div><img src=""Screen Shot 2019-06-16 at 18.49.30.png""><br></div>"	"Add <b>Adapters</b>&nbsp;at each convolution of the model.<div><br></div><div>Made of a <b>Pointwise convolution </b>and <b>batch normalizations</b>.</div><div><br></div><div><img src=""Screen Shot 2019-06-16 at 18.51.23.png""><br></div><div>Only the <b>dataset specific (<i>in blue</i>) layer are fine-tuned</b>. While the others layers are fixed from an ImageNet pre-training.</div>"
"what is <b>Grad-CAM </b>goal?<div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.13.59.png""><br></div>"	"A way of <b>displaying the zones of importance</b> of an image.<div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.13.32.png""><br></div>"
"how does <b>Grad-CAM </b>work?<div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.13.59.png""><br></div>"	"Computes the <b>derivatives </b>from each class for each <b>channel maps</b>.<div><br></div><div>Then apply <b>ReLU </b>on them.<br><div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.13.32.png""><br></div></div>"
"what is <b>Learning without Memorizing </b>innovation?<div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.16.01.png""><br></div>"	"Adding a <b>loss on the gradients between teacher &amp; student</b> using grad-cam.<div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.17.06.png""><br></div>"
"for the loss added by <b>Learning without Memorizing</b>, on what is applied the <b>distance between teacher and student</b>?<div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.16.01.png""><br></div>"	"On <b>Grad-CAM</b>. Gradients followed by a ReLU.<div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.17.06.png""><br></div>"
"why <b>Grad-CAM loss </b>of <b>Learning without Memorizing</b>&nbsp;is adding extra info compared to the distillation loss?<div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.16.01.png""><br></div><div><img src=""Screen Shot 2019-06-25 at 18.17.06.png""><br></div><div><br></div>"	"Because the distillation loss focuses on <b>what</b>.<div><br></div><div>While the Grad-CAM loss focuses on <b>why</b>.</div><div><br></div><div><img src=""Screen Shot 2019-06-25 at 18.19.50.png""><br></div>"
"visualize the <b>imbalanced magnitudes </b>problem described by:<div><br></div><div><img src=""Screen Shot 2019-06-27 at 18.29.42.png""><br></div>"	"<img src=""Screen Shot 2019-06-28 at 17.37.24.png"">"
"visualize the <b>deviation </b>problem described by:<div><br></div><div><img src=""Screen Shot 2019-06-28 at 17.36.38.png""><br></div>"	"<img src=""Screen Shot 2019-06-28 at 17.37.45.png"">"
"visualize the <b>ambiguities </b>problem described by:<div><br></div><div><img src=""Screen Shot 2019-06-28 at 17.36.38.png""><br></div>"	"<img src=""Screen Shot 2019-06-28 at 17.38.43.png"">"
"how does the following paper solved the <b>imbalanced magnitudes </b>problem?<div><br></div><div><img src=""Screen Shot 2019-06-28 at 17.36.38.png""><br></div><div><img src=""Screen Shot 2019-06-28 at 17.37.24.png""><br></div>"	"The last layer is a <b>cosine similarity + softmax</b>:<div><br></div><div><img src=""Screen Shot 2019-06-28 at 17.40.41.png""><br></div><div><i>\(\eta\) being a learned scalar.</i></div><div><i><br></i></div><div><img src=""paste-6cffe6aae97a45bcb8d9729d6e84597ec9ed9a42.jpg""><i><br></i></div>"
"how does the following paper solved the <b>deviation </b>problem?<div><br></div><div><img src=""Screen Shot 2019-06-28 at 17.36.38.png""><br></div><div><img src=""Screen Shot 2019-06-28 at 17.37.45.png""><br></div>"	"By forcing the old &amp; new <b>normalized features to be in the same direction</b>.<div><br></div><div>&nbsp;<img src=""Screen Shot 2019-06-28 at 17.47.36.png""></div><div><br></div><div><img src=""Screen Shot 2019-06-28 at 17.47.08.png""><br></div><div><br></div>"
"how does the following paper solve the <b>ambiguities </b>problem (also known as confusion)?<div><br></div><div><img src=""Screen Shot 2019-06-27 at 18.29.42.png""><br></div><div><img src=""Screen Shot 2019-06-28 at 17.38.43.png""><br></div><div><br></div>"	"Using a <b>ranking loss</b>&nbsp;with a <b>margin</b>.<div><img src=""Screen Shot 2019-07-01 at 13.24.26.png""><br></div><div>The <b>positives are ground-truth classes</b>, and the <b>negatives are new classes</b> that were <b>classified confidentely as old classes</b>.</div><div><br></div><div>There are <b>K </b>negatives.</div><div><br></div><div><img src=""Screen Shot 2019-06-28 at 17.47.20.png""><br></div>"
"what is the <b>positive \(\theta\) weight</b>&nbsp;in the <b>ranking loss </b>of the following paper?<div><img src=""Screen Shot 2019-06-27 at 18.29.42.png""><br></div><div><img src=""Screen Shot 2019-07-01 at 13.24.26.png""><br></div>"	The <b>ground-truth class </b>associated weight.<div><br></div><div>Only old classes.</div>
"what are the&nbsp;<b>K negative examples</b>&nbsp;in the&nbsp;<b>ranking loss&nbsp;</b>of the following paper?<div><img src=""Screen Shot 2019-06-27 at 18.29.42.png""><br></div><div><img src=""Screen Shot 2019-07-01 at 13.24.26.png""></div>"	Among <b>new classes</b>, the classes that had the highest confidence.
"what is the algorithm behind&nbsp;<b>Meta-Experience Replay</b>?<div><img src=""Screen Shot 2019-07-13 at 18.41.20.png""><br></div>"	"A lifelong-learning algo that uses <b>Reptile meta-learning algorithm.</b><div><br></div><div><img src=""paste-1ae4439e3f2802595613fe07bd6bffeaace3bac0.jpg""><br></div>"
"what is a <b>batch </b>in <b>Meta-Experience Replay</b>?<div><img src=""Screen Shot 2019-07-13 at 18.41.20.png""><br></div>"	"A batch is made of a <b>single sample from the new task</b>.<div><br></div><div>Several batches are then made by <b>mixing it with samples from the memory</b>.</div><div><br></div><div><img src=""Screen Shot 2019-07-13 at 18.42.53.png""><br></div>"
what is usually the matrix M in the <b>Mahalanobis distance</b>?<div><br></div><div>\((x - y)^T M (x - y)\)</div>	The <b>inverse of the covariance matrix </b>of \(y\).
<div>what is the intuition behind using the <b>inverse of the covariance </b>matrix \(S^{-1}\) in <b>Mahalanobis distance</b>?</div><div><br></div><div>\((x - y)^T S^{-1} (x - y)\)</div>	If the matrix is diagonal:<div><br></div><div><b>Dimensions with high variance (thus less reliable) will be less important</b> in the distance measure.</div>
what is the&nbsp;<b>Mahalanobis distance </b>if \(M\) is the <b>identity</b> matrix?<div><br></div><div>\((x - y)^T M (x - y)\)</div>	An <b>L2 euclidian distance</b>.
<b>hamming</b> distance 	"Distance counting the <b>number of different symbols</b>:<div><br></div><div><img src=""Screen Shot 2019-07-25 at 14.34.09.png""><br></div>  "
"Distance counting the <b>number of different symbols</b>:<div><br></div><div><img src=""Screen Shot 2019-07-25 at 14.34.09.png""><br></div>"	<b>hamming</b> distance
<b>Levenshtein</b> distance 	"Distance between two words measuring the <b>minimum number of single-character edits</b> (insertions, deletions or substitutions) required to change one word into the other.<br><div><br></div><div><img src=""Screen Shot 2019-07-25 at 14.36.26.png""><br></div>  "
"Distance between two words measuring the <b>minimum number of single-character edits</b> (insertions, deletions or substitutions) required to change one word into the other.<br><div><br></div><div><img src=""Screen Shot 2019-07-25 at 14.36.26.png""><br></div>"	<b>Levenshtein</b> distance
why <b>Mahalanobis distance</b>&nbsp;can be seen as an <b>Euclidian distance</b> with <b>linear projection</b>?	"<img src=""Screen Shot 2019-07-25 at 14.52.04.png""><br><div>With \(M = L^TL\):</div><div><br></div><div><img src=""Screen Shot 2019-07-25 at 14.52.15.png""><br></div>"
"what is the notion of <b>Relative Teacher</b>&nbsp;used for distillation in metric learning?<div><br></div><div><img src=""Screen Shot 2019-07-27 at 12.55.56.png""><br></div>"	"Instead of computing the <b>embeddings difference:</b><div><img src=""Screen Shot 2019-07-27 at 12.58.09.png""><br></div><div><br></div><div>The relative teacher computes <b>distance difference</b>:<div><img src=""Screen Shot 2019-07-27 at 12.58.27.png""><br></div><div><img src=""paste-eee1f6c6b799909ee7bda4951f998567a4472377.jpg""><br></div></div><div><br></div><div><img src=""Screen Shot 2019-07-27 at 12.57.10.png""><br></div>"
"what are <b>hints </b>in <b>knowledge distillation</b>?<div><img src=""Screen Shot 2019-07-27 at 13.00.05.png""><br></div>"	"Computes the <b>difference of embeddings </b>at <b>several layers </b>between the teacher and the student:<div><br></div><div><img src=""Screen Shot 2019-07-27 at 12.59.10.png""><br></div><div><img src=""Screen Shot 2019-07-27 at 12.59.20.png""><br></div>"
"what is the <b>attention distillation </b>of the following paper?<div><br></div><div><img src=""paste-218d783f7f7fbcb3a8af3b75b9eed442b3d1f2d5.jpg""><br></div>"	"Computes <b>normalized activation maps difference</b>&nbsp;between the teacher and the student at <b>several layers</b>:<div><br></div><div><img src=""Screen Shot 2019-07-27 at 13.02.21.png""><br></div><div><img src=""paste-051a77e3d5e072ee3297817a3520812878415b15.jpg""><br></div><div><br></div><div><img src=""Screen Shot 2019-07-27 at 12.59.10.png""><br></div>"
what is <b>N-Pair loss</b>?	"Generalizes the triplet loss, <b>where every pairs must be push further from all other pairs.</b><div><b><br></b></div><div><img src=""Screen Shot 2019-07-27 at 16.14.00.png""><b><br></b></div>"
what is <b>Angular Loss</b>?	"Similar to the <b>N-Pair loss</b>, but instead of manipulating distances, the loss manipulates an <b>angle</b>.<div><br></div><div>The <b>angle of the negative with the positive and the anchors must be bounded</b>.<br><div><br></div><div><img src=""paste-01625da6acda2d072333c326f566be021fb42b32.jpg""><br></div><div><br></div></div>"
"what is the idea of <b>Hardness-Aware metric learning</b>?<div><img src=""paste-bea042b817ed1874ada49404e3931e1ed2f045e5.jpg""><br></div>"	"Metric learning needs hard sampling. Instead we can <b>transform easy negatives into hard negatives</b>.<div><br></div><div><img src=""Screen Shot 2019-07-27 at 16.23.43.png""><br></div>"
"Explains what happens in that image:<div><img src=""Screen Shot 2019-07-27 at 16.23.50.png""><br></div><div><img src=""Screen Shot 2019-07-27 at 16.23.43.png""><br></div><div><br></div>"	\(y^{-}\) is transformed as \(\hat{y}^{-}\) with an <b>interpolation</b> with \(y\).<div><br></div><div>Then \(\hat{y}^{-}\) is <b>mapped back to the negative manifold</b> in \(\tilde{y}^{-}\) used an <b>autoencoder</b> to ensure that the negative label can still be discriminated.</div>
"what is the <b>intuition </b>behind the <b>magnet loss</b>?<div><img src=""Screen Shot 2019-07-27 at 16.56.53.png""><br></div>"	"<div>Classic metric learning considers label as notion of similarity, thus collapsing a whole class into a single cluster.</div><div style=""font-weight: bold;""><b><br></b></div>However, a<b>&nbsp;class can be made of several modes</b>.<div><br></div><div><img src=""paste-6cb76eb9514ffd80767e5353c12eb4d6bd927648.jpg""><br></div>"
Space where a <b>triangle's</b>&nbsp;<b>angle sum </b>is <span class=cloze>[]</span> than 180° have a <b>negative</b> <b>curvature.</b>	"Space where a <b>triangle's</b>&nbsp;<b>angle sum </b>is <span class=cloze>less</span> than 180° have a <b>negative</b> <b>curvature.</b><br><br> <img src=""Screen Shot 2019-07-28 at 21.38.04.png"">"
Space where a <b>triangle's</b>&nbsp;<b>angle sum </b>is less than 180° have a <span class=cloze>[]</span> <b>curvature.</b>	"Space where a <b>triangle's</b>&nbsp;<b>angle sum </b>is less than 180° have a <span class=cloze><b>negative</b></span> <b>curvature.</b><br><br> <img src=""Screen Shot 2019-07-28 at 21.38.04.png"">"
Space where a&nbsp;<b>triangle's</b>&nbsp;<b>angle sum&nbsp;</b>is <span class=cloze>[]</span> than 180° have a <b>positive</b>&nbsp;<b>curvature.</b>	"Space where a&nbsp;<b>triangle's</b>&nbsp;<b>angle sum&nbsp;</b>is <span class=cloze>more</span> than 180° have a <b>positive</b>&nbsp;<b>curvature.</b><br><br> <img src=""Screen Shot 2019-07-28 at 21.35.38.png"">"
Space where a&nbsp;<b>triangle's</b>&nbsp;<b>angle sum&nbsp;</b>is more than 180° have a <span class=cloze>[]</span>&nbsp;<b>curvature.</b>	"Space where a&nbsp;<b>triangle's</b>&nbsp;<b>angle sum&nbsp;</b>is more than 180° have a <span class=cloze><b>positive</b></span>&nbsp;<b>curvature.</b><br><br> <img src=""Screen Shot 2019-07-28 at 21.35.38.png"">"
<span class=cloze>[]</span> spaces are also called spaces of <b>negative</b> <b>curvature</b>.	"<span class=cloze><b>Hyperbolic</b></span> spaces are also called spaces of <b>negative</b> <b>curvature</b>.<br><br> <img src=""Screen Shot 2019-07-28 at 21.39.44.png"">"
<b>Hyperbolic</b> spaces are also called spaces of <span class=cloze>[]</span> <b>curvature</b>.	"<b>Hyperbolic</b> spaces are also called spaces of <span class=cloze><b>negative</b></span> <b>curvature</b>.<br><br> <img src=""Screen Shot 2019-07-28 at 21.39.44.png"">"
"Algo of <b>Magnet Loss</b>?<div><img src=""Screen Shot 2019-07-27 at 16.56.53.png""><br></div><div><br></div><div>1. <span class=cloze>[]</span></div><div>2. On a subsample of clusters and a subsample of clusters' data, minimizes intra-clusters distance and maximize inter-clusters distance.</div><div>3. Regularly re-compute the clusters using KMeans.</div>"	"Algo of <b>Magnet Loss</b>?<div><img src=""Screen Shot 2019-07-27 at 16.56.53.png""><br></div><div><br></div><div>1. <span class=cloze>Find initial clusters (several per class) using KMeans.</span></div><div>2. On a subsample of clusters and a subsample of clusters' data, minimizes intra-clusters distance and maximize inter-clusters distance.</div><div>3. Regularly re-compute the clusters using KMeans.</div><br><br> <img src=""Screen Shot 2019-07-28 at 22.00.19.png""><br><div><img src=""Screen Shot 2019-07-28 at 22.00.10.png""><br></div>"
"Algo of <b>Magnet Loss</b>?<div><img src=""Screen Shot 2019-07-27 at 16.56.53.png""><br></div><div><br></div><div>1. Find initial clusters (several per class) using KMeans.</div><div>2. <span class=cloze>[]</span></div><div>3. Regularly re-compute the clusters using KMeans.</div>"	"Algo of <b>Magnet Loss</b>?<div><img src=""Screen Shot 2019-07-27 at 16.56.53.png""><br></div><div><br></div><div>1. Find initial clusters (several per class) using KMeans.</div><div>2. <span class=cloze>On a subsample of clusters and a subsample of clusters' data, minimizes intra-clusters distance and maximize inter-clusters distance.</span></div><div>3. Regularly re-compute the clusters using KMeans.</div><br><br> <img src=""Screen Shot 2019-07-28 at 22.00.19.png""><br><div><img src=""Screen Shot 2019-07-28 at 22.00.10.png""><br></div>"
"Algo of <b>Magnet Loss</b>?<div><img src=""Screen Shot 2019-07-27 at 16.56.53.png""><br></div><div><br></div><div>1. Find initial clusters (several per class) using KMeans.</div><div>2. On a subsample of clusters and a subsample of clusters' data, minimizes intra-clusters distance and maximize inter-clusters distance.</div><div>3. <span class=cloze>[]</span></div>"	"Algo of <b>Magnet Loss</b>?<div><img src=""Screen Shot 2019-07-27 at 16.56.53.png""><br></div><div><br></div><div>1. Find initial clusters (several per class) using KMeans.</div><div>2. On a subsample of clusters and a subsample of clusters' data, minimizes intra-clusters distance and maximize inter-clusters distance.</div><div>3. <span class=cloze>Regularly re-compute the clusters using KMeans.</span></div><br><br> <img src=""Screen Shot 2019-07-28 at 22.00.19.png""><br><div><img src=""Screen Shot 2019-07-28 at 22.00.10.png""><br></div>"
"how is <b>inference </b>for the <b>Magnet Loss</b>?<div><img src=""Screen Shot 2019-07-27 at 16.56.53.png""><br></div>"	<b>Find the \(L\) closest clusters</b>, and then among them find the <b>most probable class</b>.
"what is this <b>kernel operator </b>name?<div><br></div><div><img src=""Screen Shot 2019-08-04 at 13.29.02.png""><br></div>"	"<b>Sobel</b>&nbsp;operator, to detect edges.<div><br></div><div><img src=""300px-Valve_original_(1).PNG""><img src=""300px-Valve_sobel_(3).PNG""><br></div>"
"what does the <b>Sobel </b>kernel operator do?<div><img src=""paste-d85a3d3a5d7d4eda112313f076f3d3d735cdc997.jpg""><br></div>"	"<b>Detect edges</b>.<div><br></div><div>It detects the <b>rate of change</b>&nbsp;over the x-axis or y-axis on a <b>smoothed image</b>.</div><div><br></div><div><img src=""300px-Valve_original_(1).PNG""><img src=""300px-Valve_sobel_(3).PNG""><br></div>"
"How can we <b>decompose </b>the edge detector kernel <b>Sobel</b>?<div><img src=""Screen Shot 2019-08-04 at 13.29.02.png""><br></div>"	"As the <b>products of an averaging and a differentiation kernel</b>, they compute the gradient with smoothing.<br><div><br></div><div><img src=""Screen Shot 2019-08-04 at 13.35.09.png""><br></div>"
"how can we <b>combine </b>the gradients produced by the <b>Sobel </b>operators of the x-axis and y-axis into a <b>gradient magnitude</b>?<div><img src=""Screen Shot 2019-08-04 at 13.29.02.png""><br></div>"	"We can combine the <b>two gradients </b>into a <b>gradient magnitude</b>:<div><img src=""Screen Shot 2019-08-04 at 13.39.30.png""><br></div>"
"Into what usually are&nbsp;gradients produced by the&nbsp;<b>Sobel&nbsp;</b>operators of the x-axis and y-axis <b>combined into</b>?<div><img src=""Screen Shot 2019-08-04 at 13.29.02.png""></div>"	"<b>Gradient magnitude</b>.<div><br></div><div><img src=""Screen Shot 2019-08-04 at 13.39.30.png""><br></div>"
"how can we <b>compute the gradient direction</b>&nbsp;from the transformations of the <b>Sobel </b>kernel?<div><img src=""Screen Shot 2019-08-04 at 13.29.02.png""><br></div>"	"<img src=""Screen Shot 2019-08-04 at 13.42.50.png""><br><div><i>\(\theta = 0\) for a vertical edge as \(G_y\) will be 0 (no difference with the bottom &amp; upper pixel).</i></div>"
what is <b>Difference of Gaussian </b>(DoG)?	"<div>Given an image at <b>various scales</b> (<i>octave</i>), computes the <b>difference between the same image blurred with different \(\sigma\).</b></div><div><br></div><img src=""Screen Shot 2019-08-04 at 14.20.09.png"">"
what is an <b>octave </b>in <b>Difference of Gaussian </b>(DoG)?	"<b>Scale of the image.</b><div><br></div><div>First octave is original image, second is half the size, etc.</div><div><br></div><div><img src=""Screen Shot 2019-08-04 at 14.20.09.png""><br></div>"
why using <b>Difference of Gaussian </b>before applying <b>SIFT</b>?	"To gain <b>scale invariance</b>. It also helps to detect <b>potential keypoints</b>.<div><br></div><div><img src=""Screen Shot 2019-08-04 at 14.20.09.png""><br></div>"
"how to detect <b>potential keypoints </b>after <b>Difference of Gaussian</b>&nbsp;(DoG)?<div><img src=""Screen Shot 2019-08-04 at 14.20.09.png""><br></div>"	"<b>Compare each pixel with its surrounding</b> and those <b>above &amp; under the scale.</b><div><br></div><div>If it's a <b>local extrema</b>, it's a potential keypoint.</div><div><img src=""Screen Shot 2019-08-04 at 14.24.35.png""><br></div><div><br></div>"
how to get the <b>SIFT keypoint descriptors</b>?	"The image is <b>split into patches and regions</b>.<div><div><br></div><div>For each regions' pixels, compute the <b>gradients magnitude &amp; orientation</b>.</div><div><br></div><div><b>Discretize the gradients orientation</b> for a same region.</div><div><br></div><div><b>Magnitudes are summed per bin</b> in the histogram.</div><div><br></div><div><img src=""Screen Shot 2019-08-04 at 16.13.43.png""><br></div></div><div><i>The whole square is a patch of the image, sub-squares are regions, and sub-sub-squares are pixels.</i></div>"
"why <b>normalizing the SIFT representation </b>of a patch at the end?<div><img src=""Screen Shot 2019-08-04 at 16.19.06.png""><br></div>"	To make it <b>invariant to affine changes in illumination</b>.<div><br></div><div><i>A change in image contrast in which each pixel value is multiplied by a constant will multiply gradients by the same constant, so this contrast change will be canceled by vector normalization.</i></div>
"how is&nbsp;<b>SIFT </b>invariant to <b>linear changes of illumination</b>?<div><img src=""Screen Shot 2019-08-04 at 16.19.06.png""><br></div>"	The keypoint descriptors are <b>L2 normalized</b>.
"how is&nbsp;<b>SIFT&nbsp;</b>invariant to&nbsp;<b>NON-linear changes of illumination</b>?<div><img src=""Screen Shot 2019-08-04 at 16.19.06.png""></div>"	It can cause a large change in relative magnitudes for some gradients, but are less likely to affect the gradient orientations.<br><div><br></div><div>Therefore the <b>magnitude of large gradients are thresholded</b>.</div><div><br></div><div><i>As consequence the magnitudes have less importance than distribution of orientations.</i></div>
"what does <b>KMeans </b>try to optimize?<div><img src=""hqdefault.jpg""><br></div>"	"It tries to <b>minimize the pairwise distance between the points and the centers.</b><div><b><br></b></div><div><img src=""Screen Shot 2019-08-04 at 16.33.59.png""><b><br></b></div>"
"describe the <b>KMeans </b>algo:<div><img src=""hqdefault.jpg""><br></div>"	1. Init \(M\) centers at by <b>randomly sampling points</b>.<div><br><div>2. Until convergence, <b>assign each point to its closest centers.</b></div></div><div><b><br></b></div><div>3. Recompute centers as <b>the mean of their assignated points</b>.</div>
what is the <b>Elbow method </b>for?	A method to find the <b>optimal number of clusters </b>for a clustering algorithm, like KMeans.
what is the <b>Elbow method</b>&nbsp;(used to find optimal number of clusters for KMeans)?	"1. Runs KMeans with<b> various number of clusters</b>.<div><br></div><div>2. For each, plot its <b>total Within Sum of Square</b>.</div><div><br></div><div>3. Optimal number of clusters is<b> ""around"" the elbow of the curve</b>.</div><div><br></div><div><img src=""1zQUkkjBgQBEWigJVNJc9DA.png""><br></div><div><i>Here it's around 3.</i></div>"
what is the <b>Total Within Sum of Square </b>of KMeans?	"<b>The sum of squared distance to the assignated cluster center.</b><div><img src=""Screen Shot 2019-08-04 at 17.52.47.png""><b><br></b></div>"
why <b>KMeans always converge</b>?	Each iteration minimize the within sum of square.<div><br></div><div>At some point the <b>old &amp; new state between an iteration will be the same meaning that the algorithm had converged</b>.</div>
how to make a <b>visual dictionnary </b>from descriptors (like SIFT)?	"1. Extract all descriptors from all images of the dataset.<div><br></div><div>2. Compute a <b>KMeans the descriptors</b>.</div><div><br></div><div>The visual dictionnary will be the resulting <b>cluster centers</b>.</div><div><br></div><div><img src=""Screen Shot 2019-08-04 at 18.25.40.png""><br></div>"
what is the <b>Bag of Words </b>(BoW) algorithm?	"1. Produce a <b>visual dictionnary </b>from extracted <b>descriptors</b>.<div><br></div><div>2. Given an image descriptors, produce <b>one-hot vectors</b>&nbsp;for each of its descriptors based on their closest entry in the dictionnary</div><div><br></div><div><img src=""Screen Shot 2019-08-04 at 18.34.01.png""><br></div><div>3. <b>Combine the one-hot vectors</b>, like using the <b>sum</b>.</div>"
The vectors \(u\), \(v\), \(w\) are <b>linearly independant</b> iff there are <span class=cloze>[]</span></div>	The vectors \(u\), \(v\), \(w\) are <b>linearly independant</b> iff there are <span class=cloze><b>no scalar triplet</b> \((x_1, x_2, x_3) \ne (0, 0, 0)\):<div><br></div><div>\(x_1 u + x_2 v + x_2 w = 0_3\)</span></div><br><br> 
The vectors \(u\), \(v\), \(w\) are <span class=cloze>[]</span> iff there are <b>no scalar triplet</b> \((x_1, x_2, x_3) \ne (0, 0, 0)\):<div><br></div><div>\(x_1 u + x_2 v + x_2 w = 0_3\)</div>	The vectors \(u\), \(v\), \(w\) are <span class=cloze><b>linearly independant</b></span> iff there are <b>no scalar triplet</b> \((x_1, x_2, x_3) \ne (0, 0, 0)\):<div><br></div><div>\(x_1 u + x_2 v + x_2 w = 0_3\)</div><br><br> 
how can we know that \(N\) vectors are <b>linearly independent</b>&nbsp;based on the <b>determinant</b> of their concatenation \(det(u, v, w, ...)\).	if the <b>determinant is not 0</b>&nbsp;then the vectors are <b>linearly independent</b>.
what is the <b>inner product </b>of \(X=(x_1, x_2, ..., x_n)\) and \(Y=(y_1, y_2, ..., y_n)\)?	\(X \cdot Y = \sum_{i=1}^n x_i y_i\)
what does it mean if the <b>inner product </b>of two vectors is <b>zero</b>?	The two vectors are <b>orthogonal</b>, their angle is \(\frac{\pi}{2}\).
"what is the process name producing \(B\) and \(C\) from \(A\):<div><img src=""Screen Shot 2019-08-09 at 21.28.48.png""><br></div>"	<b>Low-rank decomposition</b>
what is <b>low-rank decomposition</b>?	"<b>Factorization of a big matrix</b> into two smaller matrices:<div><img src=""Screen Shot 2019-08-09 at 21.28.48.png""><br></div>"
"why do we do <b>low-rank decomposition</b>?<div><img src=""Screen Shot 2019-08-09 at 21.28.48.png""><br></div>"	To have <b>fewer parameters</b> (\(mn\) versus \(k(m + n)\)).<div><br></div><div>Can find by compression the <b>principal directions</b>, dimensions that describe well the data.</div>
what does the <b>number of bases of a vector space</b>&nbsp;correspond to?	the <b>dimension </b>of the space
<div> <div> <div> <div>Given a vector space E, a subset F of E is a <span class=cloze>[]</span> of E iff F is non-empty and <b>\(\lambda u + \mu v \in F\)</b> for all \(u, v \in F\), and all \(\lambda, \mu \in K\).</div><div><br></div><div>As example: with \(K =&nbsp;\mathbb{R}\), and \(E\) triplets \((x, y, z) \in \mathbb{R}^3\), F can be a plane \((x, y, 0)\).</div> </div> </div></div>	<div> <div> <div> <div>Given a vector space E, a subset F of E is a <span class=cloze><b>linear subspace</b></span> of E iff F is non-empty and <b>\(\lambda u + \mu v \in F\)</b> for all \(u, v \in F\), and all \(\lambda, \mu \in K\).</div><div><br></div><div>As example: with \(K =&nbsp;\mathbb{R}\), and \(E\) triplets \((x, y, z) \in \mathbb{R}^3\), F can be a plane \((x, y, 0)\).</div> </div> </div></div><br><br> 
<div> <div> <div> <div>Given a vector space E, a subset F of E is a <b>linear subspace</b> of E iff F is non-empty and <span class=cloze>[]</span> for all \(u, v \in F\), and all \(\lambda, \mu \in K\).</div><div><br></div><div>As example: with \(K =&nbsp;\mathbb{R}\), and \(E\) triplets \((x, y, z) \in \mathbb{R}^3\), F can be a plane \((x, y, 0)\).</div> </div> </div></div>	<div> <div> <div> <div>Given a vector space E, a subset F of E is a <b>linear subspace</b> of E iff F is non-empty and <span class=cloze><b>\(\lambda u + \mu v \in F\)</b></span> for all \(u, v \in F\), and all \(\lambda, \mu \in K\).</div><div><br></div><div>As example: with \(K =&nbsp;\mathbb{R}\), and \(E\) triplets \((x, y, z) \in \mathbb{R}^3\), F can be a plane \((x, y, 0)\).</div> </div> </div></div><br><br> 
<div> <div> <div> <div>Given a vector space E, a subset F of E is a <b>linear subspace</b> of E iff F is non-empty and <b>\(\lambda u + \mu v \in F\)</b> for all \(u, v \in F\), and all \(\lambda, \mu \in K\).</div><div><br></div><div>As example: <span class=cloze>[]</span>.</div> </div> </div></div>	<div> <div> <div> <div>Given a vector space E, a subset F of E is a <b>linear subspace</b> of E iff F is non-empty and <b>\(\lambda u + \mu v \in F\)</b> for all \(u, v \in F\), and all \(\lambda, \mu \in K\).</div><div><br></div><div>As example: <span class=cloze>with \(K =&nbsp;\mathbb{R}\), and \(E\) triplets \((x, y, z) \in \mathbb{R}^3\), F can be a plane \((x, y, 0)\)</span>.</div> </div> </div></div><br><br> 
"The <span class=cloze>[]</span> of a set S of vectors is defined to be the <b style="""">intersection</b>&nbsp;of all subspaces that contain S.<br><div><br></div><div>It may be defined as the set of all finite <b style="""">linear combinations</b> of elements (vectors) of S: \(\operatorname{span}(S)=\left\{\sum_{i=1}^{k} \lambda_{i} v_{i} | k \in \mathbb{N}, v_{i} \in S, \lambda_{i} \in K\right\}\).</div>"	"The <span class=cloze><b style="""">span</b></span> of a set S of vectors is defined to be the <b style="""">intersection</b>&nbsp;of all subspaces that contain S.<br><div><br></div><div>It may be defined as the set of all finite <b style="""">linear combinations</b> of elements (vectors) of S: \(\operatorname{span}(S)=\left\{\sum_{i=1}^{k} \lambda_{i} v_{i} | k \in \mathbb{N}, v_{i} \in S, \lambda_{i} \in K\right\}\).</div><br><br> <img src=""Screen Shot 2019-08-12 at 22.45.51.png"">"
"The <b style="""">span</b> of a set S of vectors is defined to be the <span class=cloze>[]</span>&nbsp;of all subspaces that contain S.<br><div><br></div><div>It may be defined as the set of all finite <b style="""">linear combinations</b> of elements (vectors) of S: \(\operatorname{span}(S)=\left\{\sum_{i=1}^{k} \lambda_{i} v_{i} | k \in \mathbb{N}, v_{i} \in S, \lambda_{i} \in K\right\}\).</div>"	"The <b style="""">span</b> of a set S of vectors is defined to be the <span class=cloze><b style="""">intersection</b></span>&nbsp;of all subspaces that contain S.<br><div><br></div><div>It may be defined as the set of all finite <b style="""">linear combinations</b> of elements (vectors) of S: \(\operatorname{span}(S)=\left\{\sum_{i=1}^{k} \lambda_{i} v_{i} | k \in \mathbb{N}, v_{i} \in S, \lambda_{i} \in K\right\}\).</div><br><br> <img src=""Screen Shot 2019-08-12 at 22.45.51.png"">"
"The <b style="""">span</b> of a set S of vectors is defined to be the <b style="""">intersection</b>&nbsp;of all subspaces that contain S.<br><div><br></div><div>It may be defined as the set of all finite <span class=cloze>[]</span>.</div>"	"The <b style="""">span</b> of a set S of vectors is defined to be the <b style="""">intersection</b>&nbsp;of all subspaces that contain S.<br><div><br></div><div>It may be defined as the set of all finite <span class=cloze><b style="""">linear combinations</b> of elements (vectors) of S: \(\operatorname{span}(S)=\left\{\sum_{i=1}^{k} \lambda_{i} v_{i} | k \in \mathbb{N}, v_{i} \in S, \lambda_{i} \in K\right\}\)</span>.</div><br><br> <img src=""Screen Shot 2019-08-12 at 22.45.51.png"">"
The <span class=cloze>[]</span>&nbsp;of a function f, written <b>supp(<i>f</i>)</b>, is the set of points in <i>X</i> where f is <b>non-zero</b>:<b>&nbsp;</b>\(\operatorname{supp}(f)=\{x \in X | f(x) \neq 0\}\).	The <span class=cloze><b>set-theoretic support</b></span>&nbsp;of a function f, written <b>supp(<i>f</i>)</b>, is the set of points in <i>X</i> where f is <b>non-zero</b>:<b>&nbsp;</b>\(\operatorname{supp}(f)=\{x \in X | f(x) \neq 0\}\).<br><br> 
The <b>set-theoretic support</b>&nbsp;of a function f, written <span class=cloze>[]</span>, is the set of points in <i>X</i> where f is <b>non-zero</b>:<b>&nbsp;</b>\(\operatorname{supp}(f)=\{x \in X | f(x) \neq 0\}\).	The <b>set-theoretic support</b>&nbsp;of a function f, written <span class=cloze><b>supp(<i>f</i>)</b></span>, is the set of points in <i>X</i> where f is <b>non-zero</b>:<b>&nbsp;</b>\(\operatorname{supp}(f)=\{x \in X | f(x) \neq 0\}\).<br><br> 
The <b>set-theoretic support</b>&nbsp;of a function f, written <b>supp(<i>f</i>)</b>, is the set of points in <i>X</i> where f is <span class=cloze>[]</span>.	The <b>set-theoretic support</b>&nbsp;of a function f, written <b>supp(<i>f</i>)</b>, is the set of points in <i>X</i> where f is <span class=cloze><b>non-zero</b>:<b>&nbsp;</b>\(\operatorname{supp}(f)=\{x \in X | f(x) \neq 0\}\)</span>.<br><br> 
"If <span class=cloze>[]</span>, then \(f\) is said to have <b style="""">finite support</b>."	"If <span class=cloze>\(f(x)&nbsp;=&nbsp;0\) for all but a <b>finite number</b> of points x&nbsp;in&nbsp;X</span>, then \(f\) is said to have <b style="""">finite support</b>.<br><br> "
If \(f(x)&nbsp;=&nbsp;0\) for all but a <b>finite number</b> of points x&nbsp;in&nbsp;X, then \(f\) is said to have <span class=cloze>[]</span>.	"If \(f(x)&nbsp;=&nbsp;0\) for all but a <b>finite number</b> of points x&nbsp;in&nbsp;X, then \(f\) is said to have <span class=cloze><b style="""">finite support</b></span>.<br><br> "
A <b>non-singular&nbsp;matrix</b> is <span class=cloze>[]</span>.	A <b>non-singular&nbsp;matrix</b> is <span class=cloze>an invertible matrix</span>.<br><br> 
A&nbsp;<b>singular&nbsp;matrix</b>&nbsp;is <span class=cloze>[]</span>.	A&nbsp;<b>singular&nbsp;matrix</b>&nbsp;is <span class=cloze>a non-invertible matrix</span>.<br><br> 
(AB)C = <span class=cloze>[]</span> shows the property of <b>associativity</b>.&nbsp;	(AB)C = <span class=cloze>A(BC)</span> shows the property of <b>associativity</b>.&nbsp;<br><br> 
(AB)C = A(BC) shows the property of <span class=cloze>[]</span>.&nbsp;	(AB)C = A(BC) shows the property of <span class=cloze><b>associativity</b></span>.&nbsp;<br><br> 
The <span class=cloze>[]</span> of a function is the set: \(\text{Ker} f = {x \in E | f(x) = 0}\).	The <span class=cloze><b>kernel </b>(or nullspace)</span> of a function is the set: \(\text{Ker} f = {x \in E | f(x) = 0}\).<br><br> 
The <b>kernel </b>(or nullspace) of a function is the set: <span class=cloze>[]</span>.	The <b>kernel </b>(or nullspace) of a function is the set: <span class=cloze>\(\text{Ker} f = {x \in E | f(x) = 0}\)</span>.<br><br> 
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""a8aed593b26c474492da4913f01dd668-oa-1-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.09.15.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""a8aed593b26c474492da4913f01dd668-oa-1-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.09.15.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""a8aed593b26c474492da4913f01dd668-oa-2-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.09.15.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""a8aed593b26c474492da4913f01dd668-oa-2-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.09.15.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""ae3a9d92ce494427a348c33a4f8d6432-oa-1-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.11.22.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""ae3a9d92ce494427a348c33a4f8d6432-oa-1-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.11.22.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""ae3a9d92ce494427a348c33a4f8d6432-oa-2-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.11.22.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""ae3a9d92ce494427a348c33a4f8d6432-oa-2-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.11.22.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""5f7dc2393e474c0cb2d2796ec2182705-oa-1-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.16.00.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""5f7dc2393e474c0cb2d2796ec2182705-oa-1-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.16.00.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""5f7dc2393e474c0cb2d2796ec2182705-oa-2-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.16.00.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""5f7dc2393e474c0cb2d2796ec2182705-oa-2-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.16.00.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
<div> <div> <div> <div>When <span class=cloze>[]</span>, a linear map f: E → F is also called an <b>endomorphism</b>.</div> </div> </div></div>	<div> <div> <div> <div>When <span class=cloze>E = F</span>, a linear map f: E → F is also called an <b>endomorphism</b>.</div> </div> </div></div><br><br> 
<div> <div> <div> <div>When E = F, a linear map f: E → F is also called an <span class=cloze>[]</span>.</div> </div> </div></div>	<div> <div> <div> <div>When E = F, a linear map f: E → F is also called an <span class=cloze><b>endomorphism</b></span>.</div> </div> </div></div><br><br> 
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""c3d22c30efe84219928b24bba53657f8-oa-1-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.35.00.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""c3d22c30efe84219928b24bba53657f8-oa-1-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.35.00.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""18eaccb4d84c451eb30e911bc51adb9e-oa-1-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.34.51.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""18eaccb4d84c451eb30e911bc51adb9e-oa-1-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.34.51.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""330e977ff0304fc09e9112f0849e8689-oa-1-Q.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.34.47.png"" /></div> </div> <div id=""io-footer""></div>  <script> // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "	" <div id=""io-header""></div> <div id=""io-wrapper"">   <div id=""io-overlay""><img src=""330e977ff0304fc09e9112f0849e8689-oa-1-A.svg"" /></div>   <div id=""io-original""><img src=""Screen Shot 2019-08-22 at 16.34.47.png"" /></div> </div>  <button id=""io-revl-btn"" onclick=""toggle();"">Toggle Masks</button> <div id=""io-extra-wrapper"">   <div id=""io-extra"">                       </div> </div>  <script> // Toggle answer mask on clicking the image var toggle = function() {   var amask = document.getElementById('io-overlay');   if (amask.style.display === 'block' || amask.style.display === '')     amask.style.display = 'none';   else     amask.style.display = 'block' }  // Prevent original image from loading before mask aFade = 50, qFade = 0; var mask = document.querySelector('#io-overlay>img'); function loaded() {     var original = document.querySelector('#io-original');     original.style.visibility = ""visible""; } if (mask === null || mask.complete) {     loaded(); } else {     mask.addEventListener('load', loaded); } </script>  "
An <span class=cloze>[]</span> is a variable whose value exists on an arbitrary scale where only the <b>relative ordering between different values</b> is significant.	An <span class=cloze><b>ordinal variable</b></span> is a variable whose value exists on an arbitrary scale where only the <b>relative ordering between different values</b> is significant.<br><br> 
An <b>ordinal variable</b> is a variable whose value <span class=cloze>[]</span>.	An <b>ordinal variable</b> is a variable whose value <span class=cloze>exists on an arbitrary scale where only the <b>relative ordering between different values</b> is significant</span>.<br><br> 
what is <b>ordinal prediction </b>(also called ordinal regression)?	"Predict a <b>rank</b>.<div><br></div><div><i>For example sociologists could use a scale from, say, 1–5 for ""very poor"" through ""excellent"".</i></div>"
what is the <b>Spearman correlation </b>for?	To compute a <b>correlation between ranks</b>.<div><br></div><div>The higher it is, the more similar are the ranks.</div>
what is <b>Spearman correlation </b>formula?	It's the <b>Pearson correlation</b>, but using ranks as variables.<div><br></div><div>\(r_{s}=\rho_{\mathrm{rg}_{X}, \mathrm{rg}_{Y}}=\frac{\operatorname{cov}\left(\operatorname{rg}_{X}, \mathrm{rg}_{Y}\right)}{\sigma_{\mathrm{rg}_{X}} \sigma_{\mathrm{rg}_{Y}}}\)</div>
what does the <b>Pearson correlation</b>&nbsp;mean?	"Value is between -1 and +1.<div><br><div><b>Extremums mean total correlation</b> (resp. negative or positive).</div></div><div><br></div><div><b>0 means no correlation at all</b>.</div><div><br></div><div><img src=""400px-Correlation_coefficient.png""><br></div>"
how many <b>bits in a byte</b>?	"<img src=""Screen Shot 2019-08-28 at 10.21.19.png"">"
what is a <b>complex conjugate</b>?	A complex number with an opposite sign for the imaginary part.<div><br></div><div>\(a + ib\) and its conjugate \(a - ib\)</div>
what is an <b>Hermitian matrix</b>?	"Complex square matrix that is equal to its own conjugate transpose:<div><br></div><div><img src=""paste-bce5431299362efbd278054dca93c23ea10c023d.jpg""><br></div>"
what does <b>AUC </b>acronyme means?	<b>A</b>rea <b>U</b>nder the <b>C</b>urve
what does&nbsp;<b>ROC </b>(in AUC-ROC)<b>&nbsp;</b>acronyme means?	<b>R</b>eceiver <b>O</b>perating <b>C</b>haracteristics
what are the <b>axis </b>used for plotting the <b>ROC </b>curve?	"<div>TPR = <b>True Positive Rate</b></div><div>FPR = <b>False Positive Rate</b></div><img src=""1pk05QGzoWhCgRiiFbz-oKQ.png"">"
what is the <b>ROC </b>curve?	"<div>A curve plotting the <b>True Positive Rate </b>(TPR) against the <b>False Positive Rate </b>(FPR) for various <b>thresholds</b>.</div><div><i><br></i></div><div><i>The FPR is maximal when the threshold is at its lowest (on the upper right). The TPR is also maximal because we always predict.</i></div><div><br></div><img src=""1pk05QGzoWhCgRiiFbz-oKQ.png"">"
what is the ROC<b> AUC</b>?	"<div>The <b>AUC</b>&nbsp;is the area under the <b>ROC&nbsp;</b>curve. The bigger it is (up to 1.), the better the model is.</div><img src=""1pk05QGzoWhCgRiiFbz-oKQ.png"">"
<b>Four kinds of losses </b>in <b>Domain Separation Networks</b>?<div><br></div><div>Feature representation&nbsp;<b>similarity</b><div><span class=cloze>[]</span></div><div><b>Source classification</b></div><div><b>Image reconstruction</b></div></div>	"<b>Four kinds of losses </b>in <b>Domain Separation Networks</b>?<div><br></div><div>Feature representation&nbsp;<b>similarity</b><div><span class=cloze>Feature representation&nbsp;<b>difference</b></span></div><div><b>Source classification</b></div><div><b>Image reconstruction</b></div></div><br><br> <div><img src=""paste-efd0c19cb9b285181ec985e00d1c3f9fafd15b04.jpg""><br></div>"
<b>Four kinds of losses </b>in <b>Domain Separation Networks</b>?<div><br></div><div>Feature representation&nbsp;<b>similarity</b><div>Feature representation&nbsp;<b>difference</b></div><div><span class=cloze>[]</span></div><div><b>Image reconstruction</b></div></div>	"<b>Four kinds of losses </b>in <b>Domain Separation Networks</b>?<div><br></div><div>Feature representation&nbsp;<b>similarity</b><div>Feature representation&nbsp;<b>difference</b></div><div><span class=cloze><b>Source classification</b></span></div><div><b>Image reconstruction</b></div></div><br><br> <div><img src=""paste-efd0c19cb9b285181ec985e00d1c3f9fafd15b04.jpg""><br></div>"
<b>Four kinds of losses </b>in <b>Domain Separation Networks</b>?<div><br></div><div>Feature representation&nbsp;<b>similarity</b><div>Feature representation&nbsp;<b>difference</b></div><div><b>Source classification</b></div><div><span class=cloze>[]</span></div></div>	"<b>Four kinds of losses </b>in <b>Domain Separation Networks</b>?<div><br></div><div>Feature representation&nbsp;<b>similarity</b><div>Feature representation&nbsp;<b>difference</b></div><div><b>Source classification</b></div><div><span class=cloze><b>Image reconstruction</b></span></div></div><br><br> <div><img src=""paste-efd0c19cb9b285181ec985e00d1c3f9fafd15b04.jpg""><br></div>"
What are the <b>four steps of the addressing mechanism</b> in a Neural Turing Machine?<div><br></div><div>Content addressing</div><div><span class=cloze>[]</span></div><div>Convolutional shift</div><div>Sharpening<br></div>	"What are the <b>four steps of the addressing mechanism</b> in a Neural Turing Machine?<div><br></div><div>Content addressing</div><div><span class=cloze>Interpolation</span></div><div>Convolutional shift</div><div>Sharpening<br></div><br><br> <div><img src=""paste-536dc95442abedc4ae1c024eab7c4784a128c298.jpg""><br></div>"
What are the <b>four steps of the addressing mechanism</b> in a Neural Turing Machine?<div><br></div><div>Content addressing</div><div>Interpolation</div><div><span class=cloze>[]</span></div><div>Sharpening<br></div>	"What are the <b>four steps of the addressing mechanism</b> in a Neural Turing Machine?<div><br></div><div>Content addressing</div><div>Interpolation</div><div><span class=cloze>Convolutional shift</span></div><div>Sharpening<br></div><br><br> <div><img src=""paste-536dc95442abedc4ae1c024eab7c4784a128c298.jpg""><br></div>"
What are the <b>four steps of the addressing mechanism</b> in a Neural Turing Machine?<div><br></div><div>Content addressing</div><div>Interpolation</div><div>Convolutional shift</div><div><span class=cloze>[]</span><br></div>	"What are the <b>four steps of the addressing mechanism</b> in a Neural Turing Machine?<div><br></div><div>Content addressing</div><div>Interpolation</div><div>Convolutional shift</div><div><span class=cloze>Sharpening</span><br></div><br><br> <div><img src=""paste-536dc95442abedc4ae1c024eab7c4784a128c298.jpg""><br></div>"
what: <b>norm infinity</b>	\(\|\mathbf{x}\|_{\infty} :=\max \left(\left|x_{1}\right|, \ldots,\left|x_{n}\right|\right)\)
what: <b>norm L0</b>	<b>Number of non-zero elements</b>
what is a <b>confonder</b>?	"Variable that influences both the dependent variable and independent variable, causing a <b>spurious association</b>.<div><br></div><div><img src=""Confounding.PNG""><br></div><div><i>Gender Z confounds the relation between X and Y since Z is a cause of both X and Y.</i><br></div>"
what is <b>conditionning on confonding factors</b>?	"A method to <b>eliminate a confonder effect by fixing its value</b>.<div><br></div><div><i>For example, there is a&nbsp;spurious correlation between crime &amp; ice creams sale. This correlation disapear when fixing the temperature, because both are linked to summer.</i></div><div><img src=""main-qimg-6e492bf3b18d8784489f42b90a2bb308.webp""><i>&nbsp;</i></div>"
what do \(P(Y | do(X)) \gt P(Y)\) imply?	\(X\) causes \(Y\).
Causal diagram: <b>Chain</b>	"<img src=""1oTQVofKLGy0mqaNiU9P7Bw.png"">"
"what is the name of \(Y\) in this causal diagram?<div><img src=""1oTQVofKLGy0mqaNiU9P7Bw.png""><br></div> "	causal diagram:&nbsp;<b>Mediator</b>  
causal diagram:&nbsp;<b>Mediator</b>	"what is the name of \(Y\) in this causal diagram?<div><img src=""1oTQVofKLGy0mqaNiU9P7Bw.png""><br></div>"
"why does the <b>mediator </b>\(Y\) ""<b>screens off</b>"" the information between \(X\) &amp; \(Z\)?<div><img src=""1oTQVofKLGy0mqaNiU9P7Bw.png""><br></div>"	\(Y\) may be caused by <b>other factors</b>.<div><br></div><div><i>X = Fire</i></div><div><i>Y = Smoke</i></div><div><i>Z = Alarm</i></div><div><i>No need to look for X to predict Z, but it does not mean that there is a fire though.</i></div>
causal diagram: <b>Fork</b> 	"<img src=""Screen Shot 2019-09-22 at 22.49.45.png"">  "
"<img src=""Screen Shot 2019-09-22 at 22.49.45.png"">"	causal diagram: <b>Fork</b>
"what is the <b>confonding factor </b>in a <b>Fork</b>?<div><img src=""Screen Shot 2019-09-22 at 22.49.45.png""><br></div>"	X and Z have for <b>common cause </b>Y, and thus are correlated while <b>not having a causality link</b>.<div><br></div><div><i>X = Shoe size</i></div><div><i>Z = Reading ability</i></div><div><i>Y = Age of child</i></div>
"how to <b>eliminate </b>the <b>confonding</b>&nbsp;factor in a <b>Fork</b>?<div><img src=""Screen Shot 2019-09-22 at 22.49.45.png""><br></div>"	<div>By <b>conditionning on the common cause</b>&nbsp;Y.</div><div><i><br></i></div><div><i>X = Shoe size</i></div><div><i>Z = Reading ability</i></div><div><i>Y = Age of child</i></div><div><i>No spurious correlation between X &amp; Y if the age is fixed.</i></div>
causal diagram: <b>Collider</b>	"<img src=""Screen Shot 2019-09-22 at 22.57.27.png"">"
"what is the <b>explain-away factor </b>of a <b>collider</b>?<div><img src=""Screen Shot 2019-09-22 at 22.57.27.png""><br></div>"	By <b>conditionning on the common effect</b>&nbsp;Y, the two causes X and Y become <b>dependent </b>inducing a <b>spurious correlation</b>.<div><br></div><div><i>X = Talent</i></div><div><i>Z = Beauty</i></div><div><i>Y = Celebrity</i></div><div><i>If we condition Celebrity to True, there is a negative correlation between Talent &amp; Beauty. If a star is beautiful, he is probably talentless, and vice-versa.</i></div>
\(\frac{\partial x}{\partial x} = \) ?	\(\frac{\partial x}{\partial x} = 1\)
\(\frac{\partial}{\partial x}(x^n)&nbsp;= \) ?	\(\frac{\partial}{\partial x}(x^n)&nbsp;=&nbsp;n x^{n-1}\)
\(\frac{\partial}{\partial x}(c)&nbsp;= \) ?	\(\frac{\partial}{\partial x}(c)&nbsp;= 0\)
\((f \pm g)' = \) ?	\((f \pm g)'&nbsp;= f' \pm g'\)
\((f g)' = \) ?	\((f g)'= f'g + g'f&nbsp;\)
\((\frac{f}{g})' = \) ?	\((\frac{f}{g})' = \frac{f'g - g'f}{g^2}\)
\((f \circ g)' = \) ?	\((f \circ g)' = g'(f' \circ g) \) ?
\(\frac{\partial}{\partial x}(\sin x)&nbsp;= \) ?	\(\frac{\partial}{\partial x}(\sin x)&nbsp;= \cos x\)
\(\frac{\partial}{\partial x}(\cos x)&nbsp;= \) ?	\(\frac{\partial}{\partial x}(\cos x)&nbsp;= -\sin x\) ?
\(\frac{\partial}{\partial x}(\ln x)&nbsp;= \) ?	\(\frac{\partial}{\partial x}(\ln x)&nbsp;= \frac{1}{x}\)
rational number	"Number that can be <b>expressed as the quotient or fraction <i>p</i>/<i>q</i></b> of two integers, a numerator <i>p</i> and a non-zero denominator <i>q</i>.<div><br></div><div><img src=""220px-Number-systems.svg.png""><br></div>"
irrational number	"<b>Real numbers that cannot be constructed from fractions of integers.</b><br><div><img src=""paste-1fb8b2adbe78f12fd5141793e0a0636a4abd2cdd.jpg""><br></div>"
Cauchy sequence	"<b>Sequence whose elements become arbitrarily close to each other</b> as the sequence progresses.<br><div><br></div><div><img src=""250px-Cauchy_sequence_illustration.svg.png""><br></div>"
<b>Cauchy sequence</b> in a metric space	<div>Formally, given a <b>metric space (<i>X</i>, <i>d</i>)</b>, a sequence:&nbsp;<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, <i>x</i><sub>3</sub>, ... is Cauchy, if for every positive real number <i>ε</i> &gt; 0 there is a positive integer <i>N</i> such that for <b>all positive integers <i>m</i>, <i>n</i> &gt; <i>N</i>, the distance&nbsp;</b><b><i>d</i>(<i>x</i><sub><i>m</i></sub>, <i>x</i><sub><i>n</i></sub>) &lt; <i>ε</i>.</b></div><div>&nbsp; <div>Roughly speaking, the <b>terms of the sequence are getting closer and closer together</b> in a way that suggests that the sequence ought to have a limit in <i>X</i>.&nbsp;</div><div><br></div><div><i>Nonetheless, such a limit does not always exist within X</i></div></div>
<b>Complete </b>metric space	"Metric space where the <b>limit of a Cauchy sequence exists in this space.</b>  <br/><br/><br/> <img src=""250px-Mathematical_Spaces.png"">"
Metric space where the <b>limit of a Cauchy sequence exists in this space.</b>	"<b>Complete </b>metric space  <br/><br/><br/> <img src=""250px-Mathematical_Spaces.png"">"
<b>inner product </b>space	"Space that has an <b>inner product</b>: that can <b>associates pair of vectors into a scalar</b>.<div><br></div><div><img src=""paste-20b917b30b95763314c6124e2fd639670f7a431e.jpg""><br></div><div><img src=""250px-Mathematical_Spaces.png""><br></div>"
<b>Hilbert </b>space	"Space that is both an <b>inner product </b>space and a <b>complete </b>metric space (using the inner product a metric).  <br/><br/><br/> <img src=""250px-Mathematical_Spaces.png"">"
Space that is both an <b>inner product </b>space and a <b>complete </b>metric space (using the inner product a metric).	"<b>Hilbert </b>space  <br/><br/><br/> <img src=""250px-Mathematical_Spaces.png"">"
"<b>outer loop</b>&nbsp;in <b>MAML</b><div><img src=""Screen Shot 2019-09-26 at 10.09.47.png""><b><br></b></div> "	"Loop that <b>updates meta-model</b> so that task models has <b>good initialization</b>.<div><img src=""Screen Shot 2019-09-26 at 10.09.39.png""><br></div>  "
"Loop that <b>updates meta-model</b> so that task models has <b>good initialization</b>.<div><img src=""Screen Shot 2019-09-26 at 10.09.39.png""><br></div>"	"<b>outer loop</b>&nbsp;in <b>MAML</b><div><img src=""Screen Shot 2019-09-26 at 10.09.47.png""><b><br></b></div>"
"<b>inner loop </b>in <b>MAML</b><div><img src=""Screen Shot 2019-09-26 at 10.09.47.png""><b><br></b></div> "	"Based on meta-initialization, <b>fine-tune weights with </b>\(K\) <b>samples for the current task</b>.<div><img src=""Screen Shot 2019-09-26 at 10.09.39.png""><br></div>  "
"Based on meta-initialization, <b>fine-tune weights with </b>\(K\) <b>samples for the current task</b>.<div><img src=""Screen Shot 2019-09-26 at 10.09.39.png""><br></div>"	"<b>inner loop </b>in <b>MAML</b><div><img src=""Screen Shot 2019-09-26 at 10.09.47.png""><b><br></b></div>"
"<b>rapid learning </b>hypothesis in <b>MAML</b><div><img src=""Screen Shot 2019-09-26 at 10.09.47.png""><b><br></b></div> "	"Meta-initialization is <b>well conditionned</b> to do rapid learning.<div><img src=""Screen Shot 2019-09-26 at 10.13.47.png""><br></div>  "
"Meta-initialization is <b>well conditionned</b> to do rapid learning.<div><img src=""Screen Shot 2019-09-26 at 10.13.47.png""><br></div>"	"<b>rapid learning </b>hypothesis in <b>MAML</b><div><img src=""Screen Shot 2019-09-26 at 10.09.47.png""><b><br></b></div>"
"<b>feature reuse </b>hypothesis in <b>MAML</b><div><img src=""paste-8fce24f1a09449eae5208477a06d3eb3a4df8f49.jpg""><b><br></b></div> "	"Meta-initialization has already <b>good features that barely needs to be fine-tuned</b>.<div><img src=""Screen Shot 2019-09-26 at 10.13.47.png""><br></div>  "
"Meta-initialization has already <b>good features that barely needs to be fine-tuned</b>.<div><img src=""Screen Shot 2019-09-26 at 10.13.47.png""><br></div>"	"<b>feature reuse </b>hypothesis in <b>MAML</b><div><img src=""paste-8fce24f1a09449eae5208477a06d3eb3a4df8f49.jpg""><b><br></b></div>"
\(f(x) = x^T P x\) =&gt; \(\nabla f(x) = \)?	\(f(x) = x^T P x\) =&gt; \(\nabla f(x) = 2Px\)
\(f(x) = \Vert Ax - b \Vert^2_2 \) =&gt; \(\nabla f(x) = \)?	\(f(x) = \Vert Ax - b \Vert^2_2 \) =&gt; \(\nabla f(x) = 2A^T(Ax - b)\)
"<b>adaptative learning</b>&nbsp;used by <b>Adamine</b><div><img src=""Screen Shot 2019-09-27 at 11.07.59.png""><b><br></b></div>"	"Loss is based on two <b>triplet margin losses </b>\(l_\text{ins}\) and \(l_\text{sem}\).<div><br></div><div>Each margin loss <b>dicards triplets that have a null loss</b> because constraints are respected.</div><div><img src=""Screen Shot 2019-09-27 at 11.09.40.png""><br></div><div><i>This produces a curriculum learning, where at the end of the training only hard triplets are considered while others are simply ignored.</i></div>"
<b>normed vector </b>space	"Vector space on which a <b>norm is defined</b>.<div><br></div><div><i>A norm is the formalization and the generalization to real vector spaces of the intuitive notion of ""length"" in the real world.</i><br></div>  <br/><br/><br/> <img src=""250px-Mathematical_Spaces.png"">"
"Vector space on which a <b>norm is defined</b>.<div><br></div><div><i>A norm is the formalization and the generalization to real vector spaces of the intuitive notion of ""length"" in the real world.</i><br></div>"	"<b>normed vector </b>space  <br/><br/><br/> <img src=""250px-Mathematical_Spaces.png"">"
\(L_\text{2,1}\) norm 	\(\|A\|_{2,1}=\sum_{j=1}^{n}\left\|a_{j}\right\|_{2}=\sum_{j=1}^{n}\left(\sum_{i=1}^{m}\left|a_{i j}\right|^{2}\right)^{\frac{1}{2}}\)<div><br></div><div><b>Sum of the Euclidean norms of the columns of the matrix.</b><br></div>  
\(\|A\|_{2,1}=\sum_{j=1}^{n}\left\|a_{j}\right\|_{2}=\sum_{j=1}^{n}\left(\sum_{i=1}^{m}\left|a_{i j}\right|^{2}\right)^{\frac{1}{2}}\)<div><br></div><div><b>Sum of the Euclidean norms of the columns of the matrix.</b><br></div>	\(L_\text{2,1}\) norm
<b>outer product</b> 	\(\begin{array}{l}{\mathbf{u}=\left(u_{1}, u_{2}, \dots, u_{m}\right)} \\ {\mathbf{v}=\left(v_{1}, v_{2}, \dots, v_{n}\right)}\end{array}\)<div><br></div><div>\(\mathbf{u} \otimes \mathbf{v}=\left[\begin{array}{cccc}{u_{1} v_{1}} &amp; {u_{1} v_{2}} &amp; {\dots} &amp; {u_{1} v_{n}} \\ {u_{2} v_{1}} &amp; {u_{2} v_{2}} &amp; {\dots} &amp; {u_{2} v_{n}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {u_{m} v_{1}} &amp; {u_{m} v_{2}} &amp; {\dots} &amp; {u_{m} v_{n}}\end{array}\right]\)</div>  
\(\begin{array}{l}{\mathbf{u}=\left(u_{1}, u_{2}, \dots, u_{m}\right)} \\ {\mathbf{v}=\left(v_{1}, v_{2}, \dots, v_{n}\right)}\end{array}\)<div><br></div><div>\(\mathbf{u} \otimes \mathbf{v}=\left[\begin{array}{cccc}{u_{1} v_{1}} &amp; {u_{1} v_{2}} &amp; {\dots} &amp; {u_{1} v_{n}} \\ {u_{2} v_{1}} &amp; {u_{2} v_{2}} &amp; {\dots} &amp; {u_{2} v_{n}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {u_{m} v_{1}} &amp; {u_{m} v_{2}} &amp; {\dots} &amp; {u_{m} v_{n}}\end{array}\right]\)</div>	<b>outer product</b>
<b>Kronecker product</b> 	"\(\mathbf{A} \otimes \mathbf{B}=\left[\begin{array}{ccc}{a_{11} \mathbf{B}} &amp; {\cdots} &amp; {a_{1 n} \mathbf{B}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {a_{m 1} \mathbf{B}} &amp; {\cdots} &amp; {a_{m n} \mathbf{B}}\end{array}\right]\)<div><br></div><div><img src=""ScIU638WrL8fzAjl9QzPzn2c6JW8RJ3lTE_csU3z8wc.original.fullsize.png""><br></div>  "
"\(\mathbf{A} \otimes \mathbf{B}=\left[\begin{array}{ccc}{a_{11} \mathbf{B}} &amp; {\cdots} &amp; {a_{1 n} \mathbf{B}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {a_{m 1} \mathbf{B}} &amp; {\cdots} &amp; {a_{m n} \mathbf{B}}\end{array}\right]\)<div><br></div><div><img src=""ScIU638WrL8fzAjl9QzPzn2c6JW8RJ3lTE_csU3z8wc.original.fullsize.png""><br></div>"	<b>Kronecker product</b>
The <span class=cloze>[]</span> product takes a pair of <b>vectors</b> and produces a <b>matrix</b>.	The <span class=cloze><b>outer</b></span> product takes a pair of <b>vectors</b> and produces a <b>matrix</b>.<br><br> \(\mathbf{u} \otimes \mathbf{v}=\left[\begin{array}{cccc}{u_{1} v_{1}} &amp; {u_{1} v_{2}} &amp; {\dots} &amp; {u_{1} v_{n}} \\ {u_{2} v_{1}} &amp; {u_{2} v_{2}} &amp; {\dots} &amp; {u_{2} v_{n}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {u_{m} v_{1}} &amp; {u_{m} v_{2}} &amp; {\dots} &amp; {u_{m} v_{n}}\end{array}\right]\)
The <b>outer</b> product takes a pair of <span class=cloze>[]</span> and produces a <b>matrix</b>.	The <b>outer</b> product takes a pair of <span class=cloze><b>vectors</b></span> and produces a <b>matrix</b>.<br><br> \(\mathbf{u} \otimes \mathbf{v}=\left[\begin{array}{cccc}{u_{1} v_{1}} &amp; {u_{1} v_{2}} &amp; {\dots} &amp; {u_{1} v_{n}} \\ {u_{2} v_{1}} &amp; {u_{2} v_{2}} &amp; {\dots} &amp; {u_{2} v_{n}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {u_{m} v_{1}} &amp; {u_{m} v_{2}} &amp; {\dots} &amp; {u_{m} v_{n}}\end{array}\right]\)
The <b>outer</b> product takes a pair of <b>vectors</b> and produces a <span class=cloze>[]</span>.	The <b>outer</b> product takes a pair of <b>vectors</b> and produces a <span class=cloze><b>matrix</b></span>.<br><br> \(\mathbf{u} \otimes \mathbf{v}=\left[\begin{array}{cccc}{u_{1} v_{1}} &amp; {u_{1} v_{2}} &amp; {\dots} &amp; {u_{1} v_{n}} \\ {u_{2} v_{1}} &amp; {u_{2} v_{2}} &amp; {\dots} &amp; {u_{2} v_{n}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {u_{m} v_{1}} &amp; {u_{m} v_{2}} &amp; {\dots} &amp; {u_{m} v_{n}}\end{array}\right]\)
The&nbsp;<span class=cloze>[]</span> product takes a pair of <b>vectors</b> and produces a <b>scalar</b>.	The&nbsp;<span class=cloze><b>dot</b></span> product takes a pair of <b>vectors</b> and produces a <b>scalar</b>.<br><br> \(\mathbf{a} \cdot \mathbf{b}=\sum_{i=1}^{n} a_{i} b_{i}=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}\)
The&nbsp;<b>dot</b> product takes a pair of <span class=cloze>[]</span> and produces a <b>scalar</b>.	The&nbsp;<b>dot</b> product takes a pair of <span class=cloze><b>vectors</b></span> and produces a <b>scalar</b>.<br><br> \(\mathbf{a} \cdot \mathbf{b}=\sum_{i=1}^{n} a_{i} b_{i}=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}\)
The&nbsp;<b>dot</b> product takes a pair of <b>vectors</b> and produces a <span class=cloze>[]</span>.	The&nbsp;<b>dot</b> product takes a pair of <b>vectors</b> and produces a <span class=cloze><b>scalar</b></span>.<br><br> \(\mathbf{a} \cdot \mathbf{b}=\sum_{i=1}^{n} a_{i} b_{i}=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}\)
The&nbsp;<span class=cloze>[]</span> product takes a pair of <b>matrices</b> and produces a <b>matrix</b>.	The&nbsp;<span class=cloze><b>Kronecker</b></span> product takes a pair of <b>matrices</b> and produces a <b>matrix</b>.<br><br> \(\mathbf{A} \otimes \mathbf{B}=\left[\begin{array}{ccc}{a_{11} \mathbf{B}} &amp; {\cdots} &amp; {a_{1 n} \mathbf{B}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {a_{m 1} \mathbf{B}} &amp; {\cdots} &amp; {a_{m n} \mathbf{B}}\end{array}\right]\)
The&nbsp;<b>Kronecker</b> product takes a pair of <span class=cloze>[]</span> and produces a <b>matrix</b>.	The&nbsp;<b>Kronecker</b> product takes a pair of <span class=cloze><b>matrices</b></span> and produces a <b>matrix</b>.<br><br> \(\mathbf{A} \otimes \mathbf{B}=\left[\begin{array}{ccc}{a_{11} \mathbf{B}} &amp; {\cdots} &amp; {a_{1 n} \mathbf{B}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {a_{m 1} \mathbf{B}} &amp; {\cdots} &amp; {a_{m n} \mathbf{B}}\end{array}\right]\)
The&nbsp;<b>Kronecker</b> product takes a pair of <b>matrices</b> and produces a <span class=cloze>[]</span>.	The&nbsp;<b>Kronecker</b> product takes a pair of <b>matrices</b> and produces a <span class=cloze><b>matrix</b></span>.<br><br> \(\mathbf{A} \otimes \mathbf{B}=\left[\begin{array}{ccc}{a_{11} \mathbf{B}} &amp; {\cdots} &amp; {a_{1 n} \mathbf{B}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {a_{m 1} \mathbf{B}} &amp; {\cdots} &amp; {a_{m n} \mathbf{B}}\end{array}\right]\)
<b>Gram matrix</b> 	<b>Hermitian matrix of inner products</b>, whose entries are given by \(G_ij = ⟨ v_i , v_j ⟩\).  
<b>Hermitian matrix of inner products</b>, whose entries are given by \(G_ij = ⟨ v_i , v_j ⟩\).	<b>Gram matrix</b>
<b>isometric </b>metric space 	Metric space with a bijection \(f\) between \(X\) and \(Y\) that <b>preserves distance</b>.<div><br></div><div><i>i.e. a metric learning model where distances in the embeddings space correspond to their pixels space counterpart.</i></div>  
Metric space with a bijection \(f\) between \(X\) and \(Y\) that <b>preserves distance</b>.<div><br></div><div><i>i.e. a metric learning model where distances in the embeddings space correspond to their pixels space counterpart.</i></div>	<b>isometric </b>metric space
"what is the <b>ABE </b>model in:<div><img src=""Screen Shot 2019-10-01 at 15.48.15.png""><br></div>"	"An <b>ensembling model </b>were <b>attention heads</b>&nbsp;weight intermediary features given to a <b>shared final embeddings</b>:<div><br></div><div><img src=""Screen Shot 2019-10-01 at 15.48.57.png""><br></div>"
"how does <b>ABE </b>encourages <b>diversity </b>in its attention heads?<div><img src=""Screen Shot 2019-10-01 at 15.48.15.png""><br></div><div><img src=""Screen Shot 2019-10-01 at 15.48.57.png""><br></div>"	"With a <b>diversity loss</b>&nbsp;that is an <b>hinge loss</b>&nbsp;with two heads outputs as dissimilar features:<div><img src=""Screen Shot 2019-10-01 at 15.52.36.png""><br></div>"
<strong>Marginal Probability</strong> 	The <b>probability of an event</b> irrespective of the outcomes of other random variables, e.g. P(A).  
The <b>probability of an event</b> irrespective of the outcomes of other random variables, e.g. P(A).	<strong>Marginal Probability</strong>
<strong>Joint Probability</strong> 	<b>Probability of two (or more) simultaneous events</b>, e.g. P(A and B) or P(A, B).  
<b>Probability of two (or more) simultaneous events</b>, e.g. P(A and B) or P(A, B).	<strong>Joint Probability</strong>
<strong>Conditional Probability</strong> 	<b>Probability of one (or more) event given the occurrence of another event</b>, e.g. P(A given B) or P(A | B).  
<b>Probability of one (or more) event given the occurrence of another event</b>, e.g. P(A given B) or P(A | B).	<strong>Conditional Probability</strong>
<div>The <b>joint probability</b> can be calculated using the conditional probability:</div><div><br></div> P(A, B) = <span class=cloze>[]</span>	<div>The <b>joint probability</b> can be calculated using the conditional probability:</div><div><br></div> P(A, B) = <span class=cloze>P(A | B) * P(B)</span><br><br> Product Rule
<b>product rule </b>in probabilities	P(A, B) = P(A | B) * P(B)<br><div><br></div><div>With P(A, B) being the joint probability.</div>
Create a Python3 <b>virtualenv</b>	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%""><br></pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">python3 -m venv myenv </pre></div> </td></tr></tbody></table></center>"
Activate a Python <b>virtualenv</b>	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%""><br></pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000"">source</span> myenv/bin/activate </pre></div> </td></tr></tbody></table></center>"
Disable a Python virtualenv	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%""><br></pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">deactivate </pre></div> </td></tr></tbody></table></center>"
See all available version of a package with <b>Pip</b>	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%""><br></pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">pip search <span style=""color: #666666"">[</span>package-name<span style=""color: #666666"">]</span> </pre></div> </td></tr></tbody></table></center>"
relation between <b>cosine similarity </b>and <b>euclidian distance</b>?	"<img src=""Screen Shot 2019-10-22 at 15.14.15.png"">"
"<b>harmonic embedding</b>&nbsp;of FaceNet:<div><img src=""Screen Shot 2019-11-09 at 15.58.11.png""><br></div>"	"<div>Mixing samples from <b>old and new models </b>into new <b>triplets</b>.</div><img src=""Screen Shot 2019-11-09 at 15.58.01.png"">"
"what is the problem raised by <b>Horde </b>on the following diagram?<div><img src=""Screen Shot 2019-11-09 at 16.00.10.png""><br></div><div><img src=""Screen Shot 2019-11-09 at 16.00.02.png""><br></div>"	"Stars are after GAP, dots are before.<div><br></div><div>We see that the <b>features are not very informative and that GAP blur by averaging among very distant points.</b></div><div><br></div><div><i>Results using HORDE:</i></div><div><img src=""Screen Shot 2019-11-09 at 16.02.02.png""><i><br></i></div>"
"how does <b>Horde </b>wants to optimize the features distance before GAP?<div><img src=""Screen Shot 2019-11-09 at 16.00.10.png""><br></div><div><img src=""Screen Shot 2019-11-09 at 16.04.56.png""><br></div><div><br></div>"	"They <b>reduce </b>(or <b>maximize</b>) distances between <b>similar </b>(or <b>dissimilar</b>) features embedding through <b>several moments:</b><div><img src=""Screen Shot 2019-11-09 at 16.05.20.png""><b><br></b></div>"
"<b>Soft-Triple </b>innovation?<div><img src=""Screen Shot 2019-11-09 at 16.07.53.png""><br></div>"	"Merge <b>local similarity</b>&nbsp;into a single <b>global similarity:</b><div><img src=""Screen Shot 2019-11-09 at 16.08.19.png""><b><br></b></div><div>That is then used as a classic cosine classifier:</div><div><img src=""Screen Shot 2019-11-09 at 16.08.24.png""><br></div>"
"what is <b>Soft-Triple</b>'s ""<b>Adaptive Number of Centers</b>""?<div><img src=""Screen Shot 2019-11-09 at 16.07.53.png""><br></div>"	"To avoid cross-validate the number of centers per class, they add <b>regularization loss colapsing centers together</b>.<div><br></div><div><img src=""Screen Shot 2019-11-09 at 16.13.21.png""><br></div><div><br></div><div>So if a center is useless, it'll be softly merged into another.</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div><img src=""Screen Shot 2019-11-09 at 16.22.36.png""><b><br></b></div>"	"1. Trained a metric teacher model<div><br></div><div>2. Train a metric student model, <b>its triplet margin is computed from the teacher</b></div><div><b><br></b></div><div>If <b>two classes are similar, the margin will be smaller</b> than otherwise.</div><div><img src=""Screen Shot 2019-11-09 at 16.24.13.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div>Add a subplot to the current figure.</div>"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1 2 3 4</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">ax <span style=""color: #666666"">=</span> subplot(nrows, ncols, index)  ax<span style=""color: #666666"">.</span>imshow(image) ax<span style=""color: #666666"">.</span>set_title(<span style=""color: #BA2121"">""blabla""</span>)</pre></div></td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How <b>cGAN</b> condition image generation by label"	"<div><img src=""paste-0c4544c973977e76489473a4c2bb7d38787569f3.jpg""><b><br></b></div><div><b>Concatenate</b> both the image (or noise) and label embeddings along the <b>channel axis</b>.</div><div><img src=""paste-ce9ba5906ae210d899838daa73f04bbe1f73ed9f.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Gradient Reversal Layer"	"<img src=""paste-8b16765db78d198ca17e6265028fb3d68f09f56b.jpg""><br><div><b>Reverse sign of gradient</b> between ConvNet and Domain Classifier.</div><div><br></div><div>ConvNet must produce <b>features invariant to the domains</b>.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Pix2pix networks"	"<img src=""paste-122e7ee373a4c9dd0684ab3c672b495c3ff5b544.jpg""><br><div>Like cGAN but with conditionning done with another image.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Progressive GANs"	"<img src=""paste-96969189ffa04af9c6d66536baccdf822cf7279e.jpg""><br><div>Train GAN <b>progressively</b> to produce larger images.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  DC-GAN"	"<img src=""paste-84d0617761cf7f352b178e0181cfc8aebd76d739.jpg""><br><div><b>D</b>eep <b>C</b>onvolutional GAN, GANs made of <b>transpose</b>&nbsp;<b>convolutions</b> only.</div><div><br></div><div>Results are more smooth than GAN that produce discrete points.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Reparametrization trick in VAE"	"<img src=""paste-f65bc3d9ebd0475a4c1852c4a2fb0e0c6a10cba7.jpg""><br><div>Sampling is not differentiable, instead <b>sample a</b> \(\mathbf{\epsilon}\) <b>that is multiplied as a constant</b> to the predicted stddev.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Self-Supervision</u>:<div>Colorization</div>"	"<img src=""paste-98df859e5b7a3eba649cb1b991b10477c9347ea7.jpg""><br><div>Predict <b>RGB</b> values.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Self-Supervision</u>:<div>Examplars</div>"	"<img src=""paste-aa2c0844dea8c36fed530947a0981f393d0826f9.jpg""><br><div>Predict <b>similarity</b> between image and its <b>augmented</b> version.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Visualize Sigmoid and its derivative"	"<img src=""paste-2b7122618b9ce1b0e84db3cf0490817a141a3c3a.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Visualize Tanh and its derivative"	"<img src=""paste-70ed5e753281aee5284f3acc730aa278fcd688f4.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Visualize ReLU and its derivative"	"<img src=""paste-59a15e1c5ba43b1c65acd274f5cda79b6c74492c.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Function saturating"	"Function whose value at an extremum does not change<div><br></div><div><i>Like Sigmoid bounded between \([0, 1]\) for </i>\([-\infty, +\infty]\)<i>:</i></div><div><img src=""paste-2b7122618b9ce1b0e84db3cf0490817a141a3c3a.jpg""><i><br></i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Untar a gzip file"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">tar -xvzf foo.tar.gz</pre></div></td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Create a gzip tar"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">tar -cvzf foo.tar.gz foo/ </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<div>2 main components</div>"	"An <b>agent </b>and an <b>environment</b>.<div><br></div><div><img src=""paste-41933b88f6b684c17837cd23ab702a458079cf82.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Model-based&nbsp;vs Model-free</div>"	"<b>Model-based&nbsp;</b>has mapped all actions to rewards before being able to decide.<div><br></div><div><b>Model-free </b>is using trials-and-errors.</div><div><br></div><div><img src=""paste-035b3041800a92a617ce3656e9fd284341ca91d4.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Policy \(\pi\)</div>"	"The <b>strategy</b> that the agent employs to <b>determine next action</b> based on the current state.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Reward \(R\)<br></div>"	"An <b>immediate return</b> send back from the environment to evaluate the last action.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Value \(V\)</div>"	"The <b>expected long-term return</b> with discount, as opposed to the short-term reward \(R\), under the policy \(\pi\).  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Discount factor (\(\lambda\) or \(\beta\))</div>"	"Used to model the fact that the decision maker is <b>uncertain</b> about if in the next decision instant the <b><em>world</em>&nbsp;is going to end</b>.<br><div><br></div><div><i>Instead of optimizing a potentially never-ending game, the agent optimize a discounted reward.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Bellman Equation</div>"	"\(v(s) = \mathbb{E}[R_{t+1} + \lambda v(S_{t+1}) | S_t = s]\)<div><br></div><div>The current state value \(v(s\) is based on the next reward \(R_{t+1}\) and the discounted (\(\lambda\)) value of the next state \(v(S_{t+1})\).</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Policy iteration</div>"	"<img src=""paste-467f008393d3f1ed26d8d55b7682a40f8a30e52c.jpg""><div>Policy iteration runs an loop between <b>policy evaluation</b> and <b>policy improvement</b>.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Policy evaluation</div>"	"<b>Estimates the value function \(V\)</b>&nbsp;with the greedy policy obtained from the last policy improvement.<br><div><br></div><div><img src=""paste-7908fe4b06b03f0cf42337ea39888aee1c938c49.jpg""><br></div><div><i>Sampling is done greedly until convergence (based on \(\theta\) cf 2.).</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Policy improvement</div>"	"<b>Updates the policy</b> with the action that maximizes \(V\) for each state.<br><div><img src=""paste-7908fe4b06b03f0cf42337ea39888aee1c938c49.jpg""><br></div><div><i>Policy is deemed stable if for all states the associated action \(a\) defined by the policy \(\pi\) has not changed.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Value iteration</div>"	"<b>Updates the value function</b> \(V\) based on the Optimal Bellman Equation.<div><br></div><div>The optimal action is the argmax:<br><div><img src=""paste-202bf5792c650cec444b1e3cc5fdf1bed76e7e28.jpg""><br></div><div><i>Samples the action for a state \(s\) maximizing the Bellman equation. Must be done for every state.</i></div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Weakness of model-based algorithms</div>"	"They require the <b>knowledge of the transition probability</b> \(p\).<br><div><br></div><div>They are&nbsp;impractical as the <b>state space and action space grows</b>.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Greedy policy</div>"	"Try only the action which is believed to <b>yield the highest expected reward</b>.<div><br></div><div><i>The agent is not allowed to explore anything.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>Epsilon-greedy policy</div>"	"Samples a random number \([0, 1]\), if it's above \(\epsilon\) the greedy action is selected else a random action is selected.<div><br></div><div><i>The agent is allowed a few times to explore.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:&nbsp;<br><div>On-policy vs Off-policy</div>"	"<b>On-policy</b>: takes in account during the update rule the policy which yielded past state-action decisions.<div><br></div><div><b>Off-policy</b>: ignoring the previously used policy.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>File number of stdout</div>"	"1<div><img src=""paste-450e28a2810683e76576f2720331516c1e05a226.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>File number of stdin</div>"	"0<div><img src=""paste-450e28a2810683e76576f2720331516c1e05a226.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>File number of stderr</div>"	"<div><center style=""text-align: start;"">2</center></div><div><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>File-interface between devices &amp; user-space</div>"	"Device Layer<div><img src=""paste-9ac225162e517656da969c41f896b2b806a1a480.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>Path to find device-file</div>"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">/dev/... </pre></div> </td></tr></tbody></table></center><img src=""paste-9ac225162e517656da969c41f896b2b806a1a480.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>How kernel interacts with a device</div>"	"Throught a <b>device driver</b>&nbsp;that implements a common <b>API</b>.<div><img src=""paste-9ac225162e517656da969c41f896b2b806a1a480.jpg""><br></div><div><i>Here the API has two functions: device_read(), and device_write().</i></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>Special file, representing the terminal for the current process.</div>"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">/dev/tty </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>Special file, acting as a garbage.</div>"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">/dev/null </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>Hide all program outputs</div>"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">cmd &gt; /dev/null <span style=""color: #666666"">2</span>&gt;&amp;<span style=""color: #666666"">1</span> </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>How does pipe work?</div>"	"Left part writes using a <b>temporary file descriptor</b> on a <b>memory buffer.</b><div><img src=""paste-5e271da1b70fce49a8c52624d518a31a235e0d36.jpg""><b><br></b></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u><b>:</b><div>\(2^3 =\)...</div>"	"\(2^3 = 8\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u><b>:</b><div>\(2^4 =\)...</div>"	"\(2^4 = 16\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u><b>:</b><div>\(2^5 =\)...</div>"	"\(2^5 = 32\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u><b>:</b><div>\(2^6 =\)...</div>"	"\(2^6 = 64\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u><b>:</b><div>\(2^7 =\)...</div>"	"\(2^7 = 128\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u><b>:</b><div>\(2^8 =\)...</div>"	"\(2^8 =256\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u><b>:</b><div>\(2^9 =\)...</div>"	"\(2^9 = 512\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u><b>:</b><div>\(2^{10} =\)...</div>"	"\(2^{10} = 1024\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u>:<div>Binary to decimal</div>"	"<img src=""paste-262741c13caf5dcc917d09576528d5c415539223.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u>:<div>1 byte</div>"	"1 byte = 8 bits  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u>:<div>8 bits</div>"	"8 bits = 1 byte  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>How many characters in ASCII?</div>"	"<b>128 </b>available characters.<div><br></div><div>Encoded with 7 bits (\(2^7 = 128\)).</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div>Parity in ASCII</div>"	"<div><b>Extra bit to check an error</b> of one bit.</div><div><br></div><i>ASCII uses 7 bits, the extra bit describe whether there is an even or odd number of 1. If it's invalid we assume the code is corrupted.</i><div><i>Doesn't work if two bits are wrong.</i></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>UNIX</u>:<div><b>word</b>&nbsp;size in a 64 bits machine</div>"	"64 bits, or <b>8 bytes</b>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>C</u>:<div>Size of an integer</div>"	"Depends of the machine, in a 64 bits machine it's <b>64 bits</b>.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Binary</u>:<div>Decimal to binary</div>"	"<b>Repeated division</b>:<div><img src=""paste-f7424d4171a3c543c954da1f0c26c120d008eddc.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Compiler</u>:<div>Backus–Naur form (BNF)<br></div>"	"Notation describing a programming language <b>syntax</b>.<div><img src=""paste-26691788510f731b0a3d32415b718ec8e44aa4af.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>C</u>:<div>volatile qualifier</div>"	"<div>Tell the compiler to not optimize this variable as <b>it may be changed by an unseen code</b> (hardware, threads, etc.).</div><div><br></div><div><center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1 2 3 4 5 6</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">volatile</span> <span style=""color: #B00040"">bool</span> running <span style=""color: #666666"">=</span> <span style=""color: #008000"">true</span>; <span style=""color: #008000; font-weight: bold"">while</span>(running) {   <span style=""color: #408080; font-style: italic"">// do something </span>   <span style=""color: #408080; font-style: italic"">// running var may be changed</span>   <span style=""color: #408080; font-style: italic"">// in another thread</span> } </pre></div> </td></tr></tbody></table></center><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Floating point format"	"<img src=""Float_mantissa_exponent.png""><br><div><img src=""paste-4ff05d87e4d77d40dadec352489c46e98fbf7e63.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> best precision between <b>float32</b> and <b>double</b>?"	"double &gt; float32<div><br></div><div>double is a float64</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Two's complement </b>to represent signed numbers"	"Invert the digits and add one.<br><div><img src=""paste-c81e73661e9b76dcf4ee33277074090ff6984811.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  Most Significant Bit"	"The most left bit.<div><img src=""paste-b72e02b3b503b98556f19ae69ea08a63caf9feaf.jpg""><br></div> <br/> <div style=""font-style: oblique""></div>"
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  <b>Sign Extention </b>when using Two's Complement"	"Allow <b>using more bits</b> for a number without changing sign or value.<div><br></div><div><b>Add bits equal to the Most Significant Bit</b>.</div><div><br></div><div>00 1010 =&gt; 0000 0000 0000 1010</div><div>11 1111 0001 =&gt;&nbsp;1111 1111 1111 0001<br></div> <br/> <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   The most left bit.<div><img src=""paste-b72e02b3b503b98556f19ae69ea08a63caf9feaf.jpg""><br></div>"	"Most Significant Bit  <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   Allow <b>using more bits</b> for a number without changing sign or value.<div><br></div><div><b>Add bits equal to the Most Significant Bit</b>.</div><div><br></div><div>00 1010 =&gt; 0000 0000 0000 1010</div><div>11 1111 0001 =&gt;&nbsp;1111 1111 1111 0001<br></div>"	"<b>Sign Extention </b>when using Two's Complement  <div style=""font-style: oblique""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Self-supervised</u>:<div>Memory banks</div>"	"Compare similarity between augmented images and features stored in a memory bank.<div><img src=""Screen Shot 2020-01-08 at 11.22.38.png""><br></div><div><i>Softmax is done over all stored features, but can be estimated with NCE.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Parametric vs Non-parametric"	"<b>Parametric</b>&nbsp;model has a finit set of parameters (like <i>linear regression</i>).<div><br></div><div><b>Non-parametric </b>model has parameters dependent of the data (like <i>random forest</i>).<div><br></div><div><img src=""Parametric-vs-Non-Parametric-model-Artificial-Intelligence-Interview-Questions-Edureka.png""><br></div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Self-supervised</u>:<div><div> <div> <div> <div>Noise-Contrastive Estimation in Memory bank&nbsp;</div> </div> </div></div></div>"	"Binary classification with the positive features in the bank and with a <b>randomly sampled negative</b> features.<div><img src=""Screen Shot 2020-01-08 at 11.22.38.png""><br></div><div><i>The weight of the negative features is increased to take in account the sampling.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Self-supervised</u>:<div>Initialization of the memory bank</div><div><img src=""Screen Shot 2020-01-08 at 11.22.38.png""><br></div>"	"Unit random vectors.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Weight Standardization"	"Normalization of the convolution kernels.<div><img src=""Screen Shot 2020-01-08 at 12.38.51.png""><br></div><div><img src=""Screen Shot 2020-01-08 at 12.36.53.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  On what is applied Weight Standardization"	"<div>For each output channel:</div><img src=""Screen Shot 2020-01-08 at 12.36.53.png"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:<div>Path perspective of <b>Monte-Carlo estimation</b></div>"	"<img src=""Screen Shot 2020-01-08 at 13.24.14.png"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:<div>Path perspective of&nbsp;<b>Temporal Difference estimation</b></div>"	"<img src=""Screen Shot 2020-01-08 at 13.25.48.png""><br><div>Only two <b>real paths</b>&nbsp;are needed to produce this estimate, as it takes in account all paths intersecting.</div><div><img src=""Screen Shot 2020-01-08 at 13.27.04.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Variance of an estimator is proportional to:"	"<img src=""Screen Shot 2020-01-08 at 13.27.34.png"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:<div>why <b>Temporal Difference</b>&nbsp;is better than <b>Monte-Carlo</b>&nbsp;for estimation?</div>"	"Because <b>TD </b>has a lower variance by estimating over all possible paths instead of real trajectories as <b>MC</b>.<div><img src=""Screen Shot 2020-01-08 at 13.28.15.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:<div>Q-functions</div>"	"Instead of estimating the value of a state, it estimates the <b>value of a state and an action</b>.<div><img src=""Screen Shot 2020-01-08 at 14.33.29.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:<div>Sarsa is made of two algorithms:</div>"	"<b>Q-function</b>&nbsp;and <b>Temporal Difference</b>&nbsp;update rule:<div><img src=""Screen Shot 2020-01-08 at 16.04.54.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:<div>why the name <b>Sarsa</b>?</div>"	"Because the algo needs a tuple of four variables:<div><img src=""paste-63b14a26132170504cf7e468a838164de51ca360.jpg""></div><div>In:<br><div><img src=""Screen Shot 2020-01-08 at 16.04.54.png""><br></div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:<div>Sarsa vs Expected Sarsa</div>"	"<img src=""Screen Shot 2020-01-08 at 16.08.49.png""><br><div>Sarsa next action is <b>discrete</b>, while Expected Sarsa uses an <b>expectation</b> of the next action.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Reinforcement Learning</u>:<div>Off-policy value learning vs Expected Sarsa</div>"	"<b>Expected Sarsa</b>&nbsp;uses the true policy distribution to predict next state:<div><img src=""Screen Shot 2020-01-08 at 16.36.31.png""><br></div><div>While <b>Off-policy value learning </b>uses any arbitrary policy:</div><div><img src=""Screen Shot 2020-01-08 at 16.35.52.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Geometric expansion of dynamic array"	"To avoid incurring the cost of resizing many times, <b>dynamic arrays resize by a large amount, such as doubling in size</b>, and use the reserved space for future expansion:<br><div><br></div><div><center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1 2 3 4 5 6 7</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">function insertEnd(dynarray a, element e)     <span style=""color: #008000; font-weight: bold"">if</span> (a<span style=""color: #666666"">.</span>size <span style=""color: #666666"">==</span> a<span style=""color: #666666"">.</span>capacity)         <span style=""color: #666666"">//</span> resize a to twice its current capacity:         a<span style=""color: #666666"">.</span>capacity <span style=""border: 1px solid #FF0000"">←</span> a<span style=""color: #666666"">.</span>capacity <span style=""color: #666666"">*</span> <span style=""color: #666666"">2</span>          <span style=""color: #666666"">//</span> (copy the contents to the new memory location here)     a[a<span style=""color: #666666"">.</span>size] <span style=""border: 1px solid #FF0000"">←</span> e     a<span style=""color: #666666"">.</span>size <span style=""border: 1px solid #FF0000"">←</span> a<span style=""color: #666666"">.</span>size <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span> </pre></div> </td></tr></tbody></table></center>Expanding the array by any constant proportion <i>a</i> ensures that inserting <i>n</i> elements takes <i>O</i>(<i>n</i>) time overall, meaning that each insertion takes <b>amortized constant time</b>.<br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> amortized analysis"	"Method to analyse an algorithm complexity.<div><br></div><div>Instead of looking at <i>per operation</i>, it looks at <i>per algorithm</i>.</div><div><br></div><div>While certain operations for a given algorithm may have a significant cost in resources, other operations may not be as costly. <b>The amortized analysis considers both the costly and less costly operations together</b> over the whole series of operations of the algorithm.</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  On what part of the features does <b>BatchNorm</b> computes its statistics?"	"<img src=""Screen Shot 2020-01-09 at 17.58.03.png"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  BatchNorm weakness with small batches"	"BN's statistics are less reliable, more noisy<br><div><img src=""Screen Shot 2020-01-09 at 17.58.03.png"">&nbsp;</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  BatchNorm \(\mu\) and \(\sigma\) during training"	"Computed on the mini-batch:<div><img src=""Screen Shot 2020-01-09 at 18.03.26.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  BatchNorm \(\mu\) and \(\sigma\) during testing"	"<b>Running averages</b> of mini-batch statistics seen during training:<div><img src=""Screen Shot 2020-01-09 at 18.04.22.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Initialization of BatchNorm affine transformation \(\gamma\) and \(\beta\)<div><img src=""Screen Shot 2020-01-09 at 18.07.20.png""><br></div>"	"\(\gamma = 1\) and \(\beta = 0\) to produce an <b>identity</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In EWC, approximation of the <b>Fisher Information Matrix</b>"	"Mean of the square of the gradients.<div><img src=""Screen Shot 2020-01-10 at 16.25.19.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div><u>Graph</u>:</div>Adjacency matrix"	"Matrix where each row represents the <b>weights to other nodes</b>.<div>\(\begin{bmatrix}0 &amp; 1 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 1 &amp;1\\0 &amp; 1 &amp; 0 &amp; 0\\ 1 &amp; 0 &amp; 1 &amp; 0\end{bmatrix}\)<br><div><img src=""1jTW7doI_cqC_p9XQrmuu9A.png""><br></div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>Self-loops</div>"	"Loop from &amp; to the same node.<div><img src=""Loops_1000.gif""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<br><div>Add self-loops to adjacency matrix</div><div><img src=""Loops_1000.gif""><br></div>"	"Add <b>identity matrix</b> to adjacency matrix.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>Degree of a vertex</div>"	"Number of times an edge terminates at that vertex.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>Degree matrix</div>"	"Diagonal matrix containing the <b>degree</b> of each vertex.<div><img src=""Screen Shot 2020-01-10 at 17.43.22.png""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>Graph Convolution inputs</div>"	"A matrix of <b>features</b>&nbsp;\(H\)<b>&nbsp;</b>and the <b>adjacency matrix</b>&nbsp;\(A\):<div><br></div><div><div>\(f(H, A) = \sigma(AHW)\)</div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>For a Graph Convolution, possible remplacement to features matrix</div>"	"The <b>identity matrix</b>&nbsp;so that the GCN only processes the adjacency matrix.<div><br></div><div>\(f(I, A) = \sigma(AIW) = \sigma(AW)\)</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>Effect of multiplying the adjacency matrix to the features</div>"	"The representation of each node is a <b>sum of its neighbors features</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>Why does a graph need self-loops with Graph Convolution</div>"	"<b>To include the node own features</b> when multiplying the adjacency matrix to the features.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>When multiplying adjacency matrix to the features, problem related to node degree</div>"	"Nodes with <b>large degree with have large values</b>, and vice-versa.<div><br></div><div>It can lead to exploding or vanishing gradients.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <u>Graph</u>:<div>How can normalize the adjacency matrix</div>"	"Divide it by the <b>degree matrix</b>.<div><br></div><div>GCN is then: \(f(H, A) = \sigma(D^{-1}AHW) = \sigma(\tilde{A}HW)\)</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Sparse Tensor"	"Tensors designed to be computationnaly efficient when it's <b>largely sparse (many zeros)</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <u>Design Pattern</u>:<div>God Class</div>"	"An object that <b>controls way too many other objects</b> in the system and has grown beyond all logic to become The Class That Does Everything.<br><div><br></div><div><b>Bad practice</b>.</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Slurm:</div> Kill a job"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">scancel &lt;id&gt;</pre></div></td></tr></tbody></table><br></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Slurm:</div> Launch a job onto the queue"	"<center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">sbatch &lt;script.slurm&gt;</pre></div></td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Slurm:</div> slurm"	"A job scheduler used by supercomputers.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Inode count"	"Number of files and directories  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">UNIX:</div> Inode"	"A UNIX data-structure describing a file-system object such as a <b>file</b> or a <b>directory</b>.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Rotation"	"Predict 4 possible rotations of a same image:<div><img src=""paste-5d2d68135be6b5f2df1fe6560b84360d624cb191.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Patches arrangement"	"Predict which neighboring patch location it belongs:<div><img src=""paste-06a590735ccdafe14ce90c92352e1918c65de3d5.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  How to avoid learning trivial knowledge with <b>Patches arrangement</b>"	"<div>Add <b>noise</b>: gap, jiterring, color shift, etc.</div><img src=""paste-06a590735ccdafe14ce90c92352e1918c65de3d5.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Why one of the used noises is <b>color shifting</b>&nbsp;in <b>Patches</b><div><img src=""paste-06a590735ccdafe14ce90c92352e1918c65de3d5.jpg""><b><br></b></div>"	"Because of <b>chromatic aberration</b>:<div><img src=""paste-9e0c2b1d4bb90ba1da490dc3fc792ec556057768.jpg""><br></div><div>Color channels are offseted a little between patches, we want to avoid the network to rely on that.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Jigsaw"	"Re-order shuffled patches:<div><img src=""paste-1707c15008d5038d502a3eba9c076e24e725102b.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Possible classes in <b>jigsaw</b>"	"All existing patches location <b>permutation</b>.<div><img src=""paste-1707c15008d5038d502a3eba9c076e24e725102b.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Counting features"	"Expecting features of whole image to be equal to the sum of features of 4 patches.<div><img src=""paste-7283dc77748318ef440e4352c1d2ffbe5283ba39.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  How to avoid easy solutions in <b>Counting Features</b>"	"Must also be different from a <b>negative image</b>.<div><img src=""paste-7283dc77748318ef440e4352c1d2ffbe5283ba39.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Denoising autoencoder"	"Learn to reconstruct a noisy input:<div><img src=""paste-2a4d7c1b6faaf1a5b3ba7940fe4f70fc531ccad6.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Context Encoder"	"Learn to fill a missing patch:<div><img src=""paste-f24a00cf4ac3c9d455b60810d141bae97b0ce0e8.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Split-brain autoencoder"	"<b>Predict missing channels:</b><div><img src=""paste-dcc5f3ea061d1aca56c8abe0e5a2d0894e3bfc53.jpg""><br></div><div>Missing channels can be either among RGB, LAB*, or even intermediary channels.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Difference between <b>Auto-regressive model </b>and <b>RNN</b>"	"<b>RNN</b>&nbsp;have previous values coming as an <b>hidden state:</b><div><img src=""paste-0491ed8e705af23f810db25f8e6a53d875d6dd23.jpg""><b><br></b></div><div><b>Auto-regressive models </b>have previous values coming as <b>inputs</b>:</div><div><img src=""paste-892647ac7cea206bf2c248ebebc1964ca215526d.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Constrastive Predictive Coding (CPC) for audio"	"<b>Classify the future representation</b> among a set of negative samples:<div><img src=""paste-343439ab432e2a565787b4c813e17c292d1485c3.jpg""><br></div><div><i>They used a loss similar to Noise Constrastive Estimation to sample negative for a softmax-like.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Constrastive Predictive Coding (CPC) for images"	"Classify <b>features </b>based on a top-bottom context:<div><img src=""paste-494bb3957a8cf8b2d7389a823fc393b0db0a1f57.jpg""><br></div><div>Blue patches are encoded separately by a ConvNet, must classify the correct pixels (\(z_{t+x}\)) among a set of negatives.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Tracking moving objects"	"Patches with movement should have <b>similar features </b>given a small window of 30 frames:<div><img src=""paste-b05b4e6187f160660dd69716c4c21a04cf9ea136.jpg""><br></div><div><i>Used with a triplet-ranking among negative patches.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Frame Sequence"	"Classify whether <b>frames sequence are in the right order</b>:<div><img src=""paste-9864a4ec508eb1ad5f5546871491e223cf88a06f.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  Video Colorization"	"Predict frame colors based on previous frame:<div><img src=""paste-4a8370024ec796900f7d729df51bbe06d45953d6.jpg""><br></div><div>On the previous frame is in both RGB and Gray while the target frame is only in RGB.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  How the color is chosen on the target frame in <b>Video Colorization</b><div><img src=""paste-4a8370024ec796900f7d729df51bbe06d45953d6.jpg""><b><br></b></div>"	"Target colors is the <b>Source color weighted by the similarity between features</b>:<div><img src=""paste-5a4412050276c5b0df50240a11fcfca820cfe6b6.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised:</div>  How <b>Video Colorization </b>can be used for downstream tasks:<div><img src=""paste-4a8370024ec796900f7d729df51bbe06d45953d6.jpg""><br></div>"	"<b>Segmentation</b>: only color the desired object.<div><br></div><div><b>Skeleton</b>: color the skeleton.</div><div><br></div><div>Then simply predicts those colors on next frames without fine-tuning:</div><div><img src=""paste-d766c6c5fdad23726dd9582b427a66087d56be58.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Late Fusion"	"Merge different inputs <b>at the very last moment</b> using a shallow network:<div><img src=""paste-60f27aaff458be3c88751d7fb08183b2046923f9.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Early Fusion"	"Merge different inputs <b>from the beginning</b> and then <b>learn the merging</b> with a deep network:<div><img src=""paste-60f27aaff458be3c88751d7fb08183b2046923f9.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Bash:</div> for loop over files"	"<div><center><table class=""highlighttable""><tbody><tr><td><div class=""linenodiv"" style=""background-color: #f0f0f0; padding-right: 10px""><pre style=""line-height: 125%"">1 2 3 4 5</pre></div></td><td class=""code""><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">for</span> f in *yaml; <span style=""color: #008000; font-weight: bold"">do</span>  <span style=""color: #008000"">  echo</span> <span style=""color: #19177C"">$f</span>;    mv <span style=""color: #19177C"">$f</span> <span style=""color: #BB6688; font-weight: bold"">${</span><span style=""color: #19177C"">f</span><span style=""color: #BB6688; font-weight: bold"">}</span>_txt;  <span style=""color: #008000; font-weight: bold"">done</span></pre></div></td></tr></tbody></table></center><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  DropBlock"	"Randomly <b>drop continuous regions of activations</b>:<div><img src=""paste-f8791961d486be6e2217709666c92665ab4c81e6.jpg""><img src=""paste-5aae1cfd83b62cf4d1c7d54162a8dd53da1e50eb.jpg""><br></div><div><i>Used for CNN.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why <b>DropBlock </b>drops continuous regions (c) instead of (b)<div><img src=""paste-2f50996087cfd20ba97bc77421fba80ddb521a00.jpg""><br></div>"	"Because dropped features could be <b>infered from neighbours</b> in (b).  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Alternative to <b>Dropout </b>for CNN"	"<b>DropBlock</b>, dropping regions of the activations:<div><img src=""paste-f8791961d486be6e2217709666c92665ab4c81e6.jpg""><img src=""paste-5aae1cfd83b62cf4d1c7d54162a8dd53da1e50eb.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Lifelong-Learning:</div>  Sweep Reharsal <i>(Robbins 1995)</i>"	"Batch of made of 1 new data and 3 old data.<div><br></div><div><b>Learn on the same batch until the new is learn</b>.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">CogSci:</div>  Hebbian Learning"	"If two neurons on either side of a synapse (connection) are <b>activated simultaneously</b>, then the <b>strength</b> of that synapse is selectively <b>increased</b>. <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">CogSci:</div>  If two neurons on either side of a synapse (connection) are <b>activated simultaneously</b>, then the <b>strength</b> of that synapse is selectively <b>increased</b>."	"Hebbian Learning <br/> <div style=""font-style: italic""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Polyak Averaging"	"\(\hat{\theta}^{(t)}=\alpha \hat{\theta}^{(t-1)}+(1-\alpha) \hat{\theta}^{(t)} \text { with } \alpha \in[0,1]\)<br><div><br></div><div><i>Helpful when hard to fall in minima:</i></div><div><img src=""paste-15807ae05b5100f4608211f1b318857297637055.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <span class=cloze>[]</span> = \(1 \,\text{kilobyte}\) "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <span class=cloze>\(2^{10} \, \text{bytes}\)</span> = \(1 \,\text{kilobyte}\)<br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <span class=cloze>[]</span> = \(1 \,\text{megabyte}\) "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <span class=cloze>\(2^{20} \, \text{bytes}\)</span> = \(1 \,\text{megabyte}\)<br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <span class=cloze>[]</span> = \(1 \,\text{gigabyte}\) "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <span class=cloze>\(2^{30} \, \text{bytes}\)</span> = \(1 \,\text{gigabyte}\)<br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Mantissa</b> (or significand) in a floating point number"	"\(123.45 =&nbsp;1.2345\times10^2\)<div><br></div><div>And the <b>mantissa </b>is \(1.2345\).</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Radix</b> in a floating point number"	"\(123.45 =&nbsp;1.2345\times10^2\)<div><br></div><div>And the&nbsp;<b>radix&nbsp;</b>is \(10\).</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Exponent</b>&nbsp;in a floating point number"	"\(123.45 =&nbsp;1.2345\times10^2\)<div><br></div><div>And the&nbsp;<b>exponent&nbsp;</b>is \(2\).</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How does the <b>exponent </b>of a floating point number can represent both positive and negative numbers<div><br></div><div>\(1.234\times10^{2}\) and&nbsp;\(1.234\times10^{-2}\)</div>"	"Exponent is <b>substracted by 127</b>.<div><br></div><div>\(10^0\) has an exponent of 127.</div><div><br></div><div>\(10^{-1}\) has an exponent of 126.</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Bias </b>of the exponent in a floating point number"	"Method to <b>represent positive and negative</b> exponents:<div>\(1.234\times10^{2}\) and&nbsp;\(1.234\times10^{-2}\)</div><div><br></div><div>Exponent is&nbsp;substracted by 127.</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Which part of the floating point number defines its <b>precision</b>"	"The&nbsp;<b>Mantissa</b>&nbsp;(or significand).<br><div><br></div><div>\(123.45 =&nbsp;1.2345\times10^2\)<div><br></div><div>And the&nbsp;mantissa<b>&nbsp;</b>is \(1.2345\).</div></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Normalization </b>with floating point number"	"Trick where number are <b>expressed with the most significant bit to 1</b>:<div><img src=""paste-2f9a8d8dcd22a45370faf1376864590ddeb84c9b.jpg""><br></div><div><br></div><div>The number stays the same, but we can <b>gain 1 bit precision </b>as we assume the leftmost bit is always 1.</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What's the point of <b>normalization </b>in floating point number, where numbers are expressed with the most significant bit to 1:<div><img src=""paste-2f9a8d8dcd22a45370faf1376864590ddeb84c9b.jpg""><br></div>"	"The number stays the same, but we can&nbsp;<b>gain 1 bit precision&nbsp;</b>as we assume the leftmost bit is always 1.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  Program counter<div>(<i>or instruction pointer</i>)</div>"	"<b>Register</b> storing the <b>adress to the next instruction</b> to be executed <br/> <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   <b>Register</b> storing the <b>adress to the next instruction</b> to be executed"	"Program counter<div>(<i>or instruction pointer</i>)</div>  <div style=""font-style: oblique""></div>"
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  Processor register"	"Small amount of <b>fast storage</b> located in the CPU. <br/> <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   Small amount of <b>fast storage</b> located in the CPU."	"Processor register  <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Special function register"	"<b>Registers storing special values</b> such as:<div><br></div><div>I/O</div><div>stack pointer</div><div>program counter</div><div>...</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Branching (<i>CPU instructions</i>)"	"Setting the <b>program counter </b>to an adress in a different part of the program.<div><br></div><div><i>Useful in loops and condition.</i></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When does <b>branching </b>in CPU instructions is useful"	"<div>Useful in&nbsp;<b>loops</b>&nbsp;and&nbsp;<b>conditions</b>.<br></div><div><i><br></i></div><i>It sets the&nbsp;program counter&nbsp;to an adress in a different part of the program.</i>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  Clock speed"	"<b>Amount of cycles </b>(instructions) that can be <b>done per seconds</b> by the CPU. <br/> <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   <b>Amount of cycles </b>(instructions) that can be <b>done per seconds</b> by the CPU."	"Clock speed  <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> CPU's cycles"	"A <b>cycle </b>is the steps needed for the CPU to <b>process an instruction</b>.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Four steps in a CPU's cycle:<div><br></div><div>1. <span class=cloze>[]</span><br><br>2. <u>Decode</u> : internally decode what it has to do.<br><br>3. <u>Execute</u> : take the values from the registers, actually add them together<br><br>4. <u>Store</u> : store the result back into another register. You might also see the term retiring the instruction.<br></div> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Four steps in a CPU's cycle:<div><br></div><div>1. <span class=cloze><u style="""">Fetch</u> : get the instruction from memory into the processor.</span><br><br>2. <u>Decode</u> : internally decode what it has to do.<br><br>3. <u>Execute</u> : take the values from the registers, actually add them together<br><br>4. <u>Store</u> : store the result back into another register. You might also see the term retiring the instruction.<br></div><br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Four steps in a CPU's cycle:<div><br></div><div>1. <u style="""">Fetch</u> : get the instruction from memory into the processor.<br><br>2. <span class=cloze>[]</span><br><br>3. <u>Execute</u> : take the values from the registers, actually add them together<br><br>4. <u>Store</u> : store the result back into another register. You might also see the term retiring the instruction.<br></div> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Four steps in a CPU's cycle:<div><br></div><div>1. <u style="""">Fetch</u> : get the instruction from memory into the processor.<br><br>2. <span class=cloze><u>Decode</u> : internally decode what it has to do.</span><br><br>3. <u>Execute</u> : take the values from the registers, actually add them together<br><br>4. <u>Store</u> : store the result back into another register. You might also see the term retiring the instruction.<br></div><br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Four steps in a CPU's cycle:<div><br></div><div>1. <u style="""">Fetch</u> : get the instruction from memory into the processor.<br><br>2. <u>Decode</u> : internally decode what it has to do.<br><br>3. <span class=cloze>[]</span><br><br>4. <u>Store</u> : store the result back into another register. You might also see the term retiring the instruction.<br></div> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Four steps in a CPU's cycle:<div><br></div><div>1. <u style="""">Fetch</u> : get the instruction from memory into the processor.<br><br>2. <u>Decode</u> : internally decode what it has to do.<br><br>3. <span class=cloze><u>Execute</u> : take the values from the registers, actually add them together</span><br><br>4. <u>Store</u> : store the result back into another register. You might also see the term retiring the instruction.<br></div><br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Four steps in a CPU's cycle:<div><br></div><div>1. <u style="""">Fetch</u> : get the instruction from memory into the processor.<br><br>2. <u>Decode</u> : internally decode what it has to do.<br><br>3. <u>Execute</u> : take the values from the registers, actually add them together<br><br>4. <span class=cloze>[]</span><br></div> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Four steps in a CPU's cycle:<div><br></div><div>1. <u style="""">Fetch</u> : get the instruction from memory into the processor.<br><br>2. <u>Decode</u> : internally decode what it has to do.<br><br>3. <u>Execute</u> : take the values from the registers, actually add them together<br><br>4. <span class=cloze><u>Store</u> : store the result back into another register. You might also see the term retiring the instruction.</span><br></div><br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Fetch </b>step of a CPU's <b>cycle</b>"	"Get the <b>instruction from memory</b> into the processor.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Decode </b>step of a CPU's cycle"	"Internally decode what it has to do.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Execute </b>step of a CPU's <b>cycle</b>"	"Take the values from the registers, and <b>execute the instruction</b> (like <i>add</i>)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Store </b>step of a CPU's <b>cycle</b>"	"<b>Store the result back</b> into another register once the cycle is finished.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  Arithmetic Logic Unit (ALU)"	"Part of the CPU doing all the <b>operations </b>(<i>add, xor, etc.).</i> <br/> <div style=""font-style: oblique""><img src=""paste-f911cfc27c1de5aa8efc98eb46a9aece152ee247.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   Part of the CPU doing all the <b>operations </b>(<i>add, xor, etc.).</i>"	"Arithmetic Logic Unit (ALU)  <div style=""font-style: oblique""><img src=""paste-f911cfc27c1de5aa8efc98eb46a9aece152ee247.jpg""></div>"
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  Address Generation Unit (AGU)"	"Part of the CPU <b>interacting with the cache and the memory</b> to fetch data. <br/> <div style=""font-style: oblique""><img src=""paste-f911cfc27c1de5aa8efc98eb46a9aece152ee247.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   Part of the CPU <b>interacting with the cache and the memory</b> to fetch data."	"Address Generation Unit (AGU)  <div style=""font-style: oblique""><img src=""paste-f911cfc27c1de5aa8efc98eb46a9aece152ee247.jpg""></div>"
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  CPU's <b>instruction pipelining</b>"	"<b>Parallelism at the instruction-level</b> in the processor.<div><br></div><div><i>Enforces processor's always busy when others instructions are in a different cycle's step:</i></div><div><i><br></i></div><div><i>In cycle 4, the intruction 2 is being executed while the others instructions are being fetched or decoded:</i></div><div><img src=""paste-86a02ecabddd15d59f82e1554ca8c43a38ee0432.jpg""><br></div> <br/> <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   <b>Parallelism at the instruction-level</b> in the processor.<div><br></div><div><i>Enforces processor's always busy when others instructions are in a different cycle's step:</i></div><div><i><br></i></div><div><i>In cycle 4, the intruction 2 is being executed while the others instructions are being fetched or decoded:</i></div><div><img src=""paste-86a02ecabddd15d59f82e1554ca8c43a38ee0432.jpg""><br></div>"	"CPU's <b>instruction pipelining</b>  <div style=""font-style: oblique""></div>"
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  Name for processor doing <b>intruction pipelining</b> (instruction execution in parallel)"	"<b>Superscalar</b> architecture <br/> <div style=""font-style: oblique""><div>All modern processors are now superscalars.<br></div></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>   <b>Superscalar</b> architecture"	"Name for processor doing <b>intruction pipelining</b> (instruction execution in parallel)  <div style=""font-style: oblique""><div>All modern processors are now superscalars.<br></div></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Branch prediction"	"The processor's pipelining tries to <b>predict which branch </b>(<i>loops, conditions</i>)&nbsp;<b>will be taken</b>.<div><br></div><div><i>Can speed up execution by pipelining the branch before is known definitely.&nbsp;</i></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Why does <b>branch prediction </b>speed up"	"The processor can <b>pipeline instructions in the future branch in advance</b>.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What happens with <b>branch misprediction</b>"	"The processor failed to predict the right branch.<div><br></div><div>It <b>discards</b> <b>speculated branch instructions </b>and start anew on the right branch.</div><div><br></div><div>It's a <b>pipeline flush</b>.</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How does the processor does <b>branch prediction</b>"	"By seeing <b>how often a branch has been taken</b>.<div><br></div><div>Thus it may not do it the first time a branch is seen.&nbsp;</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> CPU's <b>pipeline flush</b>"	"Clearing the pipeline when a <b>branch misprediction </b>happens.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> \(2^{10} \, \text{bytes}\) = <span class=cloze>[]</span> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> \(2^{10} \, \text{bytes}\) = <span class=cloze>\(1 \,\text{kilobyte}\)</span><br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> \(2^{20} \, \text{bytes}\) = <span class=cloze>[]</span> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> \(2^{20} \, \text{bytes}\) = <span class=cloze>\(1 \,\text{megabyte}\)</span><br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> \(2^{30} \, \text{bytes}\) = <span class=cloze>[]</span> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> \(2^{30} \, \text{bytes}\) = <span class=cloze>\(1 \,\text{gigabyte}\)</span><br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Python:</div> <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000"">hasattr</span>(<span style=""color: #008000"">?</span>, ?)</pre></div></td></tr></tbody></table></center>"	"<center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000"">hasattr</span>(<span style=""color: #008000"">object</span>, name) </pre></div> </td></tr></tbody></table></center><br><div><i>Returns True if the object has the attribute &lt;name&gt;.</i></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Python:</div> <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000"">isinstance</span>(<span style=""color: #008000"">?</span>, <span style=""color: #008000"">?</span>) </pre></div> </td></tr></tbody></table></center>"	"<center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000"">isinstance</span>(<span style=""color: #008000"">object</span>, <span style=""color: #008000"">type</span>) </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Python:</div> <center><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">with</span> <span style=""color: #008000"">open</span>(<span style=""color: #008000; font-weight: bold"">False</span>) <span style=""color: #008000; font-weight: bold"">as</span> f:     f<span style=""color: #666666"">.</span>read() </pre></div> </td></tr></tbody></table></center><br></center>Behavior?"	"Open <b>stdin </b>and thus hang waiting to read an input.<div><br></div><div>False is evaluated as <b>0 the file descriptor of stdin</b>.</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Matrix \(M\)&nbsp;<b>orthogonal</b><div><b><br></b></div><div><b>Dot product </b>of two rows of \(M\)?</div>"	"<div>Dot product is <b>null</b>:</div><div><br></div>\(M = \left[\begin{array}{rrr}{\frac{2}{3}} &amp; {\frac{1}{3}} &amp; {\frac{2}{3}} \\ {-\frac{2}{3}} &amp; {\frac{2}{3}} &amp; {\frac{1}{3}} \\ {\frac{1}{3}} &amp; {\frac{2}{3}} &amp; {-\frac{2}{3}}\end{array}\right]\)<br><div><br></div><div>\(M_1 \cdot M_2 = 0\)</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Matrix \(M\) <b>orthogonal</b><div><b><br></b></div><div>\(M^{-1} =\) ...</div>"	"<div>\(M^{-1} = M^T\)</div><div><br></div><div>Equal to its&nbsp;<b>transpose</b>.<br></div><div><br></div>\(M = \left[\begin{array}{rrr}{\frac{2}{3}} &amp; {\frac{1}{3}} &amp; {\frac{2}{3}} \\ {-\frac{2}{3}} &amp; {\frac{2}{3}} &amp; {\frac{1}{3}} \\ {\frac{1}{3}} &amp; {\frac{2}{3}} &amp; {-\frac{2}{3}}\end{array}\right]\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Matrix \(M\)&nbsp;<b>orthogonal</b><div><b><br></b></div><div>\(M^T =\) ...</div>"	"<div>\(M^T = M^{-1}\)</div><div><br></div><div>Equal to its&nbsp;<b>inverse</b>.<br></div><div><br></div>\(M = \left[\begin{array}{rrr}{\frac{2}{3}} &amp; {\frac{1}{3}} &amp; {\frac{2}{3}} \\ {-\frac{2}{3}} &amp; {\frac{2}{3}} &amp; {\frac{1}{3}} \\ {\frac{1}{3}} &amp; {\frac{2}{3}} &amp; {-\frac{2}{3}}\end{array}\right]\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Matrix \(M\)&nbsp;<b>orthogonal</b><div><b><br></b></div><div><b>Norm</b> of any row?</div>"	"<div><b>Unit norm 1.</b></div><div><br></div>\(M = \left[\begin{array}{rrr}{\frac{2}{3}} &amp; {\frac{1}{3}} &amp; {\frac{2}{3}} \\ {-\frac{2}{3}} &amp; {\frac{2}{3}} &amp; {\frac{1}{3}} \\ {\frac{1}{3}} &amp; {\frac{2}{3}} &amp; {-\frac{2}{3}}\end{array}\right]\)<br><div><br></div><div>\(\Vert [\frac{2}{3},&nbsp; \frac{1}{3}, \frac{2}{3}]\Vert_2 = 1\)</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Matrix \(M\)&nbsp;<b>orthogonal</b><br><div><b><br></b></div><div>Condition?</div>"	"\(M M^{T}=M^{T} M=I\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(M M^{T}=M^{T} M=I\) implies that \(M\) is..."	"<b>Orthogonal</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Product of <b>eigenvalues</b>"	"Equal to the matrix <b>determinant</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Sum of <b>eigenvalues</b>"	"Equal to the <b>trace </b>of the matrix.  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> With 1000 servers, how many <b>fails per day</b>?<div><img src=""paste-f1a2baf3a96a3c05043841d1adbabd8392029ce2.jpg""><br></div>"	"1 per day.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Chunk server"	"Server that <b>stores a subset</b> of a file.<div><br></div><div>In a distributed file system.</div><div><img src=""paste-2cd5adcfd63ebfbf0a415a50e36481e037751d3f.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Master node"	"Server that <b>stores the file metadata</b>.<div><br></div><div>In a distributed file system.<br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> What server stores <b>metadata </b>in a distributed file system"	"Master node  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> What server stores <b>file subset&nbsp;</b>in a distributed file system"	"Chunk server<div><img src=""paste-2cd5adcfd63ebfbf0a415a50e36481e037751d3f.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> How client <b>reads</b> <b>file</b> in a distributed file system"	"1. Ask <b>master node </b>for file location<div><br></div><div>2. Read on <b>chunk server</b></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> How to avoid <b>data loss</b> in a distributed file system"	"<b>Replication </b>on several chunk servers<div><img src=""paste-2cd5adcfd63ebfbf0a415a50e36481e037751d3f.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> 3 steps of <b>MapReduce</b><div><b><br></b></div><div><span class=cloze>[]</span></div><div><br></div><div><u>Group</u>: sort item by key.</div><div><br></div><div><u>Reduce</u>: Merge values of same key.</div> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> 3 steps of <b>MapReduce</b><div><b><br></b></div><div><span class=cloze><u>Map</u>: apply function item-wise.</span></div><div><br></div><div><u>Group</u>: sort item by key.</div><div><br></div><div><u>Reduce</u>: Merge values of same key.</div><br> <img src=""paste-cd659220633e38ca30281d0e4e1faeeb22114cc6.jpg"">"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> 3 steps of <b>MapReduce</b><div><b><br></b></div><div><u>Map</u>: apply function item-wise.</div><div><br></div><div><span class=cloze>[]</span></div><div><br></div><div><u>Reduce</u>: Merge values of same key.</div> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> 3 steps of <b>MapReduce</b><div><b><br></b></div><div><u>Map</u>: apply function item-wise.</div><div><br></div><div><span class=cloze><u>Group</u>: sort item by key.</span></div><div><br></div><div><u>Reduce</u>: Merge values of same key.</div><br> <img src=""paste-cd659220633e38ca30281d0e4e1faeeb22114cc6.jpg"">"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> 3 steps of <b>MapReduce</b><div><b><br></b></div><div><u>Map</u>: apply function item-wise.</div><div><br></div><div><u>Group</u>: sort item by key.</div><div><br></div><div><span class=cloze>[]</span></div> "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> 3 steps of <b>MapReduce</b><div><b><br></b></div><div><u>Map</u>: apply function item-wise.</div><div><br></div><div><u>Group</u>: sort item by key.</div><div><br></div><div><span class=cloze><u>Reduce</u>: Merge values of same key.</span></div><br> <img src=""paste-cd659220633e38ca30281d0e4e1faeeb22114cc6.jpg"">"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> <b>Map </b>step in <b>MapReduce</b>"	"<b>Apply</b> a function <b>item-wise</b>.<div><br></div><div><img src=""paste-cd659220633e38ca30281d0e4e1faeeb22114cc6.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> <b>Group </b>step of <b>MapReduce</b>"	"<b>Regroup</b> all values with <b>same key</b>.<div><img src=""paste-cd659220633e38ca30281d0e4e1faeeb22114cc6.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> <b>Reduce </b>step of <b>MapReduce</b>"	"<b>Merge </b>all values with <b>same key</b>&nbsp;<i>(sum, mean, etc.)</i><div><img src=""paste-cd659220633e38ca30281d0e4e1faeeb22114cc6.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> <b>MapReduce </b>in <b>Parallel</b>"	"Each operation can be done on different machines.<div><br></div><div>Last step of <b>Map</b>&nbsp;(partitionning) must put on same machines items with same key.</div><div><img src=""paste-9c1de0466ac58c9ff2649c62232faac0a0779f0e.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> <b>Partionning </b>step of <b>MapReduce</b>"	"After <b>Map</b>, put items with <b>same key</b>&nbsp;on the <b>same machine</b>.<div><img src=""paste-9c1de0466ac58c9ff2649c62232faac0a0779f0e.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> When does <b>partionning </b>happen in <b>MapReduce</b>"	"Just after the <b>Map </b>operation:<div><img src=""paste-9c1de0466ac58c9ff2649c62232faac0a0779f0e.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Overhead in <b>MapReduce</b>"	"Data replication and <b>disk I/O</b>:<div><img src=""paste-c26df758d2e6bd6998b599245b14770dd917206a.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Spark's <b>RDD</b>&nbsp;acronym"	"<b>R</b>esilient <b>D</b>istributed <b>D</b>atasets  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> What does Spark's <b>RDD </b>solves from <b>MapReduce</b>"	"MapReduce<b> </b>is slow because of <b>disk I/O</b>.<div><br></div><div>RDD instead do everything <b>in-memory </b>in a distributed memory.</div><div><img src=""paste-629fe1ec02098d9c49cc7efcc8f146517181d1fb.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Advantage of Spark's <b>RDD </b>against <b>MapReduce </b>for <b>interactive query</b>"	"<b>RDD </b>reads once the data then <b>cache it</b>.<div><br></div><div><i>MapReduce:</i></div><div><img src=""paste-69b6ce38f3364c3d5207ca497ff0eaa4b4f4d73d.jpg""><i><br></i></div><div><i>RDD:</i></div><div><img src=""paste-09fa509dbc15fe21f965b95273769e652b14d642.jpg""><i><br></i></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Hardware requirements for <b>Spark</b>"	"Spark needs a <b>lot of memory</b>&nbsp;has it tries to keep everything in-memory.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Jaccard Index"	"\(J(A, B)=\frac{|A \cap B|}{|A \cup B|}\)<div><img src=""paste-88d910c86e2c3354193e98e95d7758d37ccb1fc0.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Distance to <b>compare two sets</b> of words"	"Jaccard index<div>\(J(A, B)=\frac{|A \cap B|}{|A \cup B|}\)</div><div><img src=""paste-88d910c86e2c3354193e98e95d7758d37ccb1fc0.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Big Data</div> Cluster vs outliers"	"<img src=""paste-72902ad5f25b43cb89f2e8c25ffd71be47f0b885.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> How can we <b>swap instructions order to speed up</b><div><img src=""paste-b9003f473a5cfc57bbf5f945529d7c49409fcfd6.jpg""><br></div>"	"Swap #2 with #3<div><br></div><div><i>Because #2 has <b>dependencies</b> on #1 which means it needs it to finish, while the cpu could pipeline early work for #3 directly.</i></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> What ""<b>acquire semantics</b>"" mean about CPU instructions<div><img src=""paste-b9003f473a5cfc57bbf5f945529d7c49409fcfd6.jpg""><br></div>"	"If acquiring semantic of #2, <b>results of all previous instructions must have been completed</b>.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> What ""<b>release semantics</b>"" mean about CPU instructions<div><img src=""paste-b9003f473a5cfc57bbf5f945529d7c49409fcfd6.jpg""></div>"	"If releasing semantic of #2, <b>all instructions after this one must see the current result</b>. (<i>i.e. must wait for #2 to finish)</i>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> <b>Memory barrier</b>&nbsp;requirements of the CPU instructions<div><img src=""paste-b9003f473a5cfc57bbf5f945529d7c49409fcfd6.jpg""><br></div>"	"A requirement (not common) where instructions must have been <b>committed to memory before the following can be executed</b>.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> How long an instruction lasts with <b>CISC</b>"	"Potentially <b>several</b> <b>cycles </b>depending on the complexity.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> How long an instruction lasts with&nbsp;<b>RISC</b>"	"A <b>single cycle </b>whatever the instruction.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Advantage of <b>CISC </b>vs <b>RISC</b>"	"<b>CISC </b>assembly is simpler:<div><img src=""paste-4fe5c64aa89b8e0c07e5b0195a9847bc2bbe2d3b.jpg""><br></div><div>Than <b>RISC</b>:</div><div><img src=""paste-c895074b81d2c8a043fee2130217c89c250279b3.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> <b>CISC </b>or <b>RISC </b>can do <b>pipelining</b>"	"Only <b>RISC</b><div><img src=""paste-c895074b81d2c8a043fee2130217c89c250279b3.jpg""><b><br></b></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Why <b>CISC </b>cannot do <b>pipelining</b>"	"Because instructions are not time-uniform, <b>some may be longer than other</b><div><img src=""paste-4fe5c64aa89b8e0c07e5b0195a9847bc2bbe2d3b.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Why&nbsp;<b>RISC&nbsp;</b>can do&nbsp;<b>pipelining</b>"	"Because all instructions last the <b>same amount of time</b><div><img src=""paste-c895074b81d2c8a043fee2130217c89c250279b3.jpg""><b><br></b></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> How can <b>CISC </b>do complex instructions<div><img src=""paste-4fe5c64aa89b8e0c07e5b0195a9847bc2bbe2d3b.jpg""><br></div>"	"Because CPU's <b>hardware</b> has the instructions <b>hardcoded</b>.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> <b>CISC </b>acronyme"	"<b>C</b>omplex<b> I</b>nstruction <b>S</b>et <b>C</b>omputer  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> <b>RISC </b>acronyme"	"<b>R</b>educed <b>I</b>nstruction <b>S</b>et <b>C</b>omputer  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> <b>Memory hierarchy </b>between RAM, register, cache, and&nbsp;disk"	"<img src=""paste-78503a36b6607c6fc7b976550d8c5f78697af1e0.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Where are usually located <b>caches </b>(L1, L2, L3)"	"In the CPU<div><img src=""paste-995fa1685b84e89ee39177aede0bf34c17c4993d.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> One reason why <b>caches </b>cannot be very big"	"Because they embedded into the CPU for performance reasons<div><img src=""paste-995fa1685b84e89ee39177aede0bf34c17c4993d.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Which <b>cache </b>is the fastest"	"L1 &gt; L2 &gt; L3<div><img src=""paste-e8f22896b5669ea8f46a3d076303d873eb999312.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>  Cache line"	"A small <b>chunk of fixed size</b> of a CPU's cache <br/> <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>   A small <b>chunk of fixed size</b> of a CPU's cache"	"Cache line  <div style=""font-style: oblique""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> The cache can only load and store memory in sizes a multiple of <span class=cloze>[]</span>. "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> The cache can only load and store memory in sizes a multiple of <span class=cloze>a cache line</span>.<br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Two kinds of <b>L1 cache</b>"	"<b>Instruction cache</b> to speed up executable instruction fetch<div><b><br></b></div><div><b>Data cache</b> to speed up data fetch and store<br></div><div><img src=""paste-03975ecfa68add2c46901da5e1f2840558ddd09e.jpg""><br></div><div><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> <b>Instruction cache</b>: write / read?<div><img src=""paste-03975ecfa68add2c46901da5e1f2840558ddd09e.jpg""><br></div>"	"Read-only  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> <b>Data cache</b>: read / write?<div><img src=""paste-03975ecfa68add2c46901da5e1f2840558ddd09e.jpg""></div>"	"Read and Write  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Number of <b>cycles</b> to access <b>register</b>"	"1 cycle<div><img src=""paste-224b35c716ce6a297c3b57d74fba488a28aa4079.jpg""><br></div><div>Only in RISK architecture.</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Number of <b>cycles </b>to access <b>caches</b>"	"At most a <b>dozen cycles</b><div><img src=""paste-224b35c716ce6a297c3b57d74fba488a28aa4079.jpg""><b><br></b></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Number of <b>cycles </b>to access <b>Main Memory</b>"	"<div><b>Hundred</b>&nbsp;of cycles</div><img src=""paste-224b35c716ce6a297c3b57d74fba488a28aa4079.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>  Cache hit"	"When CPU already <b>finds the data it needs on the cache</b> <br/> <div style=""font-style: oblique""><img src=""paste-03975ecfa68add2c46901da5e1f2840558ddd09e.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>   When CPU already <b>finds the data it needs on the cache</b>"	"Cache hit  <div style=""font-style: oblique""><img src=""paste-03975ecfa68add2c46901da5e1f2840558ddd09e.jpg""></div>"
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>  Cache miss"	"When <b>needed data isn't on the cache</b> and needs to be copied from memory/disk. <br/> <div style=""font-style: oblique""><img src=""paste-03975ecfa68add2c46901da5e1f2840558ddd09e.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>   When <b>needed data isn't on the cache</b> and needs to be copied from memory/disk."	"Cache miss  <div style=""font-style: oblique""><img src=""paste-03975ecfa68add2c46901da5e1f2840558ddd09e.jpg""></div>"
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>  <b>Direct Mapping</b> caches"	"<div>Cache line position in the entries is <b>unique</b> and <b>determined by the memory address</b> it references.</div> <br/> <div style=""font-style: oblique""><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Main advantage of <b>Direct Mapping </b>cache"	"<b>Fast to search</b> if data is present as there is a single position.<div><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Main disadvantage of <b>Direct Mapping </b>cache"	"<div><b>Low cache hit rate </b>(high cache miss rate).</div><div><b><br></b></div><div>Every time a new memory is referenced to the same set, the cache line is replaced, which causes conflict miss.<b><br></b></div><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>  <b>Fully Associative </b>caches"	"<div>Cache line position <b>can be in any entries </b>of the cache.</div> <br/> <div style=""font-style: oblique""><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Main advantages of <b>Fully Associative&nbsp;</b>cache"	"<div>Cache is <b>fully used</b> and has a <b>high cache hit rate</b>.</div><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Main disadvantage of <b>Fully Associative&nbsp;</b>cache"	"<div><b>Slow to search </b>because it needs to iterate through all lines.</div><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>  <b>Set Associative&nbsp;</b>caches"	"<div>Cache position <b>can be potentially in one of N entries</b> determined by the memory address it references.</div> <br/> <div style=""font-style: oblique""><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>   <div>Cache position <b>can be potentially in one of N entries</b> determined by the memory address it references.</div>"	"<b>Set Associative&nbsp;</b>caches  <div style=""font-style: oblique""><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>   <div>Cache line position in the entries is <b>unique</b> and <b>determined by the memory address</b> it references.</div>"	"<b>Direct Mapping</b> caches  <div style=""font-style: oblique""><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div>   <div>Cache line position <b>can be in any entries </b>of the cache.</div>"	"<b>Fully Associative </b>caches  <div style=""font-style: oblique""><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Main disadvantage of <b>Set Associative&nbsp;</b>cache"	"<div>The cache may <b>not be fully used</b>, producing a <b>higher cache miss rate</b>.</div><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Computer Architecture</div> Main advantages of <b>Set Associative&nbsp;</b>cache"	"<b>Lower cache hit rate</b> than direct mapping.<div><br></div><div><b>Faster to search </b>than fully associative.</div><div><img src=""paste-fb650504fbd33ff234eab18d402a941e7f973151.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  <div>\(X\) and \(Y\) <b>independent</b>.</div><div><br></div>\(\mathrm{E}[X Y]=\) <span class=cloze>[]</span>&nbsp; "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  <div>\(X\) and \(Y\) <b>independent</b>.</div><div><br></div>\(\mathrm{E}[X Y]=\) <span class=cloze>\(\mathrm{E}[X] \mathrm{E}[Y]\)</span>&nbsp;<br><br> <div style=""font-style: italic; font-size: 14px""><i>Non-multiplicativity</i></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(\mathrm{E}[X + Y]=\) <span class=cloze>[]</span>&nbsp; "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(\mathrm{E}[X + Y]=\) <span class=cloze>\(\mathrm{E}[X] + \mathrm{E}[Y]\)</span>&nbsp;<br><br> <div style=""font-style: italic; font-size: 14px""><i>Linearity of expectation</i></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(\mathrm{E}[\alpha X]=\) <span class=cloze>[]</span>&nbsp; "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(\mathrm{E}[\alpha X]=\) <span class=cloze>\(\alpha \mathrm{E}[X]\)</span>&nbsp;<br><br> <div style=""font-style: italic; font-size: 14px""><i>Linearity of expectation</i></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  <div>Using only <b>expected value</b>.</div><div><br></div>\(\text{Var}(X) = \) <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  <div>Using only <b>expected value</b>.</div><div><br></div>\(\text{Var}(X) = \) <span class=cloze>\(\mathrm{E}[X^2] - \mathrm{E}^2[X]\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(\mathrm{E}[X^2]&nbsp;= \) <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(\mathrm{E}[X^2]&nbsp;= \) <span class=cloze>\(\text{Var}(X) + \mathrm{E}^2[X]\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  <b>Polynomial </b>curve fitting"	"\(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  \(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)"	"<b>Polynomial </b>curve fitting <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)<br><div><br></div><div>\(y\) is a <span class=cloze>[]</span> function of \(x\).</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)<br><div><br></div><div>\(y\) is a <span class=cloze>non-linear</span> function of \(x\).</div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)<br><div><br></div><div>\(y\) is a <span class=cloze>[]</span> function of \(\mathbf{w}\).</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)<br><div><br></div><div>\(y\) is a <span class=cloze>linear</span> function of \(\mathbf{w}\).</div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  <b>Quadratic</b> function"	"function like \(ax^2 + bx + c\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  function like \(ax^2 + bx + c\)"	"<b>Quadratic</b> function <br/> <div style=""font-style: italic""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>Model selection </b>for a polynomial function like:<div>\(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)<br></div>"	"Choosing the optimal value \(M\) for a validation set.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When \(M\) is <b>too low</b> in:<div>\(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)<br></div>"	"<b>Underfitting</b><div><img src=""paste-c94c31f77a3db1e422fbaf470c6d9161db8646ad.jpg""><b><br></b></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When \(M\) is&nbsp;<b>too high</b>&nbsp;in:<div>\(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)</div>"	"<b>Overfitting</b><div><img src=""paste-340817c44548b48e7882d2cc94ff38588fa09a41.jpg""><b><br></b></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Mean-square error"	"\(E(\mathbf{w})=\frac{1}{2N} \sum_{n=1}^{N}\left\{y\left(x_{n}, \mathbf{w}\right)-t_{n}\right\}^{2}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which <b>error </b>for <b>polynomial curve fitting<br></b><div>\(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)&nbsp;</div>"	"<b>Mean-Square error</b><div>\(E(\mathbf{w})=\frac{1}{2N} \sum_{n=1}^{N}\left\{y\left(x_{n}, \mathbf{w}\right)-t_{n}\right\}^{2}\)<b><br></b></div><div><img src=""paste-103c5c86405379eab6f5ab818a96de0925ed0580.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Values of \(w_i\) when \(M\) is <b>high</b>:<div>\(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)<br></div>"	"\(w_i\) are also <b>high</b><div><img src=""paste-4244332e0e71ba232a7853efd525579ee1835fef.jpg""><b><br></b></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Intuitively, what happens when \(w_i\) are very <b>high</b><div>\(y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\ldots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}\)<b><br></b></div><div><img src=""paste-4244332e0e71ba232a7853efd525579ee1835fef.jpg""><br></div>"	"The model tries to <b>perfectly fit the curve</b>.<div><br></div><div>It results in an <b>overfit</b>.</div><div><img src=""paste-340817c44548b48e7882d2cc94ff38588fa09a41.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How <b>overfit </b>evolves when the <b>dataset size increase</b>"	"<div><b>Overfitting reduces</b></div><div><img src=""paste-1d486b2cf4d5b5ce00faa3169ac1ea9263f9d7bf.jpg""><b><br></b></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Effect of <b>weight decay </b>on \(w_i\):<div>\(\widetilde{E}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(x_{n}, \mathbf{w}\right)-t_{n}\right\}^{2}+\frac{\lambda}{2}\|\mathbf{w}\|^{2}\)<br></div>"	"Values are <b>smaller</b>.<div><img src=""paste-ad423409331f446579e15bdaa4ac51f143b1af96.jpg""><br></div><div><i>Left column is without weight decay.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Weight decay"	"<div><b>Penalize weight magnitude</b>.</div>\(E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}L(w, x)+\frac{\lambda}{2}\|\mathbf{w}\|^{2}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  Probability name when two events \(X\) and \(Y\) happen together"	"<b>Joint </b>probability<div>\(p(X, Y)\)&nbsp;</div> <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  <b>Joint </b>probability<div>\(p(X, Y)\)&nbsp;</div>"	"Probability name when two events \(X\) and \(Y\) happen together <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  <b>Sum </b>rule"	"\(p(X=x) = \Sigma_{i=1}\, p(X=x, Y=y_i)\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  \(p(X=x) = \Sigma_{i=1}\, p(X=x, Y=y_i)\)"	"<b>Sum </b>rule <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  <b>Product </b>rule"	"\(p(X | Y) = p(Y | X) p(X)\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  \(p(X | Y) = p(Y | X) p(X)\)"	"<b>Product </b>rule <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  <b>Bayes </b>law"	"\(p(X | Y) = \frac{p(Y | X) p(X)}{p(Y)}\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  \(p(X | Y) = \frac{p(Y | X) p(X)}{p(Y)}\)"	"<b>Bayes </b>law <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(X\) and \(Y\) are <b>independent</b>.<div>\(p(X, Y) = \) <span class=cloze>[]</span></div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(X\) and \(Y\) are <b>independent</b>.<div>\(p(X, Y) = \) <span class=cloze>\(p(X)p(Y)\)</span></div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(X\) and \(Y\) are <span class=cloze>[]</span>.<div>\(p(X, Y) = \) \(p(X)p(Y)\)</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(X\) and \(Y\) are <span class=cloze><b>independent</b></span>.<div>\(p(X, Y) = \) \(p(X)p(Y)\)</div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  When \(x\) is <span class=cloze>[]</span>.<div><br></div><div>\(p(x\in(a,b)) = \) \(\int_a^b p(x)dx\)</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  When \(x\) is <span class=cloze><b>continuous</b></span>.<div><br></div><div>\(p(x\in(a,b)) = \) \(\int_a^b p(x)dx\)</div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  When \(x\) is <b>continuous</b>.<div><br></div><div>\(p(x\in(a,b)) = \) <span class=cloze>[]</span></div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  When \(x\) is <b>continuous</b>.<div><br></div><div>\(p(x\in(a,b)) = \) <span class=cloze>\(\int_a^b p(x)dx\)</span></div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(p(x\in(a,b)) = \int_a^b p(x)dx\)<div><br></div><div>\(p(x) \ge\) <span class=cloze>[]</span></div><div><br></div><div>\(\int_{-\infty}^{+\infty} p(x) dx = \) \(1\)</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(p(x\in(a,b)) = \int_a^b p(x)dx\)<div><br></div><div>\(p(x) \ge\) <span class=cloze>\(0\)</span></div><div><br></div><div>\(\int_{-\infty}^{+\infty} p(x) dx = \) \(1\)</div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(p(x\in(a,b)) = \int_a^b p(x)dx\)<div><br></div><div>\(p(x) \ge\) \(0\)</div><div><br></div><div>\(\int_{-\infty}^{+\infty} p(x) dx = \) <span class=cloze>[]</span></div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline"">Probability</div>  \(p(x\in(a,b)) = \int_a^b p(x)dx\)<div><br></div><div>\(p(x) \ge\) \(0\)</div><div><br></div><div>\(\int_{-\infty}^{+\infty} p(x) dx = \) <span class=cloze>\(1\)</span></div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  <b>Cumulative </b>distribution function"	"<div>Probability that \(x \in (-\infty, z)\):</div><div><br></div>\(P(z)=\int_{-\infty}^{z} p(x) \mathrm{d} x\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  <div>Probability that \(x \in (-\infty, z)\):</div><div><br></div>\(P(z)=\int_{-\infty}^{z} p(x) \mathrm{d} x\)"	"<b>Cumulative </b>distribution function <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  <b>Expectation </b>of a continuous function \(f(x)\) under a probability distribution \(p(x)\)&nbsp;"	"\(\mathbb{E}[f]=\int p(x) f(x) \mathrm{d} x\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Probability</div>  \(\mathbb{E}[f]=\int p(x) f(x) \mathrm{d} x\)"	"<b>Expectation </b>of a continuous function \(f(x)\) under a probability distribution \(p(x)\)&nbsp; <br/> <div style=""font-style: italic""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  How to get <b>expectation </b>of a function with a <b>finite</b> number of points<div>\(\mathbb{E}[f]=\int p(x) f(x) \mathrm{d} x\)<br></div>"	"By <b>sampling </b>them:<div>\(\mathbb{E}[f] \simeq \frac{1}{N} \sum_{n=1}^{N} f\left(x_{n}\right)\)</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Parallelism"	"Two tasks literally run at the same time<br><div><img src=""paste-645fcc08621d8746f10f23d6dcf33c59a1d92273.jpg""><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Concurrency"	"<div>Two tasks can be ‘<b>in progress</b>’ at the same time</div><img src=""paste-645fcc08621d8746f10f23d6dcf33c59a1d92273.jpg"">  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Does <b>multi-processes </b>share memory?"	"No  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Does&nbsp;<b>multithreading&nbsp;</b>share memory?"	"Yes  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Mutex</b> (or lock)"	"Synchronisation mechanism enforcing that a <b>single thread can access its critical section of shared memory</b>.<div><br></div><div>Others threads wanting to access the same section are put on hold.</div><div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">lock <span style=""color: #666666"">=</span> Lock()  lock<span style=""color: #666666"">.</span>acquire() <span style=""color: #666666"">...</span> access shared resource lock<span style=""color: #666666"">.</span>release()</pre></div></td></tr></tbody></table></center><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>GIL </b>acronym"	"<b>G</b>lobal <b>I</b>nterpreter <b>L</b>ock  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>G</b>lobal&nbsp;<b>I</b>nterpreter&nbsp;<b>L</b>ock (GIL)"	"<b>Mutex</b> that protects access to Python objects, <b>preventing multiple threads from executing Python bytecodes at once</b>.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Multi-<span class=cloze>[]</span> is good for <b>IO</b>-bound tasks. "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Multi-<span class=cloze><b>threading</b></span> is good for <b>IO</b>-bound tasks.<br> <div><br></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Multi-<b>threading</b> is good for <span class=cloze>[]</span>-bound tasks. "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Multi-<b>threading</b> is good for <span class=cloze><b>IO</b></span>-bound tasks.<br> <div><br></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Multi-<span class=cloze>[]</span> is good for <b>CPU</b>-bound tasks. "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Multi-<span class=cloze><b>processing</b></span> is good for <b>CPU</b>-bound tasks.<br> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Multi-<b>processing</b> is good for <span class=cloze>[]</span>-bound tasks. "	"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Multi-<b>processing</b> is good for <span class=cloze><b>CPU</b></span>-bound tasks.<br> "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  The <b>variance </b>of a single-variate Gaussian \(\mathcal{N}(\mu, \sigma^2)\) for <b>multi-variate Gaussian</b>"	"<div><b>Covariance </b>matrix: \(\mathcal{N}(\mu, \Sigma)\)</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Diagonal of a <b>covariance </b>matrix \(\Sigma\)"	"The <b>variance</b>&nbsp;\(\sigma^2\) of each dimension&nbsp;  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <b>Transpose </b>of a <b>covariance </b>matrix \(\Sigma\)"	"\(\Sigma^T = \Sigma\)<div>The matrix is <b>symetric</b>.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <b>Marginalization </b>of a multivariate probability distribution<div><br><div>\(P_{X, Y}=\left[\begin{array}{c}{X} \\ {Y}\end{array}\right] \sim \mathcal{N}(\mu, \Sigma)=\mathcal{N}\left(\left[\begin{array}{c}{\mu_{X}} \\ {\mu_{Y}}\end{array}\right],\left[\begin{array}{c}{\Sigma_{X X} \Sigma_{X Y}} \\ {\Sigma_{Y X} \Sigma_{Y Y}}\end{array}\right]\right)\)</div></div>"	"\(\begin{array}{l}{X \sim \mathcal{N}\left(\mu_{X}, \Sigma_{X X}\right)} \\ {Y \sim \mathcal{N}\left(\mu_{Y}, \Sigma_{Y Y}\right)}\end{array}\)<div><br></div><div><i>Each partition \(X\) and \(Y\) only depends on its corresponding entries in \(\mu\) and \(\Sigma\).</i><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  \(P_{X, Y}=\left[\begin{array}{c}{X} \\ {Y}\end{array}\right] \sim \mathcal{N}(\mu, \Sigma)=\mathcal{N}\left(\left[\begin{array}{c}{\mu_{X}} \\ {\mu_{Y}}\end{array}\right],\left[\begin{array}{c}{\Sigma_{X X} \Sigma_{X Y}} \\ {\Sigma_{Y X} \Sigma_{Y Y}}\end{array}\right]\right)\)<br><div><br></div><div>Name of the following operations:</div><div>\(\begin{array}{l}{X \sim \mathcal{N}\left(\mu_{X}, \Sigma_{X X}\right)} \\ {Y \sim \mathcal{N}\left(\mu_{Y}, \Sigma_{Y Y}\right)}\end{array}\)<br></div>"	"<b>Marginalization&nbsp;</b>of a multivariate probability distribution  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  \(P_{X, Y}=\left[\begin{array}{c}{X} \\ {Y}\end{array}\right] \sim \mathcal{N}(\mu, \Sigma)=\mathcal{N}\left(\left[\begin{array}{c}{\mu_{X}} \\ {\mu_{Y}}\end{array}\right],\left[\begin{array}{c}{\Sigma_{X X} \Sigma_{X Y}} \\ {\Sigma_{Y X} \Sigma_{Y Y}}\end{array}\right]\right)\)<br><div><br></div><div>Name of the following operations:</div><div>\(\begin{array}{l}{X | Y \sim \mathcal{N}\left(\mu_{X}+\Sigma_{X Y} \Sigma_{Y Y}^{-1}\left(Y-\mu_{Y}\right), \Sigma_{X X}-\Sigma_{X Y} \Sigma_{Y Y}^{-1} \Sigma_{Y X}\right)} \\ {Y | X \sim \mathcal{N}\left(\mu_{Y}+\Sigma_{Y X} \Sigma_{X X}^{-1}\left(X-\mu_{X}\right), \Sigma_{Y Y}-\Sigma_{Y X} \Sigma_{X X}^{-1} \Sigma_{X Y}\right)}\end{array}\)</div>"	"<b>Conditionning </b>of a multivariate probability distribution  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Gaussian property of <b>being closed under conditioning and marginalization</b>.<div>\(P_{X, Y}=\left[\begin{array}{c}{X} \\ {Y}\end{array}\right] \sim \mathcal{N}(\mu, \Sigma)=\mathcal{N}\left(\left[\begin{array}{c}{\mu_{X}} \\ {\mu_{Y}}\end{array}\right],\left[\begin{array}{c}{\Sigma_{X X} \Sigma_{X Y}} \\ {\Sigma_{Y X} \Sigma_{Y Y}}\end{array}\right]\right)\)</div>"	"<b>Resulting distributions are also gaussian.</b><div><br></div><div><i>Conditionning:</i></div><div>\(\begin{array}{l}{X | Y \sim \mathcal{N}\left(\mu_{X}+\Sigma_{X Y} \Sigma_{Y Y}^{-1}\left(Y-\mu_{Y}\right), \Sigma_{X X}-\Sigma_{X Y} \Sigma_{Y Y}^{-1} \Sigma_{Y X}\right)} \\ {Y | X \sim \mathcal{N}\left(\mu_{Y}+\Sigma_{Y X} \Sigma_{X X}^{-1}\left(X-\mu_{X}\right), \Sigma_{Y Y}-\Sigma_{Y X} \Sigma_{X X}^{-1} \Sigma_{X Y}\right)}\end{array}\)<i><br></i></div><div><br></div><div><i>Marginalization:</i></div><div>\(\begin{array}{l}{X \sim \mathcal{N}\left(\mu_{X}, \Sigma_{X X}\right)} \\ {Y \sim \mathcal{N}\left(\mu_{Y}, \Sigma_{Y Y}\right)}\end{array}\)<i><br></i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Common application of <b>Gaussian Process</b>"	"<b>Regression </b>for time-series<div><img src=""paste-a233639474143f36975a2471b5d1755f19a726a3.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Using only <b>expected value</b>:<div><br></div><div>\(\text{cov}[x, y] = \) <span class=cloze>[]</span></div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Using only <b>expected value</b>:<div><br></div><div>\(\text{cov}[x, y] = \) <span class=cloze>\(\mathrm{E}_{x,y}[xy] - \mathrm{E}_x[x]\mathrm{E}_y[y]\)</span></div><br><br> <div style=""font-style: italic; font-size: 14px"">If \(x\) and \(x\) are independent, their covariance vanishes.</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  With few examples (<i>whether Arctic ice cap will be there next century</i>) best solution is <b>frequentist </b>or <b>bayesian</b>"	"<b>Bayesian </b>as there is not repeated event so we cannot use frequentist, but can still use a <b>prior</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>Name of \(p(\mathcal{D} | \mathbf{w})\)</div>"	"<b>Likelihood function</b><div><b><br></b></div><div><i>We want to maximize this probability.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<div><br></div><div>How to interpret \(p(\mathbf{w} | \mathcal{D})\)</div>"	"<b>Uncertainty </b>of \(\mathbf{w}\) after we have seen the dataset \(\mathcal{D}\).<div><br></div><div><i>It is called the posterior. The lower it is, the more uncertaint we are.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<div><br></div><div>Name of \(p(\mathbf{w} | \mathcal{D})\)</div>"	"<b>Posterior </b>probability.<div><br></div><div><i>As what is \(\mathbf{w}\) once we have seen the dataset \(\mathcal{D}\). It models the uncertainty.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>What is the <b>posterior </b>here</div>"	"\(p(\mathbf{w} | \mathcal{D})\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>What is the&nbsp;<b>likelihood&nbsp;</b>here</div>"	"\(p(\mathcal{D} | \mathbf{w})\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>What is the&nbsp;<b>prior&nbsp;</b>here</div>"	"\(p(\mathbf{w})\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Estimator name of <b>choosing the </b>\(\mathbf{w}\) <b>maximizing the likelihood</b> \(p(\mathcal{D} | \mathbf{w})\)."	"Maximum Likelihood  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Maximum Likelihood"	"Estimator where we choose the parameter \(\mathbf{w}\) such that is <b>maximize the likelihood</b>&nbsp;\(p(\mathcal{D} | \mathbf{w})\).  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  Frequentist's <b>bootstrap</b>"	"Method to evaluate the <b>uncertainty </b>(with error bars) by training a <b>different subsets </b>of the dataset.<div><br></div><div>The watched metric is the <b>variance </b>between runs.</div> <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  Method to evaluate the <b>uncertainty </b>(with error bars) by training a <b>different subsets </b>of the dataset.<div><br></div><div>The watched metric is the <b>variance </b>between runs.</div>"	"Frequentist's <b>bootstrap</b> <br/> <div style=""font-style: italic""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>Which <b>distribution</b></div>\(\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}\)"	"<b>Gaussian </b>distribution<div><br></div><div>\(\mathcal{N}\left(x | \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}\)</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why often using the <b>log </b>of probabilities <i>(such as the likelihood</i>)"	"Because the <b>product of probabilities may underflow</b>.<div><br></div><div>By taking the log, it becomes a sum of log probabilities.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  Sample <b>variance</b>"	"\(\sigma^2 = \frac{1}{N}\sum_{n=1}^N (x_n - \mu)^2\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  \(\sigma^2 = \frac{1}{N}\sum_{n=1}^N (x_n - \mu)^2\)"	"Sample <b>variance</b> <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  \(\sigma^2\)"	"Variance <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  Variance"	"\(\sigma^2\) <br/> <div style=""font-style: italic""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Problem of the <b>sample variance</b>&nbsp;computed with <b>maximum likelihood</b> compared to true variance"	"It <b>underestimate </b>the true variance by a factor \(\frac{N-1}{N}\).<div><b><br></b></div><div>\(\begin{aligned} \mathbb{E}\left[\mu_{\mathrm{ML}}\right] &amp;=\mu \\ \mathbb{E}\left[\sigma_{\mathrm{ML}}^{2}\right] &amp;=\left(\frac{N-1}{N}\right) \sigma^{2} \end{aligned}\)</div><div><br></div><div><i>It is less important with enough data.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does the <b>maximum likelihood underestimate</b>"	"The <b>variance</b>.<div><br></div><div>\(\begin{aligned} \mathbb{E}\left[\mu_{\mathrm{ML}}\right] &amp;=\mu \\ \mathbb{E}\left[\sigma_{\mathrm{ML}}^{2}\right] &amp;=\left(\frac{N-1}{N}\right) \sigma^{2} \end{aligned}\)<br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  To what is related the <b>variance underestimation </b>of the <b>maximum likelihood</b><div>\(\begin{aligned} \mathbb{E}\left[\mu_{\mathrm{ML}}\right] &amp;=\mu \\ \mathbb{E}\left[\sigma_{\mathrm{ML}}^{2}\right] &amp;=\left(\frac{N-1}{N}\right) \sigma^{2} \end{aligned}\)<b><br></b></div>"	"To <b>overfitting</b>.<div><br></div><div><i>And as overfitting, it can be reduced with more data.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why <b>maximum likelihood </b>induces the <b>overfitting</b>"	"Because it <b>underestimate the true variance</b>:<div><br></div><div><div>\(\begin{aligned} \mathbb{E}\left[\mu_{\mathrm{ML}}\right] &amp;=\mu \\ \mathbb{E}\left[\sigma_{\mathrm{ML}}^{2}\right] &amp;=\left(\frac{N-1}{N}\right) \sigma^{2} \end{aligned}\)</div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When does the <b>maximum likelihood</b>'s <b>bias </b>reduce"	"With <b>more data</b>, as seen there:<div>\(\begin{aligned} \mathbb{E}\left[\mu_{\mathrm{ML}}\right] &amp;=\mu \\ \mathbb{E}\left[\sigma_{\mathrm{ML}}^{2}\right] &amp;=\left(\frac{N-1}{N}\right) \sigma^{2} \end{aligned}\)<br></div><div>The variance reduces as \(N\) gets bigger.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When using a <b>gaussian </b>to fit a curve:<div>\(p(t | x, \mathbf{w}, \beta)=\mathcal{N}\left(t | y(x, \mathbf{w}), \beta^{-1}\right)\)</div><div><br></div><div>What is \(\beta\) name</div>"	"The <b>precision </b>parameter  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When using a&nbsp;<b>gaussian&nbsp;</b>to fit a curve:<div>\(p(t | x, \mathbf{w}, \beta)=\mathcal{N}\left(t | y(x, \mathbf{w}), \beta^{-1}\right)\)</div><div><br></div><div>What is \(\beta\) value</div>"	"The inverse of the sample <b>variance</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  If we fit a curve with a <b>gaussian</b>:<div><div>\(p(t | x, \mathbf{w}, \beta)=\mathcal{N}\left(t | y(x, \mathbf{w}), \beta^{-1}\right)\)</div></div><div><br></div><div>What is the <b>likelihood </b>function over the sampled distribution</div>"	"\(p(\mathbf{t} | \mathbf{x}, \mathbf{w}, \beta)=\prod_{n=1}^{N} \mathcal{N}\left(t_{n} | y\left(x_{n}, \mathbf{w}\right), \beta^{-1}\right)\)<div><br></div><div><i>Assuming that data is sampled independently.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  K cross-validation"	"Split the dataset in train/val with <b>K folds</b>:<div><img src=""paste-38a779abf8aa3ba9716bc37f98531166cab9770e.jpg""><br></div> <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  Split the dataset in train/val with <b>K folds</b>:<div><img src=""paste-38a779abf8aa3ba9716bc37f98531166cab9770e.jpg""><br></div>"	"K cross-validation <br/> <div style=""font-style: italic""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Curse of Dimensionality"	"Harder to work with <b>high-dimensional space</b>&nbsp;as the manifold grows exponentially bigger than the dimension.<div><br></div><div><img src=""paste-1739df2c77278256881984f9b6d23399774dae50.jpg""><br></div><div><div> <div> <div> <div> <div><i>Plot of the fraction of the volume of a sphere lying in the range \(r = 1−\epsilon\) to \(r = 1\) for various values of the dimensionality \(D\).&nbsp;</i></div> </div> </div> </div></div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Two reasons why the <b>Curse of Dimensionality </b>is not impossible to beat:<div><br></div><div>1. <span class=cloze>[]</span></div><div><br></div><div>2. The manifold is assumed <b>smooth</b>: small changes in the input space results in small changes in the target space.</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Two reasons why the <b>Curse of Dimensionality </b>is not impossible to beat:<div><br></div><div>1. <span class=cloze>Real data is <b>confined to a lower-dimension space</b> of the total manifold.</span></div><div><br></div><div>2. The manifold is assumed <b>smooth</b>: small changes in the input space results in small changes in the target space.</div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the assumed <b>smoothness </b>of the manifold of real data"	"<b>Small changes</b> in <b>input</b> space results in small changes in <b>target</b> space.  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  <b>Entropy </b>formula of a proba \(p(x)\):&nbsp;"	"\(h(x) = -\log_2 p(x)\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  \(h(x) = -\log_2 p(x)\)"	"<b>Entropy </b>formula of a proba \(p(x)\):&nbsp; <br/> <div style=""font-style: italic""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Intuitively, explain:<div><br></div><div>\(\mathrm{H}[x]=-\sum_{x} p(x) \log _{2} p(x)\)</div>"	"It's the <b>average amount of information </b>(\(-\log _{2} p(x)\)) as weighted by the probability distribution \(p(x)\).  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Biggest <b>entropy </b>is with uniform or non-uniform distribution"	"Uniform  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The <span class=cloze>[]</span> is a lower bound of the number of bits needed to transmit the state of a random variable. "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The <span class=cloze><b>entropy</b></span> is a lower bound of the number of bits needed to transmit the state of a random variable.<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The <b>entropy</b> is a <span class=cloze>[]</span> bound of the number of bits needed to transmit the state of a random variable. "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The <b>entropy</b> is a <span class=cloze>lower</span> bound of the number of bits needed to transmit the state of a random variable.<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The <b>entropy</b> is a lower bound of the <span class=cloze>[]</span> needed to transmit the state of a random variable. "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The <b>entropy</b> is a lower bound of the <span class=cloze>number of bits</span> needed to transmit the state of a random variable.<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  <b>Kullback-Leibler </b>formula"	"\(\mathrm{KL}(p \| q) = -\sum p(\mathbf{x})\log\frac{q(\mathbf{x})}{p(\mathbf{x})}\) <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  \(\mathrm{KL}(p \| q) = -\sum p(\mathbf{x})\log\frac{q(\mathbf{x})}{p(\mathbf{x})}\)"	"<b>Kullback-Leibler </b>formula <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\mathrm{KL}(p \| q)\) <span class=cloze>[]</span> \(\mathrm{KL}(q \| p)\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\mathrm{KL}(p \| q)\) <span class=cloze>\(\not \equiv\)</span> \(\mathrm{KL}(q \| p)\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Intuitive explanation of <b>Kullback-Leibler</b>:<div>\(\mathrm{KL}(p \| q) = -\sum p(\mathbf{x})\log\frac{q(\mathbf{x})}{p(\mathbf{x})}\)<b><br></b></div>"	"<div> <div> <div> <div>With some <b>unknown distribution p(x)</b>, that we model with an <b>approximating distribution q(x)</b>.&nbsp;</div><div><br></div><div>If we use q(x) to construct a coding scheme for the purpose of transmitting values of x to a receiver, it is the <b>average additional amount of information</b>.</div> </div> </div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Intuitively explanation of a <b>convex function</b>"	"We can draw a cord (<i>blue</i>) between any point of \(f(x)\) without crossing the function (<i>red</i>).<div> <div> <div> <div><img src=""paste-cd2a3efc6402bb5197ba4f75b17eaee871efacc2.jpg""><br></div> </div> </div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  Condition for a fonction to be <b>convex</b>"	"\(f(\lambda a+(1-\lambda) b) \leqslant \lambda f(a)+(1-\lambda) f(b)\)<div><br></div><div>Condition to what?</div> <br/> <div style=""font-style: italic""><img src=""paste-cd2a3efc6402bb5197ba4f75b17eaee871efacc2.jpg""><br><div><i>Think about triangle inequality.</i></div></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  \(f(\lambda a+(1-\lambda) b) \leqslant \lambda f(a)+(1-\lambda) f(b)\)<div><br></div><div>Condition to what?</div>"	"Condition for a fonction to be <b>convex</b> <br/> <div style=""font-style: italic""><img src=""paste-cd2a3efc6402bb5197ba4f75b17eaee871efacc2.jpg""><br><div><i>Think about triangle inequality.</i></div></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>Concave </b>function \(f(x)\)"	"\(f(x)\) is concave if \(-f(x)\) <b>is convex.</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>Mutual information </b>formula"	"\(\mathrm{I}[\mathrm{x}, \mathrm{y}]=\mathrm{H}[\mathrm{x}]-\mathrm{H}[\mathrm{x} | \mathrm{y}]=\mathrm{H}[\mathrm{y}]-\mathrm{H}[\mathrm{y} | \mathrm{x}]\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  If the <b>mutual information </b>\(\mathrm{I}[\mathrm{x}, \mathrm{y}] = 0\):<div><b><br></b></div><div>\(\mathrm{I}[\mathrm{x}, \mathrm{y}]=\mathrm{H}[\mathrm{x}]-\mathrm{H}[\mathrm{x} | \mathrm{y}]=\mathrm{H}[\mathrm{y}]-\mathrm{H}[\mathrm{y} | \mathrm{x}]\)<b><br></b></div>"	"It means that \(\mathrm{x}\) and \(\mathrm{y}\) are <b>independent</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Two reasons why the <b>Curse of Dimensionality </b>is not impossible to beat:<div><br></div><div>1. Real data is <b>confined to a lower-dimension space</b> of the total manifold.</div><div><br></div><div>2. <span class=cloze>[]</span></div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Two reasons why the <b>Curse of Dimensionality </b>is not impossible to beat:<div><br></div><div>1. Real data is <b>confined to a lower-dimension space</b> of the total manifold.</div><div><br></div><div>2. <span class=cloze>The manifold is assumed <b>smooth</b>: small changes in the input space results in small changes in the target space.</span></div><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>MC Dropout </b>acronym"	"<b>Monte Carlo </b>dropout  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>MC Dropout </b>algorithm in training"	"Simply train with <b>dropout</b>:<div><img src=""paste-a2f6bb2c6966a2207abbb7f6bfe8fc180270da9f.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>MC Dropout </b>in inference"	"Do <b>several forwards with different dropout sampling</b>.<div><br></div><div>The <b>mean </b>is the prediction, and the <b>variance </b>is<b>&nbsp;</b>the uncertainty.</div><div><img src=""paste-a2f6bb2c6966a2207abbb7f6bfe8fc180270da9f.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How does <b>MC Dropout </b>models the <b>uncertainty</b>"	"Do&nbsp;<b>several forwards with different dropout sampling</b>.<div><br></div><div>The&nbsp;mean<b>&nbsp;</b>is the prediction, and <b>the&nbsp;variance&nbsp;is&nbsp;the uncertainty</b>.</div><div><img src=""paste-a2f6bb2c6966a2207abbb7f6bfe8fc180270da9f.jpg""></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>MC Dropout </b>use case"	"To measure <b>uncertainty </b>using the prediction variance of several forwards.<div><img src=""paste-a2f6bb2c6966a2207abbb7f6bfe8fc180270da9f.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>Intuition of what \(p(\mathbf{w})\) represents</div>"	"What we think about \(\mathbf{w}\)&nbsp;<b>before we have seen any data</b>. It's a prior.<br><div><br></div><div><i>For example, some Deep Bayesian Network assumes that the prior&nbsp;\(\mathbf{w}\) follows a gaussian distribution.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>Intuition of what \(p(\mathcal{D})\) represents</div>"	"<b>What we see from the data distribution</b>, it's the evidence prior.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>Name of \(p(\mathcal{D})\)</div>"	"The <b>evidence</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>In theory, how to compute the <b>evidence</b>&nbsp;\(p(\mathcal{D})\)</div>"	"<div>\(p(\mathcal{D}) = \int p(\mathcal{D}, \mathbf{w'}) \mathrm{d}\mathbf{w'}\)<br></div><div><br></div><div><i>However integrating over all possible parameters in untractable.</i></div><div><i><br></i></div><div><i>It's the Sum Rule for continuous variables.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>Why computing the exact&nbsp;<b>evidence</b>&nbsp;\(p(\mathcal{D})\) is <b>untractable:</b><br></div><div><b><br></b></div><div>\(p(\mathcal{D}) = \int p(\mathcal{D}, \mathbf{w'}) \mathrm{d}\mathbf{w'}\)</div>"	"<div>Because there is an <b>infinity of parameters to integrate over</b>.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Principle of <b>Monte Carlo</b>"	"<b>Sampling a large number</b> of random numbers to <b>approximate an untractable distribution</b>.<div><img src=""paste-7a39d6000da85e02eb8c63d032b976f62214fce3.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>When \(p(\mathbf{w} | \mathcal{D})\) and \(p(\mathbf{w})\) are called <b>conjugate distributions</b></div>"	"When they follow the <b>same probability distribution family </b><i>(gauss, poisson, etc.)</i>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>Name of the pair \(p(\mathbf{w} | \mathcal{D})\) and \(p(\mathbf{w})\) when they follow the <b>same distribution family</b></div>"	"<b>Conjugate </b>distributions  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <b>MCMC </b>acronym"	"<b>M</b>arcov <b>C</b>hain&nbsp;<b>M</b>onte-<b>C</b>arlo  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  Goal of <b>MCMC</b>"	"<b>Approximate posterior distribution</b>&nbsp; \(p(\mathbf{w} | \mathcal{D})\) in:<div><br></div><div>\(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  Principle of <b>MCMC</b>"	"<b>Random walk </b>to find the distribution parameter<i> (\(\mu\) and \(\sigma^2\) for gaussian) </i>until an <b>equilibrium is reached</b>.<div><br></div><div><img src=""paste-33d49665f9d4cb0eaa0c63194a593ac1e43070ba.jpg""><br></div><div><i>Orange is the approximation of blue.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  How do we determine if a <b>random walk </b>is accepted in <b>MCMC</b><div><img src=""paste-33d49665f9d4cb0eaa0c63194a593ac1e43070ba.jpg""><b><br></b></div>"	"Either the <b>likelihood has increased</b>.<div><br></div><div>Or some random chances to accept it anyway.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <div>What is the <b>proposal width </b>in <b>MCMC </b><i>(also known as jump width)</i><br></div><div><img src=""paste-33d49665f9d4cb0eaa0c63194a593ac1e43070ba.jpg""><b><br></b></div>"	"How <b>far </b>the random walk can go.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  What is the <b>Markov Chain </b>in <b>MCMC</b><div><img src=""paste-33d49665f9d4cb0eaa0c63194a593ac1e43070ba.jpg""><b><br></b></div>"	"At each step we held a supposed value of our approximation (<i>like \(\mu\)</i>) which is a <b>state of the Marcov Chain</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <b>Trace </b>of the <b>MCMC </b>algorithm"	"<b>Plot of all states value</b>:<div><img src=""paste-7005a2d5a87c5aeedc5ac781caa3966ab58bf42f.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  How do we get the <b>belief </b>of the found parameter with <b>MCMC</b>"	"Build an <b>histogram </b>of the values reported in the <b>trace</b>:<div><br></div><div>The trace:</div><div><img src=""paste-7005a2d5a87c5aeedc5ac781caa3966ab58bf42f.jpg""><br></div><div>The histogram:</div><div><img src=""paste-c33fa1574bdb216ea7c1ae7f24883bf6c12ca706.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  Consequence of a <b>large proposal width</b>&nbsp;in <b>MCMC</b>"	"Each new candidate will be too far from the real distribution.<div><br></div><div>Thus the <b>likelihood will be low</b>&nbsp;and <b>most jumps will be refused</b>.</div><div><img src=""paste-2c0e6f7706090079f4295222f79ddcc4c75a2582.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  Consequence of a&nbsp;<b>small proposal width</b>&nbsp;in&nbsp;<b>MCMC</b>"	"<b>Very slow convergence </b>and may lead to a <b>random-walk behavior</b>.<div><img src=""paste-a19df1fef96133cadbfa3629731f939d3857f76b.jpg""><b><br></b></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  How to estimate the <b>quality of a sampler </b>in <b>MCMC</b>"	"With the <b>autocorrelation </b>of all samples.<div><br></div><div><i>i.e. how correlated a sample i is to sample i-1, i-2, etc.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  What is the <b>autocorrelation </b>of the samples of <b>MCMC</b>"	"How much a sample \(i\) is <b>correlated to the previous samples</b> \(i-1\), \(i-2\), etc.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  How is the <b>autocorrelation </b>when using <b>small proposal width </b>in <b>MCMC</b>"	"<b>Very high</b>, which is a bad thing.<div><img src=""paste-2b2b843c9c8d740d85cf90e5940f88a8fe7126e6.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  In <b>MCMC</b>, how do we want the <b>autocorrelation </b>to be"	"<b>Low </b>else the samples are dependent and the posterior estimation will be bad.<div><img src=""paste-2b2b843c9c8d740d85cf90e5940f88a8fe7126e6.jpg""><b><br></b></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <b>Metropolis </b>method&nbsp;to choose the <b>sample width </b>in <b>MCMC</b>"	"Keep adjusting the proposal width so that roughly <b>50% proposals are rejected</b>.<br><div><img src=""paste-2b2b843c9c8d740d85cf90e5940f88a8fe7126e6.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  Disadvantage of the <b>MCMC </b>algorithm<div><img src=""paste-33d49665f9d4cb0eaa0c63194a593ac1e43070ba.jpg""><br></div>"	"Very <b>slow to converge</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <b>Burn-in </b>period in <b>MCMC</b>&nbsp;algo<div><img src=""paste-33d49665f9d4cb0eaa0c63194a593ac1e43070ba.jpg""><br></div>"	"Beginning where <b>initial samples are of low quality </b>because often too correlated:<div><img src=""paste-2b2b843c9c8d740d85cf90e5940f88a8fe7126e6.jpg""><br></div><div><i>Whatever the proposal width, at the beginning, there is a high autocorrelation.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  How to handle the <b>burn-in </b>period of <b>MCMC</b><div><b><br></b></div><div><i>(Bad early estimation)<br></i><div><img src=""paste-2b2b843c9c8d740d85cf90e5940f88a8fe7126e6.jpg""><b><br></b></div></div>"	"We <b>discard the early samples</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In distributed learning, what is the <b>parameter server</b>"	"Server holding the <b>authoritative souce of the parameters</b>.<div><br></div><div>In doubt, we trust this server</div><div><img src=""paste-37346557cd1769008e40c62dab91e7aa9aa5694b.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>bottleneck </b>in this distributed system:<div><img src=""paste-37346557cd1769008e40c62dab91e7aa9aa5694b.jpg""><br></div>"	"The <b>network communication </b>between the parameter server and the workers.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to reduce <b>network communication</b>&nbsp;in this distributed system:<div><img src=""paste-37346557cd1769008e40c62dab91e7aa9aa5694b.jpg""><br></div>"	"By doing <b>several updates on the worker server</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the problem of doing <b>several updates on the worker servers</b>&nbsp;here:<div><!--anki--><img src=""paste-37346557cd1769008e40c62dab91e7aa9aa5694b.jpg""><br></div>"	"The worker servers can become <b>desynchronised </b>from the parameter server.<div><br></div><div>They will provide <b>stale updates </b>that will only add noise.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  In a bayesian setting, what would be the jobs the <b>workers</b>:<div><img src=""paste-37346557cd1769008e40c62dab91e7aa9aa5694b.jpg""><br></div>"	"Estimate the <b>likelihood</b>.<div><img src=""paste-a795974c144ce6f278a5553a0406bde27eea4e89.jpg""><br></div><div><i>The parameter server will combine them into a posterior.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  In a bayesian setting, what would be the job of the <b>parameter server</b>:<div><img src=""paste-37346557cd1769008e40c62dab91e7aa9aa5694b.jpg""><br></div>"	"Produce the <b>posterior </b>distribution.<div><img src=""paste-a795974c144ce6f278a5553a0406bde27eea4e89.jpg""><br></div><div><i>To do so, it'll use the estimated likelihood of the workers.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  <b>Aleatoric </b>uncertainty"	"<b>Natural noise</b> from nature or sensors. <br/> <div style=""font-style: italic""><img src=""paste-c6eef9ddde971b975dab317afeb91e1cd24ac477.jpg""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  <b>Natural noise</b> from nature or sensors."	"<b>Aleatoric </b>uncertainty <br/> <div style=""font-style: italic""><img src=""paste-c6eef9ddde971b975dab317afeb91e1cd24ac477.jpg""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  <b>Epistemic </b>uncertainty"	"Noise linked to the <b>model bias</b>. <br/> <div style=""font-style: italic""><img src=""paste-c6eef9ddde971b975dab317afeb91e1cd24ac477.jpg""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  Noise linked to the <b>model bias</b>."	"<b>Epistemic </b>uncertainty <br/> <div style=""font-style: italic""><img src=""paste-c6eef9ddde971b975dab317afeb91e1cd24ac477.jpg""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  <div> <div> <div> <div> <div><b>Predictive</b> uncertainty&nbsp;</div> </div> </div> </div></div>"	"<b>Epistemic </b>+ <b>Aleatoric </b>uncertainties <br/> <div style=""font-style: italic""><img src=""paste-c6eef9ddde971b975dab317afeb91e1cd24ac477.jpg""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  <b>Epistemic </b>+ <b>Aleatoric </b>uncertainties"	"<div> <div> <div> <div> <div><b>Predictive</b> uncertainty&nbsp;</div> </div> </div> </div></div> <br/> <div style=""font-style: italic""><img src=""paste-c6eef9ddde971b975dab317afeb91e1cd24ac477.jpg""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How does <b>Dropout </b>behaves in inference<div><img src=""paste-a2f6bb2c6966a2207abbb7f6bfe8fc180270da9f.jpg""><br></div>"	"All neurons are used, but the <b>weights are divided \(p\)</b>.<div><br></div><div><i>The rescaling happens so to keep the activations at the same level as during training.</i></div><div><i><br></i></div><div><i>It is called the ""weight scaling rule"".</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Name of the operation done so that <b>dropout </b>works in inference<div><img src=""paste-a2f6bb2c6966a2207abbb7f6bfe8fc180270da9f.jpg""><br></div>"	"The <b>Weight Scaling Rule</b>, where all weights are divided by \(p\).  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which probability distribution does <b>dropout </b>follow<div><img src=""paste-a2f6bb2c6966a2207abbb7f6bfe8fc180270da9f.jpg""><br></div>"	"A <b>Bernoulli </b>distribution.<div><br></div><div><i>Neurons can be either in active or inactive state randomly.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>What does <b>Maximum Likelihood </b>optimizes here</div>"	"It maximizes the <b>likelihood</b> \(p(\mathcal{D} | \mathbf{w})\).  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>What does&nbsp;<b>Maximum A Posteriori&nbsp;</b>optimizes here</div>"	"It maximizes the <b>posterior </b>\(p(\mathbf{w} | \mathcal{D})\).  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <b>MAP </b>acronym"	"<b>M</b>aximum <b>A P</b>osteriori<div><br></div><div><i>Tries to maximizes the posterior&nbsp;\(p(\mathcal{D} | \mathbf{w})\) while having access to the prior \(p(\mathbf{w})\).</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Closed form"	"A solution that is <b>finite</b>.<div>i.e. no integration, limit, or differentiation.<br></div><div><br></div><div><i>It a problem has no closed form, it must be approximated.<br></i><div><br></div><div><i>For example the posterior is a closed form at it requires integrate over all possible parameters, which is unfeasible.</i></div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  <div> <div> <div> <div> <div><b>Homoscedastic</b> uncertainty&nbsp;</div> </div> </div> </div></div>"	"Uncertainty that <b>does not depend</b> of the input values. <br/> <div style=""font-style: italic""> <div> <div> <div> <div><img src=""paste-a9abf52a76421d70d4f4ef366389d047d89d6f05.jpg""><br></div> </div> </div> </div></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  Uncertainty that <b>does not depend</b> of the input values."	"<div> <div> <div> <div> <div><b>Homoscedastic</b> uncertainty&nbsp;</div> </div> </div> </div></div> <br/> <div style=""font-style: italic""> <div> <div> <div> <div><img src=""paste-a9abf52a76421d70d4f4ef366389d047d89d6f05.jpg""><br></div> </div> </div> </div></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  <b>Heteroscedastic</b>&nbsp;uncertainty&nbsp;"	"Uncertainty that&nbsp;<b>depends</b>&nbsp;of the input values. <br/> <div style=""font-style: italic""> <div> <div> <div> <div><img src=""paste-a9abf52a76421d70d4f4ef366389d047d89d6f05.jpg""><br></div> </div> </div> </div></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline"">Bayesian</div>  Uncertainty that&nbsp;<b>depends</b>&nbsp;of the input values."	"<b>Heteroscedastic</b>&nbsp;uncertainty&nbsp; <br/> <div style=""font-style: italic""> <div> <div> <div> <div><img src=""paste-a9abf52a76421d70d4f4ef366389d047d89d6f05.jpg""><br></div> </div> </div> </div></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <div>\(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br></div><div><br></div>Evolution of <b>posterior </b>\(p(\mathbf{w} | \mathcal{D})\) with MAP with more data points"	"The <b>posterior space gets restricted</b>&nbsp;as less values of \(\mathbf{w}\) becomes suitable:<div><br></div><div><i>It means than less values of the parameter are certain as the task become more complex.</i><br><div><img src=""paste-2075b055b4db47ea55ddc01841474097842b6b3c.jpg""><br></div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  Relation between <b>MAP </b>and <b>weight decay</b>"	"Maximizing the posterior is equivalent to <b>minimizing the squared error with a weight decay</b>:<div><img src=""paste-4d772720f2b01066669a99ddd9d94f92611b2a63.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>[]</span> = \(\log(x) + \log(y)\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>\(\log(xy)\)</span> = \(\log(x) + \log(y)\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\log(xy)\) = <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\log(xy)\) = <span class=cloze>\(\log(x) + \log(y)\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>[]</span> = \(y\log(x)\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>\(\log(x^y)\)</span> = \(y\log(x)\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\log(x^y)\) = <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\log(x^y)\) = <span class=cloze>\(y\log(x)\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>[]</span> = \(\log(x) - \log(y)\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>\(\log(\frac{x}{y})\)</span> = \(\log(x) - \log(y)\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\log(\frac{x}{y})\) = <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\log(\frac{x}{y})\) = <span class=cloze>\(\log(x) - \log(y)\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <b>Variation-ratio </b>goal"	"A <b>measure of uncertainty</b><div><b><br></b></div><div>\(\begin{array}{l}{c^{*}: \text { most frequent label over the } T \text { passes, with frequency } f_{x}^{c^{*}}} \\ {\text { Compute variation-ratio var-ratio }[x]=1-\frac{f_{x}^{c^{*}}}{T}}\end{array}\)<b><br></b></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Bayesian</div>  <b>Variation-ratio </b>formula"	"<div>\(\begin{array}{l}{c^{*}: \text { most frequent label over the } T \text { passes, with frequency } f_{x}^{c^{*}}} \\ {\text { Compute variation-ratio var-ratio }[x]=1-\frac{f_{x}^{c^{*}}}{T}}\end{array}\)<br></div><div><br></div><div><i>It measures the uncertainty.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <b>Bernoulli </b>distribution"	"<b>Discrete </b>probability distribution with random values <b>0 and 1</b>.<div><img src=""paste-cd5dc372df153c076a0feacf1c185b75082d35c6.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Density estimation"	"<div> <div> <div> <div> <div>Given a <b>finite set</b> \(x_1, . . . , x_N\) of observations, <b>find distribution</b> \(p(x)\) of \(x\).&nbsp;</div> </div> </div> </div></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  With <b>bernoulli</b>, what does represent \(\mu\) here:<div><br></div><div>\(\operatorname{Bern}(x | \mu)=\mu^{x}(1-\mu)^{1-x}\)</div>"	"The <b>prior </b>on the random event.<div><br></div><div><i>With a coin flip, \(\mu = 0.5\).</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  What is the <b>maximum likelihood </b>of the <b>bernoulli </b>distribution:<div>\(\operatorname{Bern}(x | \mu)=\mu^{x}(1-\mu)^{1-x}\)</div>"	"\(\mu^{\mathrm{ML}}=\frac{m}{N} \quad \text { with } \quad m=(\# \text { observations of } x=1)\)<div><br></div><div><i>Simply the number of times we find 1 divided by the total tries.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <b>Maximum likelihood </b>of the Bernoulli distribution<div>\(\operatorname{Bern}(x | \mu)=\mu^{x}(1-\mu)^{1-x}\)<br></div>"	"\(\mu^{\mathrm{ML}}=\frac{m}{N} \quad \text { with } \quad m=(\# \text { observations of } x=1)\)<div><br></div><div><i>Simply the average amount of times the random event was 1.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <div><div>\(\operatorname{Bern}(x | \mu)=\mu^{x}(1-\mu)^{1-x}\)</div></div><div><br></div>What is the problem with the&nbsp;<b>Maximum likelihood&nbsp;</b>of the Bernoulli distribution<div>\(\mu^{\mathrm{ML}}=\frac{m}{N}\)<br></div>"	"It severely <b>overfits </b>for <b>small datasets</b>.<div><br></div><div><i>If a coin falls twice on 1, we'll asume all next tries will be also be 1.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>When does the posterior \(p(\mathbf{w} | \mathcal{D})\) and the prior \(p(\mathbf{w})\) are <b>conjugate distributions</b></div>"	"When they follow the <b>same probability distribution family </b>(<i>i.e. gaussian, beta, etc.</i>)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>What is the name of the pair posterior \(p(\mathbf{w} | \mathcal{D})\) and prior \(p(\mathbf{w})\) when they follow&nbsp;<b>same probability distribution family</b></div>"	"<b>Conjugate distributions</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  \(p(\mathbf{w} | \mathcal{D})=\frac{p(\mathcal{D} | \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}\)<br><div><br></div><div>When the posterior \(p(\mathbf{w} | \mathcal{D})\) and the prior \(p(\mathbf{w})\) are&nbsp;<b>conjugate distributions.</b></div><div><b><br></b></div><div>What is the name of the&nbsp;<b>prior to the likelihood</b>&nbsp;\(p(\mathcal{D} | \mathbf{w})\)?</div>"	"<b>Conjugate prior </b>to the likelihood function  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  When does <b>bayesian </b>and <b>maximum likelihood </b>agree?"	"When the dataset grows to \(+\infty\).<div><br></div><div><i>Remember that the underestimation of the variance of ML is 0 when the dataset is infinite.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Intuively, where is the <b>posterior </b>compared to the <b>likelihood </b>and <b>prior</b>?"	"<div>As an <b>intermediary distribution </b>between them.</div><img src=""paste-5813b2bb7d7ff7231af2676024b9bfd9f2da3e56.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  For a <b>binomial likelihood </b>function, what kind of <b>conjugate prior </b>can we chose?"	"A <b>beta </b>distribution.<div><img src=""paste-5813b2bb7d7ff7231af2676024b9bfd9f2da3e56.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  For a <b>multinomial likelihood&nbsp;</b>function, what kind of&nbsp;<b>conjugate prior&nbsp;</b>can we chose?"	"A <b>Dirichlet </b>distribution<div><img src=""paste-4b9a845bd4923f0d7a52b0edae6087452fcddab4.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  What is the <b>multinomial </b>distribution?"	"A vector of \(K\) random values.<div>\(\mathbf{x}=(0,0,1,0,0,0)^{\mathrm{T}}\)<br></div><div><br></div><div>With constraints that \(\sum_k x_k = 1\).</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  To which distribution can we see the <b>multinomial</b>&nbsp;its generalization<div>\(\mathbf{x}=(0,0,1,0,0,0)^{\mathrm{T}}\)</div>"	"A generalization of <b>Bernoulli </b>to \(K\) values.  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Which <b>distribution</b>&nbsp;is it?<div>\(\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\mathrm{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}\)</div>"	"<div><b>multivariate-Gaussian </b>distribution.</div><div><br></div>\(\mathcal{N}(\mathrm{x} | \boldsymbol{\mu}, \mathbf{\Sigma})=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\mathrm{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Name of the following <b>distance</b>:<div>\((\mathbf{x}-\mathbf{y})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{y})\)</div>"	"<b>Mahalanobis </b>distancd  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Intuition of \(\Sigma\) role in the <b>Mahalanobis</b> distance:<div><br><div>\((\mathbf{x}-\mathbf{y})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{y})\)</div></div>"	"<b>Dimension with high variability will matter less</b> than dimension with low variability.<div><br></div><div>This makes sense as we are less certain of those high-variability dimensions.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  What is \(\Sigma\) in the <b>Mahalanobis</b> distance:<div><br><div>\((\mathbf{x}-\mathbf{y})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{y})\)</div></div>"	"The <b>covariance </b>matrix  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  How the <b>Mahalanobis</b> distance can reduce to a <b>Euclidean </b>distance?<div><br><div>\((\mathbf{x}-\mathbf{y})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{y})\)</div></div>"	"When \(\Sigma = I\), the <b>identity matrix</b>.  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> <b>Class selector </b>symbol"	"The dot ""<b>.</b>""<div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">.<span style=""color: #0000FF; font-weight: bold"">intro</span> {     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #008000; font-weight: bold"">red</span>;     <span style=""color: #008000; font-weight: bold"">font-weight</span>: <span style=""color: #008000; font-weight: bold"">bold</span>; }</pre></div></td></tr></tbody></table></center><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> <b>id selector&nbsp;</b>symbol"	"The hashtag ""<b>#</b>""<div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">#<span style=""color: #0000FF; font-weight: bold"">top</span> {     <span style=""color: #008000; font-weight: bold"">background-color</span>: <span style=""color: #666666"">#ccc</span>;     <span style=""color: #008000; font-weight: bold"">padding</span>: <span style=""color: #666666"">20</span><span style=""color: #B00040"">px</span> }</pre></div></td></tr></tbody></table></center><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> Apply same rules to <b>several selectors</b>"	"<center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">h2</span><span style=""color: #666666"">,</span> .<span style=""color: #0000FF; font-weight: bold"">thisOtherClass</span><span style=""color: #666666"">,</span> .<span style=""color: #0000FF; font-weight: bold"">yetAnotherClass</span> {     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #008000; font-weight: bold"">red</span>; }</pre></div></td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> Nest several <b>selectors</b>"	"<center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%"">.<span style=""color: #0000FF; font-weight: bold"">top</span> <span style=""color: #008000; font-weight: bold"">h1</span> {     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #666666"">#ff0</span>; } </pre></div> </td></tr></tbody></table></center><br><div><i>Only title (h1) in class ""top"" will be affected.</i></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> Pseudo-class"	"<b>Temporary attributes</b> of a class<div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">a</span>:<span style=""color: #AA22FF"">active</span> {     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #008000; font-weight: bold"">red</span>; }  <span style=""color: #008000; font-weight: bold"">a</span>:<span style=""color: #AA22FF"">hover</span> {     <span style=""color: #008000; font-weight: bold"">text-decoration</span>: <span style=""color: #008000; font-weight: bold"">none</span>;     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #008000; font-weight: bold"">blue</span>;     <span style=""color: #008000; font-weight: bold"">background-color</span>: <span style=""color: #008000; font-weight: bold"">yellow</span>; }</pre></div></td></tr></tbody></table></center><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> Pseudo-class for <b>unvisited links</b>"	"<center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">a</span>:<span style=""color: #AA22FF"">link</span> {     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #008000; font-weight: bold"">blue</span>; } </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> Pseudo-class for <b>visited links</b>"	"<center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">a</span>:<span style=""color: #AA22FF"">visited</span> {     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #008000; font-weight: bold"">purple</span>; } </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> Pseudo-class when a <b>link is clicked on</b>"	"<center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">a</span>:<span style=""color: #AA22FF"">active</span> {     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #008000; font-weight: bold"">red</span>; } </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">CSS</div> Pseudo-class when a <b>the mouse is over a link</b>"	"<center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">a</span>:<span style=""color: #AA22FF"">hover</span> {     <span style=""color: #008000; font-weight: bold"">color</span>: <span style=""color: #008000; font-weight: bold"">blue</span>; } </pre></div> </td></tr></tbody></table></center>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> LRU acronym"	"<b>L</b>east <b>R</b>ecently <b>U</b>sed  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> LRU algorithm"	"<b>Discard the oldest unused item</b> to make place for an incoming new item.  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Lifelong-Learning</div>  Meta-transfer"	"Learns a small <b>scaling &amp; shifting</b> applied to the conv parameters.<div><img src=""paste-78dd9efcdae8a7b24bb68575252a764bc6cb3b14.jpg""><br></div><div>This allows a <b>model adaptation</b> without modifying its main weights.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Lifelong-Learning</div>  What is <b>mnemonic exemplars</b>?"	"<b>Optimizes the exemplars</b>, as actual parameters<div><img src=""paste-735abe6bbdca8887440077085784c9b3057a8f9a.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Lifelong-Learning</div>  How are&nbsp;<b>mnemonic exemplars </b>situed in the class distribution?"	"On the <b>class boundaries</b>:<div><img src=""paste-177c9d1f818ef200a61d2045908abab7e9e0abbf.jpg""><br></div><div>It facilites the classes discrimination.</div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised</div>  <b>Denoising Auto-encoder</b> are robust to what?"	"Robustness of <b>re-construction</b>.<br><div><img src=""paste-6f2c04fdbdc0a286f3ad2fb91ded65d5198a8dec.jpg""><br></div><div><i>It can correctly re-construct inputs even with noise.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised</div>  <b>Contractive Auto-encoder</b>&nbsp;are robust to what?"	"Robustness of <b>representation</b> \(f(x)\).<br><div><img src=""paste-8c8715bc427d40db7b4890abf19005f6f0f72d54.jpg""><br></div><div><i>Therefore they are a better features extractor than denoising autoencoder.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Self-supervised</div>  Penalty in <b>contractive auto-encoder</b>"	"<div>The frobenius norm of the <b>jacobian of the inputs</b>.</div><img src=""paste-db54eb7b62cdad1dd319e0938c66bedde48cb1ef.jpg""><div><i>This jacobian represents the model's sensitivity to the inputs: a small change in x shouldn't affect the representation \(f(x)\).</i></div><div><i>Note that the decoder \(g(f(x))\) isn't directly affected.</i></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  Contraction mapping"	"Function with property:<div>\(d(f(x), f(y)) \leq k d(x, y)\)</div><div><br></div><div>With \(0 \le k \le 1\), the&nbsp;Lipschitz constant of \(f\).</div><div><br></div><div>It means that the manifold generated by \(f\) is <b>smooth</b>.</div> <br/> <div style=""font-style: italic""></div>"
"<h5>🤖 Deep Learning</h5>  <div style=""text-decoration: underline""></div>  Function with property:<div>\(d(f(x), f(y)) \leq k d(x, y)\)</div><div><br></div><div>With \(0 \le k \le 1\), the&nbsp;Lipschitz constant of \(f\).</div><div><br></div><div>It means that the manifold generated by \(f\) is <b>smooth</b>.</div>"	"Contraction mapping <br/> <div style=""font-style: italic""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Best Time to Buy and Sell Stocks</div> <div>Say you have an array for which the <em>i</em><sup>th</sup> element is the price of a given stock on day <em>i</em>.</div><div><br></div> <div>Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).</div><div><br></div> <div><strong>Note:</strong> You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again).</div>  <br/><br/> <div class=""example""><strong>Input:</strong> [7,1,5,3,6,4]<div><strong>Output:</strong> 7</div><div><strong>Explanation:</strong> Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4. &nbsp; Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3.<br></div></div>"	"Buy in valley, sell in peak.<div><img src=""paste-5a84ef45a8d8de1e591f2677d5e4e8328a94a891.jpg""><br></div><div>\(\text { Total Profit }=\sum_{i}\left(\text { height }\left(\text { peak }_{i}\right)-\text { height }\left(\text {valley}_{i}\right)\right)\)</div> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxProfit</span>(<span style=""color: #008000"">self</span>, prices: List[<span style=""color: #008000"">int</span>]):         holding_stock <span style=""font-family: Arial; color: rgb(102, 102, 102);"">=</span><span style=""font-family: Arial;""> </span><span style=""font-family: Arial; color: rgb(0, 128, 0); font-weight: bold;"">False</span><pre style=""line-height: 25px;"">                 profit <span style=""color: rgb(102, 102, 102);"">=</span> <span style=""color: rgb(102, 102, 102);"">0</span>         <span style=""color: rgb(0, 128, 0); font-weight: bold;"">for</span> i <span style=""color: rgb(170, 34, 255); font-weight: bold;"">in</span> <span style=""color: rgb(0, 128, 0);"">range</span>(<span style=""color: rgb(0, 128, 0);"">len</span>(prices)): </pre><span style=""font-family: Arial;"">            </span><span style=""font-family: Arial; color: rgb(0, 128, 0); font-weight: bold;"">if</span><span style=""font-family: Arial;""> i </span><span style=""font-family: Arial; color: rgb(102, 102, 102);"">==</span><span style=""font-family: Arial;""> </span><span style=""font-family: Arial; color: rgb(0, 128, 0);"">len</span><span style=""font-family: Arial;"">(pr</span>ices) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>:                 <span style=""color: #008000; font-weight: bold"">if</span> holding_stock:                     profit <span style=""color: #666666"">+=</span> prices[i]             <span style=""color: #008000; font-weight: bold"">elif</span> prices[i] <span style=""color: #666666"">&gt;</span> prices[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]:                 <span style=""color: #008000; font-weight: bold"">if</span> holding_stock:                     profit <span style=""color: #666666"">+=</span> prices[i]                     holding_stock <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">False</span>             <span style=""color: #008000; font-weight: bold"">elif</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> holding_stock:                 profit <span style=""color: #666666"">-=</span> prices[i]                 holding_stock <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 <span style=""color: #008000; font-weight: bold"">pass</span>                          <span style=""color: #008000; font-weight: bold"">return</span> profit </pre></div> </td></tr></tbody></table></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Complexity</div> Big O"	"<b>Upper bound </b>on time or memory<div><br></div><div>Worst case</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Complexity</div> Big \(\Omega\)"	"<b>Lower bound&nbsp;</b>on time or memory  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Complexity</div> Big \(\theta\)"	"When <b>upper </b>and <b>lower bounds </b>are equivalent to a constant  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Two Sum</div> Solve <b>TwoSum </b>problem:<div><br></div><div>Find indexes of two different numbers adding up to a third number.</div>  <br/><br/> <div class=""example"">Given nums = [2, 7, 11, 15], target = 9,<div>Because nums[<strong>0</strong>] + nums[<strong>1</strong>] = 2 + 7 = 9,</div><div>return [<strong>0</strong>, <strong>1</strong>].<br></div></div>"	"Brute force would have a time complexity of O(\(n^2\)). <br> <div class=""algo""><center></center><div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">twoSum</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>], target: <span style=""color: #008000"">int</span>):         hashmap <span style=""color: #666666"">=</span> {}         <span style=""color: #008000; font-weight: bold"">for</span> i, n <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">enumerate</span>(nums):             complement <span style=""color: #666666"">=</span> target <span style=""color: #666666"">-</span> n             j <span style=""color: #666666"">=</span> hashmap<span style=""color: #666666"">.</span>get(complement)             <span style=""color: #008000; font-weight: bold"">if</span> j <span style=""color: #AA22FF; font-weight: bold"">is</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> <span style=""color: #008000; font-weight: bold"">None</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> i <span style=""color: #666666"">!=</span> j:                 <span style=""color: #008000; font-weight: bold"">return</span> [i, j]             hashmap[n] <span style=""color: #666666"">=</span> i </pre></div> </td></tr></tbody></table></center></div><div><br></div></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Contains Duplicates</div> <b>Check if an array contains duplicates.</b><div><br></div><div>Search for the fastest algorithm.</div>  <br/><br/> <div class=""example""><strong>Input:</strong>&nbsp;[1,2,3,1]&nbsp;<strong>Output:</strong>&nbsp;true<br><div><strong>Input:&nbsp;</strong>[1,2,3,4]&nbsp;<strong>Output:</strong>&nbsp;false</div></div>"	"The Python set is a hashmap with a lookup complexity of O(1). <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">containsDuplicate</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         s <span style=""color: #666666"">=</span> <span style=""color: #008000"">set</span>()         <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> nums:             <span style=""color: #008000; font-weight: bold"">if</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> s:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>             s<span style=""color: #666666"">.</span>add(n)         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Best Time to Buy and Sell Stocks</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxProfit</span>(<span style=""color: #008000"">self</span>, prices: List[<span style=""color: #008000"">int</span>]):         holding_stock <span style=""font-family: Arial; color: rgb(102, 102, 102);"">=</span><span style=""font-family: Arial;""> </span><span style=""font-family: Arial; color: rgb(0, 128, 0); font-weight: bold;"">False</span><pre style=""line-height: 25px;"">                 profit <span style=""color: rgb(102, 102, 102);"">=</span> <span style=""color: rgb(102, 102, 102);"">0</span>         <span style=""color: rgb(0, 128, 0); font-weight: bold;"">for</span> i <span style=""color: rgb(170, 34, 255); font-weight: bold;"">in</span> <span style=""color: rgb(0, 128, 0);"">range</span>(<span style=""color: rgb(0, 128, 0);"">len</span>(prices)): </pre><span style=""font-family: Arial;"">            </span><span style=""font-family: Arial; color: rgb(0, 128, 0); font-weight: bold;"">if</span><span style=""font-family: Arial;""> i </span><span style=""font-family: Arial; color: rgb(102, 102, 102);"">==</span><span style=""font-family: Arial;""> </span><span style=""font-family: Arial; color: rgb(0, 128, 0);"">len</span><span style=""font-family: Arial;"">(pr</span>ices) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>:                 <span style=""color: #008000; font-weight: bold"">if</span> holding_stock:                     profit <span style=""color: #666666"">+=</span> prices[i]             <span style=""color: #008000; font-weight: bold"">elif</span> prices[i] <span style=""color: #666666"">&gt;</span> prices[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]:                 <span style=""color: #008000; font-weight: bold"">if</span> holding_stock:                     profit <span style=""color: #666666"">+=</span> prices[i]                     holding_stock <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">False</span>             <span style=""color: #008000; font-weight: bold"">elif</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> holding_stock:                 profit <span style=""color: #666666"">-=</span> prices[i]                 holding_stock <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 <span style=""color: #008000; font-weight: bold"">pass</span>                          <span style=""color: #008000; font-weight: bold"">return</span> profit </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Best Time to Buy and Sell Stocks</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxProfit</span>(<span style=""color: #008000"">self</span>, prices: List[<span style=""color: #008000"">int</span>]):         holding_stock <span style=""font-family: Arial; color: rgb(102, 102, 102);"">=</span><span style=""font-family: Arial;""> </span><span style=""font-family: Arial; color: rgb(0, 128, 0); font-weight: bold;"">False</span><pre style=""line-height: 25px;"">                 profit <span style=""color: rgb(102, 102, 102);"">=</span> <span style=""color: rgb(102, 102, 102);"">0</span>         <span style=""color: rgb(0, 128, 0); font-weight: bold;"">for</span> i <span style=""color: rgb(170, 34, 255); font-weight: bold;"">in</span> <span style=""color: rgb(0, 128, 0);"">range</span>(<span style=""color: rgb(0, 128, 0);"">len</span>(prices)): </pre><span style=""font-family: Arial;"">            </span><span style=""font-family: Arial; color: rgb(0, 128, 0); font-weight: bold;"">if</span><span style=""font-family: Arial;""> i </span><span style=""font-family: Arial; color: rgb(102, 102, 102);"">==</span><span style=""font-family: Arial;""> </span><span style=""font-family: Arial; color: rgb(0, 128, 0);"">len</span><span style=""font-family: Arial;"">(pr</span>ices) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>:                 <span style=""color: #008000; font-weight: bold"">if</span> holding_stock:                     profit <span style=""color: #666666"">+=</span> prices[i]             <span style=""color: #008000; font-weight: bold"">elif</span> prices[i] <span style=""color: #666666"">&gt;</span> prices[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]:                 <span style=""color: #008000; font-weight: bold"">if</span> holding_stock:                     profit <span style=""color: #666666"">+=</span> prices[i]                     holding_stock <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">False</span>             <span style=""color: #008000; font-weight: bold"">elif</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> holding_stock:                 profit <span style=""color: #666666"">-=</span> prices[i]                 holding_stock <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 <span style=""color: #008000; font-weight: bold"">pass</span>                          <span style=""color: #008000; font-weight: bold"">return</span> profit </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Two Sum</div>  <center></center><div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">twoSum</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>], target: <span style=""color: #008000"">int</span>):         hashmap <span style=""color: #666666"">=</span> {}         <span style=""color: #008000; font-weight: bold"">for</span> i, n <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">enumerate</span>(nums):             complement <span style=""color: #666666"">=</span> target <span style=""color: #666666"">-</span> n             j <span style=""color: #666666"">=</span> hashmap<span style=""color: #666666"">.</span>get(complement)             <span style=""color: #008000; font-weight: bold"">if</span> j <span style=""color: #AA22FF; font-weight: bold"">is</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> <span style=""color: #008000; font-weight: bold"">None</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> i <span style=""color: #666666"">!=</span> j:                 <span style=""color: #008000; font-weight: bold"">return</span> [i, j]             hashmap[n] <span style=""color: #666666"">=</span> i </pre></div> </td></tr></tbody></table></center></div><div><br></div>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Two Sum</div>  <center></center><div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">twoSum</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>], target: <span style=""color: #008000"">int</span>):         hashmap <span style=""color: #666666"">=</span> {}         <span style=""color: #008000; font-weight: bold"">for</span> i, n <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">enumerate</span>(nums):             complement <span style=""color: #666666"">=</span> target <span style=""color: #666666"">-</span> n             j <span style=""color: #666666"">=</span> hashmap<span style=""color: #666666"">.</span>get(complement)             <span style=""color: #008000; font-weight: bold"">if</span> j <span style=""color: #AA22FF; font-weight: bold"">is</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> <span style=""color: #008000; font-weight: bold"">None</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> i <span style=""color: #666666"">!=</span> j:                 <span style=""color: #008000; font-weight: bold"">return</span> [i, j]             hashmap[n] <span style=""color: #666666"">=</span> i </pre></div> </td></tr></tbody></table></center></div><div><br></div>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Contains Duplicates</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">containsDuplicate</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         s <span style=""color: #666666"">=</span> <span style=""color: #008000"">set</span>()         <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> nums:             <span style=""color: #008000; font-weight: bold"">if</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> s:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>             s<span style=""color: #666666"">.</span>add(n)         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Contains Duplicates</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">containsDuplicate</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         s <span style=""color: #666666"">=</span> <span style=""color: #008000"">set</span>()         <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> nums:             <span style=""color: #008000; font-weight: bold"">if</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> s:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>             s<span style=""color: #666666"">.</span>add(n)         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <b>Check if an array contains duplicates.</b><div><br></div><div>Search for the algorithm with lowest space complexity.</div>  <br/><br/> <div class=""example""><strong>Input:</strong>&nbsp;[1,2,3,1]&nbsp;<strong>Output:</strong>&nbsp;true<br><div><strong>Input:&nbsp;</strong>[1,2,3,4]&nbsp;<strong>Output:</strong>&nbsp;false</div></div>"	"<span style=""text-align: center;"">Because elements are sorted, duplicates will be contiguous.</span> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">containsDuplicate</span>(nums: List[<span style=""color: #008000"">int</span>]):         nums<span style=""color: #666666"">.</span>sort()         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-1</span>):             <span style=""color: #008000; font-weight: bold"">if</span> nums[i] <span style=""color: #666666"">==</span> nums[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center><center style=""text-align: center;""><br></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">containsDuplicate</span>(nums: List[<span style=""color: #008000"">int</span>]):         nums<span style=""color: #666666"">.</span>sort()         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-1</span>):             <span style=""color: #008000; font-weight: bold"">if</span> nums[i] <span style=""color: #666666"">==</span> nums[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center><center style=""text-align: center;""><br></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n\log n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">containsDuplicate</span>(nums: List[<span style=""color: #008000"">int</span>]):         nums<span style=""color: #666666"">.</span>sort()         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-1</span>):             <span style=""color: #008000; font-weight: bold"">if</span> nums[i] <span style=""color: #666666"">==</span> nums[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center><center style=""text-align: center;""><br></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div> <div>Given two strings <em>s</em> and <em>t</em>, write a function to determine if <em>t</em> is an anagram of <em>s</em>.</div><div><br></div><div>Fastest algorithm.</div>  <br/><br/> <div class=""example""><b>Input:</b> <em>s</em> = ""anagram"", <em>t</em> = ""nagaram"" <b>Output:</b> true<br><div><b>Input:</b> <em>s</em> = ""rat"", <em>t</em> = ""car"" <b>Output: </b>false<br></div></div>"	"Using sorting would have been slower. <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isAnagram</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>, t: <span style=""color: #008000"">str</span>):         <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #008000"">len</span>(s) <span style=""color: #666666"">!=</span> <span style=""color: #008000"">len</span>(t):             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>         hashmap <span style=""color: #666666"">=</span> collections<span style=""color: #666666"">.</span>defaultdict(<span style=""color: #008000"">int</span>)         <span style=""color: #008000; font-weight: bold"">for</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> s:             hashmap[ss] <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">for</span> tt <span style=""color: #AA22FF; font-weight: bold"">in</span> t:             <span style=""color: #008000; font-weight: bold"">if</span> hashmap<span style=""color: #666666"">.</span>get(tt, <span style=""color: #666666"">0</span>) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>             hashmap[tt] <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span> </pre></div> </td></tr></tbody></table></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isAnagram</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>, t: <span style=""color: #008000"">str</span>):         <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #008000"">len</span>(s) <span style=""color: #666666"">!=</span> <span style=""color: #008000"">len</span>(t):             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>         hashmap <span style=""color: #666666"">=</span> collections<span style=""color: #666666"">.</span>defaultdict(<span style=""color: #008000"">int</span>)         <span style=""color: #008000; font-weight: bold"">for</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> s:             hashmap[ss] <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">for</span> tt <span style=""color: #AA22FF; font-weight: bold"">in</span> t:             <span style=""color: #008000; font-weight: bold"">if</span> hashmap<span style=""color: #666666"">.</span>get(tt, <span style=""color: #666666"">0</span>) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>             hashmap[tt] <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span> </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isAnagram</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>, t: <span style=""color: #008000"">str</span>):         <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #008000"">len</span>(s) <span style=""color: #666666"">!=</span> <span style=""color: #008000"">len</span>(t):             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>         hashmap <span style=""color: #666666"">=</span> collections<span style=""color: #666666"">.</span>defaultdict(<span style=""color: #008000"">int</span>)         <span style=""color: #008000; font-weight: bold"">for</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> s:             hashmap[ss] <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">for</span> tt <span style=""color: #AA22FF; font-weight: bold"">in</span> t:             <span style=""color: #008000; font-weight: bold"">if</span> hashmap<span style=""color: #666666"">.</span>get(tt, <span style=""color: #666666"">0</span>) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>             hashmap[tt] <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span> </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div> <div>Given two strings&nbsp;<em>s</em>&nbsp;and&nbsp;<em>t</em>, write a function to determine if&nbsp;<em>t</em>&nbsp;is an anagram of&nbsp;<em>s</em>.</div><div><br></div><div>Most memory efficient algorithm.</div>  <br/><br/> <div class=""example""><b>Input:</b>&nbsp;<em>s</em>&nbsp;= ""anagram"",&nbsp;<em>t</em>&nbsp;= ""nagaram""&nbsp;<b>Output:</b>&nbsp;true<br><div><b>Input:</b>&nbsp;<em>s</em>&nbsp;= ""rat"",&nbsp;<em>t</em>&nbsp;= ""car""&nbsp;<b>Output:&nbsp;</b>false</div></div>"	"<span style=""text-align: -webkit-center;"">A hash table would have use \(O(n)\) space.</span> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isAnagram</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>, t: <span style=""color: #008000"">str</span>):         <span style=""color: #408080; font-style: italic""># Cannot sort directly strings</span>         s <span style=""color: #666666"">=</span> <span style=""color: #BA2121"">""""</span><span style=""color: #666666"">.</span>join(<span style=""color: #008000"">sorted</span>(s))           t <span style=""color: #666666"">=</span> <span style=""color: #BA2121"">""""</span><span style=""color: #666666"">.</span>join(<span style=""color: #008000"">sorted</span>(t))         <span style=""color: #008000; font-weight: bold"">return</span> s <span style=""color: #666666"">==</span> t</pre></div></td></tr></tbody></table><br></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isAnagram</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>, t: <span style=""color: #008000"">str</span>):         <span style=""color: #408080; font-style: italic""># Cannot sort directly strings</span>         s <span style=""color: #666666"">=</span> <span style=""color: #BA2121"">""""</span><span style=""color: #666666"">.</span>join(<span style=""color: #008000"">sorted</span>(s))           t <span style=""color: #666666"">=</span> <span style=""color: #BA2121"">""""</span><span style=""color: #666666"">.</span>join(<span style=""color: #008000"">sorted</span>(t))         <span style=""color: #008000; font-weight: bold"">return</span> s <span style=""color: #666666"">==</span> t</pre></div></td></tr></tbody></table><br></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n \log n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isAnagram</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>, t: <span style=""color: #008000"">str</span>):         <span style=""color: #408080; font-style: italic""># Cannot sort directly strings</span>         s <span style=""color: #666666"">=</span> <span style=""color: #BA2121"">""""</span><span style=""color: #666666"">.</span>join(<span style=""color: #008000"">sorted</span>(s))           t <span style=""color: #666666"">=</span> <span style=""color: #BA2121"">""""</span><span style=""color: #666666"">.</span>join(<span style=""color: #008000"">sorted</span>(t))         <span style=""color: #008000; font-weight: bold"">return</span> s <span style=""color: #666666"">==</span> t</pre></div></td></tr></tbody></table><br></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div> Given a string containing just the characters '(', ')', '{', '}', '[' and ']', determine if the input string is valid.  <br/><br/> <div class=""example""><strong>Input:</strong> ""()[]{}"" <strong>Output:</strong> true<br><div><strong>Input:</strong> ""(]"" <strong>Output:</strong> false<br></div></div>"	"<div>Use a <b>stack</b>&nbsp;to process progressively the sub-expression.</div><img src=""paste-d8f936e8f3b887447bd9303d57552e47afd7206a.jpg""> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isValid</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">bool</span>:         stack <span style=""color: #666666"">=</span> []         <span style=""color: #008000; font-weight: bold"">for</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> s:             <span style=""color: #008000; font-weight: bold"">if</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> (<span style=""color: #BA2121"">""(""</span>, <span style=""color: #BA2121"">""{""</span>, <span style=""color: #BA2121"">""[""</span>):                 stack<span style=""color: #666666"">.</span>append(ss)             <span style=""color: #008000; font-weight: bold"">elif</span> <span style=""color: #008000"">len</span>(stack) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 elt <span style=""color: #666666"">=</span> stack<span style=""color: #666666"">.</span>pop()                 <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>is_pair(elt, ss):                     <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000"">len</span>(stack) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>              <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">is_pair</span>(<span style=""color: #008000"">self</span>, opener, ender):         <span style=""color: #008000; font-weight: bold"">if</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""(""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">"")""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">elif</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""[""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""]""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">elif</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""{""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""}""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isValid</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">bool</span>:         stack <span style=""color: #666666"">=</span> []         <span style=""color: #008000; font-weight: bold"">for</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> s:             <span style=""color: #008000; font-weight: bold"">if</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> (<span style=""color: #BA2121"">""(""</span>, <span style=""color: #BA2121"">""{""</span>, <span style=""color: #BA2121"">""[""</span>):                 stack<span style=""color: #666666"">.</span>append(ss)             <span style=""color: #008000; font-weight: bold"">elif</span> <span style=""color: #008000"">len</span>(stack) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 elt <span style=""color: #666666"">=</span> stack<span style=""color: #666666"">.</span>pop()                 <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>is_pair(elt, ss):                     <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000"">len</span>(stack) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>              <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">is_pair</span>(<span style=""color: #008000"">self</span>, opener, ender):         <span style=""color: #008000; font-weight: bold"">if</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""(""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">"")""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">elif</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""[""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""]""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">elif</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""{""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""}""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isValid</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">bool</span>:         stack <span style=""color: #666666"">=</span> []         <span style=""color: #008000; font-weight: bold"">for</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> s:             <span style=""color: #008000; font-weight: bold"">if</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> (<span style=""color: #BA2121"">""(""</span>, <span style=""color: #BA2121"">""{""</span>, <span style=""color: #BA2121"">""[""</span>):                 stack<span style=""color: #666666"">.</span>append(ss)             <span style=""color: #008000; font-weight: bold"">elif</span> <span style=""color: #008000"">len</span>(stack) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 elt <span style=""color: #666666"">=</span> stack<span style=""color: #666666"">.</span>pop()                 <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>is_pair(elt, ss):                     <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000"">len</span>(stack) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>              <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">is_pair</span>(<span style=""color: #008000"">self</span>, opener, ender):         <span style=""color: #008000; font-weight: bold"">if</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""(""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">"")""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">elif</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""[""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""]""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">elif</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""{""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""}""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Best Time to Buy and Sell Stock</div> <div>Say you have an array for which the <em>i</em><sup>th</sup> element is the price of a given stock on day <em>i</em>.</div><div><br></div> <div>If you were only permitted to complete <b>at most one transaction</b> (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.</div><div><br></div> <div>Note that you cannot sell a stock before you buy one.</div>  <br/><br/> <div class=""example""><strong>Input:</strong> [7,1,5,3,6,4]<div><strong>Output:</strong> 5</div><div><strong>Explanation:</strong> Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5. &nbsp; Not 7-1 = 6, as selling price needs to be larger than buying price.<br></div></div>"	"<img src=""paste-3f1f2156ae4d39eb61b770efbea243fe6db87088.jpg""><br><div>The points of interest are the <b>peaks</b> and <b>valleys</b> in the given graph. We need to find the largest peak following the smallest valley. We can maintain two variables - <b>minprice</b> and <b>maxprofit</b> corresponding to the smallest valley and maximum profit.<br></div> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxProfit</span>(<span style=""color: #008000"">self</span>, prices: List[<span style=""color: #008000"">int</span>]):         minprice <span style=""color: #666666"">=</span> <span style=""color: #008000"">float</span>(<span style=""color: #BA2121"">""inf""</span>)         maxprofit <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>                  <span style=""color: #008000; font-weight: bold"">for</span> p <span style=""color: #AA22FF; font-weight: bold"">in</span> prices:             <span style=""color: #008000; font-weight: bold"">if</span> p <span style=""color: #666666"">-</span> minprice <span style=""color: #666666"">&gt;</span> maxprofit:                 maxprofit <span style=""color: #666666"">=</span> p <span style=""color: #666666"">-</span> minprice             <span style=""color: #008000; font-weight: bold"">elif</span> p <span style=""color: #666666"">&lt;</span> minprice:                 minprice <span style=""color: #666666"">=</span> p                      <span style=""color: #008000; font-weight: bold"">return</span> maxprofit </pre></div> </td></tr></tbody></table></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Best Time to Buy and Sell Stock</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxProfit</span>(<span style=""color: #008000"">self</span>, prices: List[<span style=""color: #008000"">int</span>]):         minprice <span style=""color: #666666"">=</span> <span style=""color: #008000"">float</span>(<span style=""color: #BA2121"">""inf""</span>)         maxprofit <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>                  <span style=""color: #008000; font-weight: bold"">for</span> p <span style=""color: #AA22FF; font-weight: bold"">in</span> prices:             <span style=""color: #008000; font-weight: bold"">if</span> p <span style=""color: #666666"">-</span> minprice <span style=""color: #666666"">&gt;</span> maxprofit:                 maxprofit <span style=""color: #666666"">=</span> p <span style=""color: #666666"">-</span> minprice             <span style=""color: #008000; font-weight: bold"">elif</span> p <span style=""color: #666666"">&lt;</span> minprice:                 minprice <span style=""color: #666666"">=</span> p                      <span style=""color: #008000; font-weight: bold"">return</span> maxprofit </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Best Time to Buy and Sell Stock</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxProfit</span>(<span style=""color: #008000"">self</span>, prices: List[<span style=""color: #008000"">int</span>]):         minprice <span style=""color: #666666"">=</span> <span style=""color: #008000"">float</span>(<span style=""color: #BA2121"">""inf""</span>)         maxprofit <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>                  <span style=""color: #008000; font-weight: bold"">for</span> p <span style=""color: #AA22FF; font-weight: bold"">in</span> prices:             <span style=""color: #008000; font-weight: bold"">if</span> p <span style=""color: #666666"">-</span> minprice <span style=""color: #666666"">&gt;</span> maxprofit:                 maxprofit <span style=""color: #666666"">=</span> p <span style=""color: #666666"">-</span> minprice             <span style=""color: #008000; font-weight: bold"">elif</span> p <span style=""color: #666666"">&lt;</span> minprice:                 minprice <span style=""color: #666666"">=</span> p                      <span style=""color: #008000; font-weight: bold"">return</span> maxprofit </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Product of Array Except Self</div></div> Given an array nums of <em>n</em> integers where <em>n</em> &gt; 1, &nbsp;return an array output such that output[i] is equal to the product of all the elements of nums except nums[i].<br><div><br></div><div>Do it without division, and with space complexity \(O(n)\).</div>  <br/><br/> <div class=""example""><b>Input:</b> [1,2,3,4]&nbsp;<div><b>Output:</b> [24,12,8,6]</div></div>"	"<img src=""paste-ce91ad85cd899b3178470cd47017ad655e9d2c06.jpg""><br><div>We compute an array for the left-product (red) and the right-product (yellow).</div> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">productExceptSelf</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         left <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         right <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             left[i] <span style=""color: #666666"">=</span> left[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">2</span>, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):                 right[i] <span style=""color: #666666"">=</span> right[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums)):             nums[i] <span style=""color: #666666"">=</span> left[i] <span style=""color: #666666"">*</span> right[i]         <span style=""color: #008000; font-weight: bold"">return</span> nums </pre></div> </td></tr></tbody></table></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Product of Array Except Self</div></div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">productExceptSelf</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         left <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         right <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             left[i] <span style=""color: #666666"">=</span> left[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">2</span>, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):                 right[i] <span style=""color: #666666"">=</span> right[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums)):             nums[i] <span style=""color: #666666"">=</span> left[i] <span style=""color: #666666"">*</span> right[i]         <span style=""color: #008000; font-weight: bold"">return</span> nums </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(N)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Product of Array Except Self</div></div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">productExceptSelf</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         left <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         right <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             left[i] <span style=""color: #666666"">=</span> left[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">2</span>, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):                 right[i] <span style=""color: #666666"">=</span> right[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums)):             nums[i] <span style=""color: #666666"">=</span> left[i] <span style=""color: #666666"">*</span> right[i]         <span style=""color: #008000; font-weight: bold"">return</span> nums </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(N)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Product of Array Except Self</div></div> Given an array nums of&nbsp;<em>n</em>&nbsp;integers where&nbsp;<em>n</em>&nbsp;&gt; 1, &nbsp;return an array output such that output[i] is equal to the product of all the elements of nums except nums[i].<br><div><br></div><div>Do it without division, and with space complexity \(O(1)\) (excluding output array).</div>  <br/><br/> <div class=""example""><b>Input:</b> [1,2,3,4]<div><b>Output:</b> [24,12,8,6]</div></div>"	"<img src=""paste-ce91ad85cd899b3178470cd47017ad655e9d2c06.jpg""><br><div>We compute an array for the left-product (red). The right-product are computed <i>on-the-fly </i>by tracking the running product.</div> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">productExceptSelf</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         left <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             left[i] <span style=""color: #666666"">=</span> left[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>]          tmp <span style=""color: #666666"">=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):               left[i] <span style=""color: #666666"">=</span> left[i] <span style=""color: #666666"">*</span> tmp             tmp <span style=""color: #666666"">*=</span> nums[i]          <span style=""color: #008000; font-weight: bold"">return</span> left </pre></div> </td></tr></tbody></table></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Product of Array Except Self</div></div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">productExceptSelf</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         left <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             left[i] <span style=""color: #666666"">=</span> left[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>]          tmp <span style=""color: #666666"">=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):               left[i] <span style=""color: #666666"">=</span> left[i] <span style=""color: #666666"">*</span> tmp             tmp <span style=""color: #666666"">*=</span> nums[i]          <span style=""color: #008000; font-weight: bold"">return</span> left </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(N)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Product of Array Except Self</div></div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">productExceptSelf</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         left <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             left[i] <span style=""color: #666666"">=</span> left[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>]          tmp <span style=""color: #666666"">=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):               left[i] <span style=""color: #666666"">=</span> left[i] <span style=""color: #666666"">*</span> tmp             tmp <span style=""color: #666666"">*=</span> nums[i]          <span style=""color: #008000; font-weight: bold"">return</span> left </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\) (the output array doesn't count)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Max SubArray</div></div> Given an integer array nums, find the contiguous subarray&nbsp;(containing at least one number) which has the largest sum and return its sum.<br><div><br></div><div>Naive algorithm in \(O(n)\).</div>  <br/><br/> <div class=""example""><strong>Input:</strong> [-2,1,-3,4,-1,2,1,-5,4],<div><strong>Output:</strong> 6</div><div><strong>Explanation:</strong>&nbsp;[4,-1,2,1] has the largest sum = 6.<br></div></div>"	"We simply tracking the running sum and the best running sum. <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxSubArray</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         c <span style=""color: #666666"">=</span> nums[<span style=""color: #666666"">0</span>]         best_val <span style=""color: #666666"">=</span> c          <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> nums[<span style=""color: #666666"">1</span>:]:             c <span style=""color: #666666"">+=</span> n             <span style=""color: #008000; font-weight: bold"">if</span> n <span style=""color: #666666"">&gt;</span> c:                 c <span style=""color: #666666"">=</span> n               best_val <span style=""color: #666666"">=</span> <span style=""color: #008000"">max</span>(best_val, c)                      <span style=""color: #008000; font-weight: bold"">return</span> best_val </pre></div> </td></tr></tbody></table></center></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Max SubArray</div></div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxSubArray</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         c <span style=""color: #666666"">=</span> nums[<span style=""color: #666666"">0</span>]         best_val <span style=""color: #666666"">=</span> c          <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> nums[<span style=""color: #666666"">1</span>:]:             c <span style=""color: #666666"">+=</span> n             <span style=""color: #008000; font-weight: bold"">if</span> n <span style=""color: #666666"">&gt;</span> c:                 c <span style=""color: #666666"">=</span> n               best_val <span style=""color: #666666"">=</span> <span style=""color: #008000"">max</span>(best_val, c)                      <span style=""color: #008000; font-weight: bold"">return</span> best_val </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(N)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Max SubArray</div></div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">maxSubArray</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         c <span style=""color: #666666"">=</span> nums[<span style=""color: #666666"">0</span>]         best_val <span style=""color: #666666"">=</span> c          <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> nums[<span style=""color: #666666"">1</span>:]:             c <span style=""color: #666666"">+=</span> n             <span style=""color: #008000; font-weight: bold"">if</span> n <span style=""color: #666666"">&gt;</span> c:                 c <span style=""color: #666666"">=</span> n               best_val <span style=""color: #666666"">=</span> <span style=""color: #008000"">max</span>(best_val, c)                      <span style=""color: #008000; font-weight: bold"">return</span> best_val </pre></div> </td></tr></tbody></table></center>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Superpixels"	"<b>A group of several pixels</b>&nbsp;that have <u>common characteristics</u>.  <div class=""details1"">Mostly Old School CV algos, as <u>selective search</u>.</div>  <div class=""details2""></div>   <img src=""paste-ce71c77054939e42b55c8e90343b9cbc4db3ec8f.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why the word <u>naive</u>&nbsp;in <b>Naive Bayes</b>?"	"Because of the <u>naive assumption</u>&nbsp;that all dimensions of the features are <b>independent</b>.  <div class=""details1""></div>  <div class=""details2""></div>   \(\begin{aligned} p\left(c | x_{1}, x_{2}, \ldots, x_{n}\right) &amp; \propto p\left(c, x_{1}, x_{2}, \ldots, x_{n}\right) \\ &amp; \propto p(c) p\left(x_{1} | c\right) p\left(x_{2} | c\right) \ldots p\left(x_{n} | c\right) \\ &amp; \propto p(c) \prod_{i=1}^{n} p\left(x_{i} | c\right) \end{aligned}\)<br><div><br></div><div>\(\hat{y}=\arg \max _{c \in 1, \ldots, C} p(c) \prod_{i=1}^{n} p\left(x_{i} | c\right)\)</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to measure the <u>contribution</u>&nbsp;of each feature in <b>Naive Bayes</b>?<div><br></div><div>\(\hat{y}=\arg \max _{c \in 1, \ldots, C} p(c) \prod_{i=1}^{n} p\left(x_{i} | c\right)\)</div>"	"By measuring the <b>posterior </b>\(p\left(c | x_{i}\right)=p(c) p\left(x_{i} | c\right) / p\left(x_{i}\right)\).  <div class=""details1"">The higher the posterior is for \(x_i\), the more the \(i^{th}\) dimension <u>explains</u>&nbsp;the label \(c\).</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What are the <u>interpretable components</u><b>&nbsp;</b>in <b>LIME</b>?"	"Groups of <b>superpixels</b>.  <div class=""details1"">They are constructed by old school cv like <u>selective search</u>.</div>  <div class=""details2""></div>   <img src=""paste-efbb6b8a508ce74c603fac16d64fa49d2f713a53.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How does <b>LIME </b>learns <u>interpretability</u>&nbsp;?"	"It learns a <u>linear model</u>&nbsp;to <b>match original model prediction</b> on a few <u>superpixels</u>.  <div class=""details1"">The linear model learns a binary output: is this superpixel useful or not?</div>  <div class=""details2""></div>   <img src=""paste-b7212391e3af514b154283c1955bcb9aa2e8df84.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Occam’s Razor"	"<div><i>“Simpler solutions are more likely to be correct than complex ones.”</i></div>  <div class=""details1"">Somewhat related to <u>overfitting</u>: simpler solutions usually generalize better.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Minimum Description Length"	"View learning as <b>data compression</b>.  <div class=""details1"">By compressing the data, we need to discover <u>regularity</u>&nbsp;in the data.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Kolmogorov Complexity"	"<b>Length of the shortest binary computer program</b> that describes the object.  <div class=""details1"">It is related to the <u>minimum length description</u>&nbsp;of models, and is approximately equal to the <u>entropy</u>.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Universal Approximation Theorem"	"<div>An <b>MLP </b>with a hidden layer and some non-linearity can approximate <b>any continuous functions</b>.</div>  <div class=""details1"">The hidden layer may be extremely <u>large</u>.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>MLP </b>acronym"	"<b>M</b>ulti-<b>L</b>ayers&nbsp;<b>P</b>erceptron  <div class=""details1"">Aka several FC layers stacked.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>Double-U-shaped risk curve</b> for deep neural networks"	"Empirically, NNs may <u>overfit</u>&nbsp;first, to <u>generalize</u>&nbsp;only after.  <div class=""details1"">Contrary to <u>classic stats model </u>that only have the first U-shaped curve.</div>  <div class=""details2""></div>   <img src=""paste-167225f6ab35df2e3322c5c1e427235f152bf6e9.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why a <b>smoother manifold</b>&nbsp;is beneficial to optimization?"	"Because gradients are more <b>predictive</b>&nbsp;and thus may improve the model, and allows <u>higher learning rates</u>.  <div class=""details1"">This is a reason why <u>BatchNormalization</u>&nbsp;stabilize training.</div>  <div class=""details2""></div>   <img src=""paste-b07c322a68be2df2d94882694cd31f59fea3012e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How big is the <b>Intrinsic Dimension</b>&nbsp;compared to the model dimension?"	"Much <b>lower</b>.  <div class=""details1"">This shows that <u>model size</u> is not a good indicator to <u>overfitting</u>.</div>  <div class=""details2""></div>   <img src=""paste-df00274faabfe1bb466d3d6b44e5b3bb809b3125.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which layers are more <b>critical </b>to the model performance?"	"The <b>first </b>layers.  <div class=""details1"">If <u>re-initialized</u>, performance collapse much more than later layers.</div>  <div class=""details2""></div>   <img src=""paste-113d65f60a6dc2d122d8d4167257d737ed3818c5.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which layers are more&nbsp;<b>critical&nbsp;</b>to <u>ResNet</u> performance?"	"The <b>first layers</b> of each <u>residual block</u>.  <div class=""details1"">Contrary to other model, ResNet distribute <u>evenly</u>&nbsp;the sensitive layers thanks to its <u>shortcuts</u>.</div>  <div class=""details2""></div>   <img src=""paste-b262dfc4d5e10b877d09fa035e1242e4380fa6bc.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  M Competitions"	"A serie of <b>forecasting </b>competitions based on various <u>real&nbsp;data</u> and period.  <div class=""details1"">It was created in 1982 by&nbsp;<u>M</u>akridakis. As of 2020, the 5th version was pending.</div>  <div class=""details2""></div>   <img src=""paste-b14e4e943e6a48b59cde3ba22c423e1763f17f68.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  When can be have a <b>close-form</b>&nbsp;of the <u>posterior</u>?<div>\(p(\theta | x)=\frac{p(x | \theta) p(\theta)}{p(x)}\)</div>"	"When we have a <b>conjugate prior</b>.  <div class=""details1"">Otherwise&nbsp;numerical integration or <u>approximation</u> are needed as we cannot have infinite data.</div>  <div class=""details2"">The conjugate prior and the posterior have the same <u>distribution family</u>.</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  What are the <b>free parameters </b>of a <u>Gaussian distribution</u>?"	"The <b>mean </b>\(\mathbf{\mu}\) and the <b>covariance matrix </b>\(\mathbf{\Sigma}\).  <div class=""details1"">\(\mathbf{\mu}\) has \(D\) parameters.</div>  <div class=""details2"">\(\mathbf{\Sigma}\) has \(D(D + 1)/2\) parameters.</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  What is the <b>computational problem </b>in a <u>Gaussian distribution</u>?<div><br><div>\(\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}\)</div></div>"	"We need to <b>inverse </b>the <u>covariance matrix</u>&nbsp;\(\mathbf{\Sigma}\).  <div class=""details1"">\(\mathbf{\Sigma}\) has \(D(D + 1)/2\) parameters.</div>  <div class=""details2"">It can be unfeasible for very large dimension space.</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  How to <b>reduce</b>&nbsp;the number of free parameters of a <u>Gaussian distribution</u>?<br><div><br></div><div>\(\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}\)<br></div>"	"By considering <u>covariance matrices</u>&nbsp;that are <b>diagonal</b>, so that \(\mathbf{\Sigma} = \text{diag}(\sigma_i^2)\).  <div class=""details1"">The covariance matrix now only have \(D\) parameters, with the mean \(2D\).</div>  <div class=""details2"">We can reduce furthermore the number of parameters with <u>isotropic</u>&nbsp;covariance \(\mathbf{\Sigma} = \sigma^2\mathbf{I}\).</div>   <img src=""paste-24af954c5f8b95e259e85c28075745986400c8ad.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Interpret intuitively a <u>covariance matrix</u>&nbsp;that is <b>diagonal</b>: \(\mathbf{\Sigma} = \text{diag}(\sigma_i^2)\)."	"All dimensions are independents to each other \(\sigma_{ij} = 0\).  <div class=""details1"">This reduces the number of parameters from \(D(D+1)/2\) to \(D\).</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  How can we have a&nbsp;<u>covariance matrix</u>&nbsp;with a <b>single parameter</b>?"	"With \(\mathbf{\Sigma} = \sigma^2\mathbf{I}\).  <div class=""details1"">The matrix is called <u>isotropic</u>.</div>  <div class=""details2""></div>   <img src=""paste-c0bf9af3550541f17ffcced7bab381f3938d56e7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <div>Which contour is parametrized by a <b>diagonal </b><u>covariance matrix,</u>&nbsp;and why?</div><div><br></div><img src=""paste-f272ad1b91c37544739dc9cb0c51e9912f9af294.jpg"">"	"The (b), because the elliptical contours are <u>aligned</u> to the axes as <b>dimensions are independent</b>&nbsp;to each others.  <div class=""details1"">\(\mathbf{\Sigma} = \text{diag}(\sigma_i^2)\)</div>  <div class=""details2"">\(\sigma_{ij} = 0\)</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <div>Which contour is parametrized by an&nbsp;<b>isotropic&nbsp;</b><u>covariance matrix,</u>&nbsp;and why?</div><div><br></div><img src=""paste-f272ad1b91c37544739dc9cb0c51e9912f9af294.jpg"">"	"The (c), because the contours are <u>concentric circles</u>&nbsp;as it is parametrized by a <b>single parameter</b>.  <div class=""details1"">\(\mathbf{\Sigma} = \sigma_i \mathbf{I}\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  What is an <b>isotropic </b>covariance matrix?<div></div>"	"\(\mathbf{\Sigma} = \sigma_i \mathbf{I}\)  <div class=""details1"">It is parametrized by a single parameter.</div>  <div class=""details2""></div>   <img src=""paste-808cc659c19123c34f4662b7d324a4cf5195dda8.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <div></div><div>In what ways are <u>gaussian distributions</u>&nbsp;<b>limited</b>?</div>"	"They are <b>unimodal</b>, and thus failed to represent multimodal distributions.  <div class=""details1"">unimodal = has a single maximum</div>  <div class=""details2"">Most real-world distributions are multimodal.</div>   <img src=""paste-f272ad1b91c37544739dc9cb0c51e9912f9af294.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  What is the name of the <b>inverse of the covariance</b>?<div>\(\Lambda \equiv \Sigma^{-1}\)</div>"	"The <b>precision</b>.  <div class=""details1"">Intuitively, the higher the precision, the lower the covariance.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <b>Marginalization</b>&nbsp;of a <u>Gaussian distribution</u>?"	"Taking the distribution of a single dimension \(x_a\).<div><br></div><div>\(\boldsymbol{\mu}=\left(\begin{array}{c}\boldsymbol{\mu}_{a} \\ \boldsymbol{\mu}_{b}\end{array}\right)\).</div>  <div class=""details1"">The result is also <u>gaussian</u>.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Family of the <b>conditionning</b> of one <u>gaussian</u>&nbsp;by another?"	"It is also a <b>gaussian</b>.  <div class=""details1"">We say that the family is <u>closed</u>&nbsp;to conditionning.</div>  <div class=""details2"">It also works with <u>marginalization</u>.</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Marginal distribution of \(\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \text { with } \mathbf{\Lambda} \equiv \mathbf{\Sigma}^{-1}\)?"	"\(p\left(\mathbf{x}_{a}\right)=\mathcal{N}\left(\mathbf{x}_{a} | \boldsymbol{\mu}_{a}, \boldsymbol{\Sigma}_{a a}\right)\)  <div class=""details1"">The marginalization of a gaussian distribution is also gaussian.</div>  <div class=""details2""></div>   <img src=""paste-f7aab8bacdf3a2843e3b7e068f77a748c621b6b1.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Conditional distribution of \(\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \text { with } \mathbf{\Lambda} \equiv \mathbf{\Sigma}^{-1}\)?"	"\(\begin{aligned} p\left(\mathbf{x}_{a} | \mathbf{x}_{b}\right) &amp;=\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}_{a | b}, \boldsymbol{\Lambda}_{a a}^{-1}\right) \\ \boldsymbol{\mu}_{a | b} &amp;=\boldsymbol{\mu}_{a}-\boldsymbol{\Lambda}_{a a}^{-1} \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \end{aligned}\)  <div class=""details1"">The conditional of a gaussian distribution is also gaussian.</div>  <div class=""details2""></div>   <img src=""paste-f7aab8bacdf3a2843e3b7e068f77a748c621b6b1.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In a <b style="""">sequential setting</b>, how to interpret this result?<div><br></div><div>\(\begin{aligned} \boldsymbol{\mu}_{\mathrm{ML}}^{(N)} &amp;=\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_{n} \\ &amp;=\frac{1}{N} \mathbf{x}_{N}+\frac{1}{N} \sum_{n=1}^{N-1} \mathbf{x}_{n} \\ &amp;=\frac{1}{N} \mathbf{x}_{N}+\frac{N-1}{N} \boldsymbol{\mu}_{\mathrm{ML}}^{(N-1)} \\ &amp;=\boldsymbol{\mu}_{\mathrm{ML}}^{(N-1)}+\frac{1}{N}\left(\mathbf{x}_{N}-\boldsymbol{\mu}_{\mathrm{ML}}^{(N-1)}\right) \end{aligned}\)</div>"	"We move the old estimate by a fraction \(1/N\) of the <u>error signal</u>&nbsp;\(\left(\mathbf{x}_{N}-\boldsymbol{\mu}_{\mathrm{ML}}^{(N-1)}\right)\).  <div class=""details1"">Contribution from successive data points get smaller.</div>  <div class=""details2"">At some points, the estimate converge.</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  How does the <b>eigenvectors</b>&nbsp;of the <u>covariance</u>&nbsp;\(\Sigma\) affect this Gaussian distribution?<div><img src=""paste-874a64df396aecbf36473c40a4e4c6d328189523.jpg""><br></div>"	"It affect the <i>orientation</i>&nbsp;of the elliptical surface.  <div class=""details1"">With \(u_1\) and \(u_2\) the <u>eigenvectors</u>.</div>  <div class=""details2"">Note that \(\lambda_1\) and \(\lambda_2\) are the&nbsp;<u>eigenvalues</u>.</div>   <img src=""paste-3e0c82de4f5f5febaf5f430de0989421682190f7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  How does the <b>eigenvalues</b>&nbsp;of the <u>covariance</u>&nbsp;\(\Sigma\) affect this Gaussian distribution?<div><img src=""paste-874a64df396aecbf36473c40a4e4c6d328189523.jpg""><br></div>"	"It affect the <i>spread</i>&nbsp;of the elliptical surface.  <div class=""details1"">With \(\lambda_1\) and \(\lambda_2\)&nbsp;the&nbsp;<u>eigenvalues</u>.</div>  <div class=""details2"">Note that \(u_1\) and \(u_2\)&nbsp;are the&nbsp;<u>eigenvectors</u>.</div>   <img src=""paste-3e0c82de4f5f5febaf5f430de0989421682190f7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  <div>With \(\mathbf{y}\) and \(\mathbf{x}\) two <u>Gaussians</u>, describe the mean of the <b>conditionnal gaussian&nbsp;</b>\(p(\mathbf{y} | \mathbf{x})\).</div>"	"<div>It is a <b>linear transformation </b>of \(\mathbf{x}\)'s mean:</div><div><br></div>\(p(\mathbf{y} | \mathbf{x})=N\left(\mathbf{y}, \mathbf{A} \mathbf{x}+\mathbf{b}, L^{-1}\right)\)  <div class=""details1"">We call this a <u>linear gaussian model</u>.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Given <u>gaussian</u> data, what is the distribution of the <b>conjugate prior </b>\(p(\mu)\)?"	"A <b>gaussian </b>distribution: \(p(\mu)=\mathcal{N}\left(\mu | \mu_{0}, \sigma_{0}^{2}\right)\)  <div class=""details1"">The posterior \(p(\mu | \mathbf{X}) \propto p(\mathbf{X} | \mu) p(\mu)\) is therefore also&nbsp;<u>gaussian</u>.</div>  <div class=""details2"">In this case, the parameter \(\sigma^2\) is known while \(\mu\) unknown.</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Given <u>gaussian</u> data, what is the distribution of the <b>conjugate prior </b>\(p(\frac{1}{\sigma^2}) = p(\lambda)\)?"	"A <b>gamma&nbsp;</b>distribution: \(\operatorname{Gam}(\lambda | a, b)=\frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda)\)  <div class=""details1"">The posterior \(p(\mu | \mathbf{X}) \propto p(\mathbf{X} | \mu) p(\mu)\) is therefore also <u>gaussian</u>.</div>  <div class=""details2"">In this case, the parameter \(\mu\) is known while \(\sigma^2\) unknown.</div>   <img src=""paste-8b2a48bb98d3a5e59e3fdbf6b339d184dc1706b2.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  What is this function name \(\text{?}(n)=(n-1)!\)"	"The <b>Gamma </b>function \(\Gamma(n)=(n-1)!\).  <div class=""details1"">It's the <u>normalizing factor</u>&nbsp;in the <u>Gamma</u>&nbsp;distribution \(\operatorname{Gam}(\lambda | a, b)=\frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda)\).</div>  <div class=""details2"">The Gamma&nbsp;distribution&nbsp;as prior for&nbsp;<u>gaussian</u>&nbsp;data when \(\sigma^2\) is unknown.</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Given&nbsp;<u>gaussian</u>&nbsp;data, what is the distribution of the&nbsp;<b>conjugate prior&nbsp;</b>\(p(\mu, \frac{1}{\sigma^2}) = p(\mu, \lambda)\)?"	"The <b>normal gamma </b>distribution: \(N\left(\mu | \mu_{0}, \lambda_{0}^{-1}\right) \operatorname{Gam}(\lambda | a, b)\).  <div class=""details1"">The prior for \(\mu\) is <u>gaussian</u>&nbsp;and for \(sigma^2\) <u>gamma</u>.</div>  <div class=""details2"">If the gaussian is <u>multivariate</u>, the prior is the <u>Gaussian-Wishart</u>&nbsp;distribution.</div>   <img src=""paste-377ae46e68153f17978d21ab1d745580d2158d16.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Relation of the <b>t-Student </b>distribution to the <u>Gaussian</u>&nbsp;distribution?"	"The student&nbsp;distribution is an <b>infinite sum of Gaussian</b>, with same mean but <u>different precisions</u>:<div><br></div><div><div>\(p(x | \mu, a, b)=\int_{0}^{\infty} N\left(x | \mu, \tau^{-1}\right) \operatorname{Gam}(\tau | a, b) d \tau\)</div></div>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-f82e7a4b94425074e64d4fa4024c5f9fc3988d66.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Probability</div>  Why does the <b><font color=""#fc0107"">Student</font></b>&nbsp;distribution&nbsp;is less sensible to <u>outliers</u>&nbsp;than the <font color=""#21ff06"">Gaussian</font> distribution?<div><img src=""paste-b82309ec14384305249bcc3f2606d0349b377861.jpg""><br></div>"	"Because the Student distribution has a <b>longer tails</b>.  <div class=""details1"">Outliers are catched by this long tails without alterating the main mode.</div>  <div class=""details2"">Regression can be more robust with a Student distribution.</div>   <img src=""paste-32c7f57c209de91c9a0dea7255e15b0c29ea0b96.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How does a <b>Gaussian </b>in <u>high dimension</u>&nbsp;looks like on a <u>unit-sphere</u>?"	"Like a <b>uniform </b>distribution.  <div class=""details1"">The <u>mode</u>&nbsp;is meaningless. <i>i.e. a gray image is the mode of all images.</i></div>  <div class=""details2""></div>   <img src=""paste-5740e1aa4001e67302af460d2271f2f1da5f8c54.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is dangerous when <u>interpolating</u>&nbsp;from a <b>Gaussian </b>in <u>high dimension</u>?"	"The interpolation results <b>inside the ""soap bubble""</b>.<div><br></div><div>Gaussian looks like a uniform over a unit-sphere.</div>  <div class=""details1"">Instead of doing a <u>convex combination</u>&nbsp;\(p x_1 + (1 - p)x_2\), do an interpolation in <u>polar coordinates</u>&nbsp;\(\sqrt{p} x_1 + \sqrt{(1 - p)} x_2\).</div>  <div class=""details2""></div>   <img src=""paste-164730e622ffb63fb304bb9d93d17050263d7e8b.jpg"">"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Python</div> What are <b>memoryviews</b>?"	"A data structure where we can access a <b>slice of the data without copying.</b><div><b><br></b></div><div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">import</span> <span style=""color: #0000FF; font-weight: bold"">time</span> <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> (<span style=""color: #666666"">100000</span>, <span style=""color: #666666"">200000</span>, <span style=""color: #666666"">300000</span>, <span style=""color: #666666"">400000</span>):     data <span style=""color: #666666"">=</span> <span style=""color: #BA2121"">'x'</span><span style=""color: #666666"">*</span>n     start <span style=""color: #666666"">=</span> time<span style=""color: #666666"">.</span>time()     b <span style=""color: #666666"">=</span> data     <span style=""color: #008000; font-weight: bold"">while</span> b:         b <span style=""color: #666666"">=</span> b[<span style=""color: #666666"">1</span>:]     <span style=""color: #008000"">print</span> <span style=""color: #BA2121"">'bytes'</span>, n, time<span style=""color: #666666"">.</span>time()<span style=""color: #666666"">-</span>start  <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> (<span style=""color: #666666"">100000</span>, <span style=""color: #666666"">200000</span>, <span style=""color: #666666"">300000</span>, <span style=""color: #666666"">400000</span>):     data <span style=""color: #666666"">=</span> <span style=""color: #BA2121"">'x'</span><span style=""color: #666666"">*</span>n     start <span style=""color: #666666"">=</span> time<span style=""color: #666666"">.</span>time()     b <span style=""color: #666666"">=</span> <span style=""color: #008000"">memoryview</span>(data)     <span style=""color: #008000; font-weight: bold"">while</span> b:         b <span style=""color: #666666"">=</span> b[<span style=""color: #666666"">1</span>:]     <span style=""color: #008000"">print</span> <span style=""color: #BA2121"">'memoryview'</span>, n, time<span style=""color: #666666"">.</span>time()<span style=""color: #666666"">-</span>start</pre></div></td></tr></tbody></table></center><br></div><div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000"">bytes</span> <span style=""color: #666666"">100000</span> <span style=""color: #666666"">0.200068950653</span> <span style=""color: #008000"">bytes</span> <span style=""color: #666666"">200000</span> <span style=""color: #666666"">0.938908100128</span> <span style=""color: #008000"">bytes</span> <span style=""color: #666666"">300000</span> <span style=""color: #666666"">2.30898690224</span> <span style=""color: #008000"">bytes</span> <span style=""color: #666666"">400000</span> <span style=""color: #666666"">4.27718806267</span> <span style=""color: #008000"">memoryview</span> <span style=""color: #666666"">100000</span> <span style=""color: #666666"">0.0100269317627</span> <span style=""color: #008000"">memoryview</span> <span style=""color: #666666"">200000</span> <span style=""color: #666666"">0.0208270549774</span> <span style=""color: #008000"">memoryview</span> <span style=""color: #666666"">300000</span> <span style=""color: #666666"">0.0303030014038</span> <span style=""color: #008000"">memoryview</span> <span style=""color: #666666"">400000</span> <span style=""color: #666666"">0.0403470993042</span> </pre></div> </td></tr></tbody></table></center><br></div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Two reasons to use <b>exponential</b>&nbsp;in the <u>softmax</u>:<div><br></div><div>\(\sigma(\mathbf{x})_{i}=\frac{e^{x_{i}}}{\sum_{j=1}^{N} e^{x_{j}}}\)</div>"	"1. Values are <b>positive</b><div><br></div><div>2. It <b>increases the difference</b> between the largest and second largest values, making the function more <u>maximum-like</u></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>self-attention</b>?"	"The <b>average</b> of all vector elements, <b>weighted</b> by the inner <b>similarities</b>.<br><div><br></div><div><img src=""paste-1fd836e16c6206b07db82957e198b1a195ea3696.jpg""><br></div><div><img src=""paste-399f47615a7215a655a4bf2f8ab2dede4a20171b.jpg""><br><img src=""paste-299109d676620e151bdb21c2f235691ac2b2a0eb.jpg""><br></div>  <div class=""details1"">It propagates information between vectors</div>  <div class=""details2""></div>   <img src=""paste-a394c4e74e3f43d5302e03eb57dc09cc455996a5.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is intuitively <b>self-attention </b>for sentences?"	"How much the <b>words are similar to each others</b>.  <div class=""details1"">""cat"" will be similar to ""walks"" because of their <u>relation</u></div>  <div class=""details2""></div>   <img src=""paste-8495bc9c4f1e2265770b555bccf7f6e4660f9446.jpg""><br><img src=""paste-41c204b7fb76bb9e8858a45fe8498d3a98016ccc.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>query, key, value </b>trick?"	"Similar to <b>self-attention</b>, but each copy of the inputs is <b>linearly transformed</b>.  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-cf8699cb2c45558162ab2337b40edd4e75e0aaac.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What are the <b>multi-heads </b>in <u>transformers</u>?"	"We use \(k\) matrices for linear transformations before the <u>self-attention</u>:<br><img src=""paste-7549988dcca2fde4ce8860092f8918b075a0a897.jpg"">  <div class=""details1"">Each head is supposed to learn a <u>different kind of similarity</u></div>  <div class=""details2"">All head outputs \(y\) are then concatenated and fed to a linear to <u>reduce the dimension</u> to a single output</div>   <img src=""paste-0aacccda505a16c6b3bf6944106e9ef64618584a.jpg"">"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the class of problems&nbsp;<b>P </b>in the P=NP conjecture?"	"Problems that can be <b>solved</b> in <b>polynomial</b>&nbsp;time.  <br/><br/> <span style=""font-size: 12px"">\(N\), \(N^2\), \(N^{100}\)... but not \(2^N\) for example&nbsp;</span> <br/><br/> <img src=""paste-91bc57cf8d977a075b217286d421f9c2533df550.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the class of problems <b>NP </b>in the P=NP conjecture?"	"Problems that can be <b>verified</b> in <b>polynomial</b>&nbsp;time.  <br/><br/> <span style=""font-size: 12px"">Check if the solution is correct, not what is the solution</span> <br/><br/> <img src=""paste-91bc57cf8d977a075b217286d421f9c2533df550.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What would it mean if <b>P=NP</b>?"	"All problems that can be <b>verified </b>in polynomial time, can also be <b>solved </b>in polynomial time.  <br/><br/> <span style=""font-size: 12px"">In this case, it means some algos could break RSA in polynomial time</span> <br/><br/> <img src=""paste-91bc57cf8d977a075b217286d421f9c2533df550.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the set of problem <b>NP-Complete</b>?"	"Set of problems to which any <b>NP problems can be reduced to</b>.  <br/><br/> <span style=""font-size: 12px"">Solving P = NP-Complete, would mean that P = NP</span> <br/><br/> <img src=""paste-f1f41f906ecb8e5b15c6a6185f7f35332f73e2e4.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Why does mathematicians try to see if <b>P =&nbsp;NP-Complete</b>?"	"If one problem NP-Complete is solvable in polynomial time (P) then all NP problems are also  <br/><br/> <span style=""font-size: 12px"">NP-Complete is a set of problems to which any<b>&nbsp;</b><u>NP problems can be reduced to.</u></span> <br/><br/> <img src=""paste-f1f41f906ecb8e5b15c6a6185f7f35332f73e2e4.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <b>NP-Hard </b>problems?"	"Problems that are <b>not</b> necessarely <b>solvable in polynomial </b>time  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-f1f41f906ecb8e5b15c6a6185f7f35332f73e2e4.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Why is the <b>SAT </b>problem famous?"	"Because it's the first problem proven to be <b>NP-Complete</b>  <br/><br/> <span style=""font-size: 12px"">It was proven by Cook &amp; Levin, 1971</span> <br/><br/> <img src=""paste-f1f41f906ecb8e5b15c6a6185f7f35332f73e2e4.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the <b>SAT </b>problem?"	"A <b>Boolean satisfiability </b>to check if a boolean formula can ever be true.&nbsp;  <br/><br/> <span style=""font-size: 12px"">The formula ""<i>a</i> AND NOT <i>b</i>"" is satisfiable because one can find the values <i>a</i>&nbsp;=&nbsp;TRUE and <i>b</i>&nbsp;=&nbsp;FALSE, which make (<i>a</i> AND NOT <i>b</i>)&nbsp;=&nbsp;TRUE. In contrast, ""<i>a</i> AND NOT <i>a</i>"" is unsatisfiable.</span> <br/><br/> <img src=""paste-09b5a60be249db73fdc8f5503dc4392bbf1ec73c.jpg""> "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Derivative of Composite Functions</div>  \(\frac{d}{d x} f(g(x))=\) ..."	"\(\frac{d}{d x} f(g(x))=f^{\prime}(g(x)) \cdot g^{\prime}(x)\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Derivative of Inverse Functions</div>  \(\frac{d}{d x} f^{-1}(x)=\) ..."	"\(\frac{d}{d x} f^{-1}(x)=\frac{1}{f^{\prime}\left(f^{-1}(x)\right)}\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-394c95d2c86efc2575bf6e42650575eb030b6e47.jpg""><br><div><img src=""paste-24f2c6410c57d318e39e6c4a74b06d7126c3e34e.jpg""><br></div><div>Then by <b>chain rule</b>:</div><div><img src=""paste-7f1e11bb74bcbd270a0d5597711ecd152c4db005.jpg""><br></div><div><img src=""paste-15dbe8fe24f3b5165bf644ce2102ea036ac8c4fc.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to find a matrix <b>determinant </b>from its <b>eigenvalues</b>?"	"determinant = <b>product</b> of eigenvalues  <div class=""details1"">It's how much we scale in all directions&nbsp;</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Determinant of a triangular matrix?<div><img src=""paste-e736fca1669ba0f31abbe3406d084769b91c2eb9.jpg""><br></div>"	"Product of the diagonal values  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Product of the diagonal values for a triangular matrix?<div><img src=""paste-e736fca1669ba0f31abbe3406d084769b91c2eb9.jpg""><br></div>"	"Equal to the matrix <b>determinant</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Determinant of an identity matrix?"	"Equal to 1  <div class=""details1"">Because the identity factor doesn't change the space, the scaling factor is kept constant (1)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\text{det}(c A) = \)... with \(A \in \mathbb{R}^{n \times n}\) and \(c \in \mathbb{R}\)"	"\(\text{det}(c A) = c^n \text{det}(A)\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Determinant of the inverse of a matrix?<div><br></div><div>\(\operatorname{det}\left(\mathrm{A}^{-1}\right)=\) ...<br></div>"	"\(\operatorname{det}\left(\mathrm{A}^{-1}\right)= \frac{1}{\operatorname{det}(\mathrm{A})}\)  <div class=""details1"">Intuitively, the negative inverses the scaling factor</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Determinant of tranpose matrix?<div><br></div><div>\(\operatorname{det}\left(\mathrm{A}^{\mathrm{T}}\right)=\) ...<br></div>"	"\(\operatorname{det}\left(\mathrm{A}^{\mathrm{T}}\right)=\operatorname{det}(\mathrm{A})\)  <div class=""details1"">The rotation changes but not the scaling factor</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How <b>rotation matrices </b>can be intepretated with <b>coordinate systems</b>?"	"Each <b>column</b> is the <b>unit vector </b>of a new coordinate system  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-9d77094fad527807158301dc297e670a96b37eeb.jpg"">"
A <span class=cloze>[yes or non?]</span>-singular&nbsp;matrix</b> is an invertible matrix.	A <span class=cloze><b>non</span>-singular&nbsp;matrix</b> is an invertible matrix.<br><br> 
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When is <b>StarGAN </b>useful compared to <u>CycleGAN</u>?"	"When there is <b>many domains</b>  <div class=""details1"">StarGAN only needs to learn one generator and one discriminator for ›\(k\) domains</div>  <div class=""details2"">CycleGAN would have learn \((k - 1)k\) models</div>   <img src=""paste-5d71ec268943aee851d2a82653fb8c4ea1e28232.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the new loss of <b>StarGAN </b>over <u>CycleGAN</u>?"	"A <b>domain classification </b>loss  <div class=""details1"">The generator aims to generate a particular domain, not simply a ""target domain""</div>  <div class=""details2""></div>   <img src=""paste-5d71ec268943aee851d2a82653fb8c4ea1e28232.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why is&nbsp;<b>CycleGAN</b>&nbsp;hard to scale to many domains?"	"Because the <b>number of models to train grows quadratically</b>  <div class=""details1"">To \((k-1)k\) models for \(k\) domains</div>  <div class=""details2""></div>   <img src=""paste-3f6e8e09e09d8a76cb1c20364d7f6e36733cb7d3.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the other name of <b>Atrous Convolutions</b>?"	"Dilated Convolutions  <div class=""details1"">Atrous was introduced by DeepLab</div>  <div class=""details2""></div>   <img src=""paste-183f6a96ec4d06366a58aaf3c893c21e5c46699b.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the other name of <b>Dilated Convolutions</b>?"	"Atrous Convolutions  <div class=""details1"">Atrous was introduced by DeepLab</div>  <div class=""details2""></div>   <img src=""paste-183f6a96ec4d06366a58aaf3c893c21e5c46699b.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is&nbsp;<b>Dilated Convolutions</b>?"	"Convolutions with ""<b>holes</b>"" inside the kernel  <div class=""details1"">It expands the output features dimensions</div>  <div class=""details2"">Also called ""Atrous"" convolutions because of its holes = ""à trous""</div>   <img src=""paste-183f6a96ec4d06366a58aaf3c893c21e5c46699b.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the other name of&nbsp;<b>Transposed Convolutions</b>?"	"<b>Deconvolutions</b>  <div class=""details1"">However the name ""deconvolution"" is incorrect because it imply the inverse of a convolutions</div>  <div class=""details2""></div>   <img src=""paste-f3661109ff45c0d1a413da58b631cc708d0bcf6e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the other name of&nbsp;<b>Deconvolution</b>?"	"<b>Transposed convolutions</b>  <div class=""details1"">However the name ""deconvolution"" is incorrect because it imply the inverse of a convolutions</div>  <div class=""details2""></div>   <img src=""paste-f3661109ff45c0d1a413da58b631cc708d0bcf6e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is&nbsp;<b>Transposed Convolutions</b>?"	"Convolutions with padding between the images pixels  <div class=""details1"">It expands the output features dimensions</div>  <div class=""details2""></div>   <img src=""paste-f3661109ff45c0d1a413da58b631cc708d0bcf6e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the advantages of <b>Dilated </b>convolutions vs <b>Transposed </b>convolutions?"	"Dilated convolutions use <b>less memory </b>because the <u>kernel is smaller</u> as it has <u>holes</u>.  <div class=""details1""></div>  <div class=""details2""></div>   <div>Dilated:</div><div><img src=""paste-5fb0c764a532b159bf9702097184dc4c561681dc.jpg""><br></div><div><br></div>Tranposed:<div><img src=""paste-f3661109ff45c0d1a413da58b631cc708d0bcf6e.jpg""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>Taylor serie</b>?"	"An expansion of a function into an <b>infinite sum of terms</b>  <div class=""details1"">\(\cos x-1=-\frac{x^{2}}{2}+\frac{x^{4}}{24}-\frac{x^{6}}{720}+O\left(x^{8}\right)\)</div>  <div class=""details2"">It is useful to get an <u>approximation</u>&nbsp;that is more precise as more terms are computed</div>   <img src=""paste-51d884a5a1545bf11fb5fa68ee4250d539d133f3.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the generic form of a <b>Taylor serie</b>?"	"<div>\(\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n !}(x-a)^{n}\)</div><div><br></div>\(f(a)+\frac{f^{\prime}(a)}{1 !}(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^{2}+\frac{f^{\prime \prime \prime}(a)}{3 !}(x-a)^{3}+\cdots\)  <div class=""details1"">This is a power serie</div>  <div class=""details2"">We usually set a=0, also called <u>Maclaurin serie</u></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the condition to develop a function into a <b>Taylor serie</b>?"	"<div>It must be <b>infinitely differentiable </b>at a point \(a\)</div><div></div>  <div class=""details1"">\(\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n !}(x-a)^{n}\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>Maclaurin serie</b>?"	"<div>A <b>Taylor serie</b>&nbsp;with fixed point \(a=0\)</div><div></div>  <div class=""details1""><div>\(\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n !}(x-a)^{n} = \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n !}x^{n}\)</div></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Maclaurin serie of the <b>exponential</b>?"	"\(e^{x}=\sum_{n=0}^{\infty} \frac{x^{n}}{n !}=1+x+\frac{x^{2}}{2 !}+\frac{x^{3}}{3 !}+\cdots\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Derivative of the <b>exponential</b>?"	"equal to itself  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the interest of the <b>SVRG </b>algorithm over SGD?"	"It reduces the <b>variance of the gradients</b>&nbsp;induced by the random sampling  <div class=""details1"">\(w_{t}=w_{t-1}-\eta_{t}\left(\nabla \psi_{i_{t}}\left(w_{t-1}\right)-\nabla \psi_{i_{t}}(\tilde{w})+\nabla P(\tilde{w})\right)\)</div>  <div class=""details2""></div>   <img src=""paste-3e600ac76b9576c04427cbe41b432c9463351d5d.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why do we need to reduce the <b>learning rate </b>in <b>SGD</b>&nbsp;(non-batched)?"	"Because of the <b>gradient variance </b>we may never reach convergence  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the effect of <b>mini-batch</b>&nbsp;on the SGD convergence?"	"It <b>reduces gradient variance</b>&nbsp;by averaging several (a mini-batch) together  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How does the <b>SVGR </b>algorithm reduce the <b>gradient variance</b>?"	"The gradient variance is reduced using the <b>variance of a previous parameters snapshot</b>.  <div class=""details1"">\(w_{t}=w_{t-1}-\eta_{t}\left(\nabla \psi_{i_{t}}\left(w_{t-1}\right)-\nabla \psi_{i_{t}}(\tilde{w})+\nabla P(\tilde{w})\right)\)</div>  <div class=""details2"">I think \(\nabla P(\tilde{w})\) can be seen as some kind of <u>memory</u>&nbsp;of a very averaged gradient</div>   <img src=""paste-3e600ac76b9576c04427cbe41b432c9463351d5d.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>SAG </b>iteration?"	"We update with <b>all previously stored gradients</b> and <b>a new gradient for a randomly sampled</b> example  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-ad30339ea1c59665c8bd0f3095d05b74109c1387.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the full name of the algorithm <b>SVRG</b>&nbsp;used as remplacement of SGD?"	"<b>S</b>tochastic <b>V</b>ariance <b>R</b>educed <b>G</b>radient  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the full name of the algorithm <b>SAG</b>&nbsp;used as remplacement of SGD?"	"<b>S</b>tochastic <b>A</b>verage&nbsp;<b>G</b>radient&nbsp;  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the interest of the <b>SAG&nbsp;</b>algorithm over SGD?"	"It <b>speed up </b>convergence without needing to compute several gradients per iteration as with mini-batch  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-b89f0c5df731208f58f71833c1cc45b2be0b7fa0.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>convergence rate</b>?"	"The number of iterations needed to approximate an optimal value with an error of \(\epsilon\).  <div class=""details1"">\(f\left(x^{(k)}\right)-f\left(x^{*}\right) \leq \epsilon\) with \(k\) the current number of iterations</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>convergence rate </b>of SGD on a <u>convex</u>&nbsp;function?"	"\(O(1 / k)\) which means that we need \(O(1/\epsilon)\) iterations for \(f\left(x^{(k)}\right)-f\left(x^{*}\right) \leq \epsilon\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to describe a <b>convergence rate </b>of \(O(1 / k)\)?"	"It is a <b>sub-linear </b>convergence  <div class=""details1"">This is the convergence rate of <u>SGD</u> on a <u>convex</u> function</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is (put it simply) <b>SAGA</b>?"	"An improvement of <b>SAG</b>&nbsp;with simpler proof and <b>unbiased gradient estimate</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How is this matrix called?<div><img src=""paste-6cdf8227c85a20223d907cbb442f06691f888e75.jpg""><br></div>"	"A <b>Toeplitz </b>matrix  <div class=""details1"">Each diagonal from left to right is constant</div>  <div class=""details2"">The <u>convolution</u> can be done in a single matrix multiplication using a Toeplitz matrix</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is a <b>Toeplitz </b>matrix?</div>"	"A matrix which <b>each diagonal from left to right is constant</b>  <div class=""details1"">The&nbsp;<u>convolution</u>&nbsp;can be done in a single matrix multiplication using a Toeplitz matrix</div>  <div class=""details2""></div>   <img src=""paste-6cdf8227c85a20223d907cbb442f06691f888e75.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>How can we do a <b>convolution </b>on an image in a single matrix multiplication?<br><br><i>(short answer)</i></div>"	"By transfering the convolution <u>kernel</u>&nbsp;into a group of<b>&nbsp;Toeplitz</b>&nbsp;matrix  <div class=""details1""></div>  <div class=""details2""></div>   <div>This is a Toeplitz matrix</div><img src=""paste-6cdf8227c85a20223d907cbb442f06691f888e75.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How can we do a <b>convolution </b>on an image in a single matrix multiplication using <u>Toeplitz</u>&nbsp;matrix?<br>"	"1. Given a signal \(I\) and a kernel \(F\):<div><img src=""paste-8fe5c634cf0ec27ad44acd0af01a79a23f21296d.jpg""><br></div><div><br></div><div>2. <b>Pad the kernel</b> to the output size:</div><div><img src=""paste-2fccedab3588fc6e0337783ee175d333055230ca.jpg""><br></div><div><br></div><div>3. Create <b>Toeplitz</b>&nbsp;matrix for each row:</div><div><img src=""paste-3bba6cb24865634c3618f8c6cf2a7c2eacaaa4da.jpg""><br></div><div><br></div><div>4. Combine them in a big <b>doubly blocked </b>Toeplitz matrix:</div><div><img src=""paste-e6f6f9ce0410d2cc41e3c2db12592cd3d6a1b63b.jpg""><br></div><div><br></div><div>5. Convert the input matrix into a <b>column vector</b>:</div><div><img src=""paste-8a01661ec5cbfafa33c413b555191e3cc6c98b4b.jpg""><br></div><div><br></div><div>6. <b>Multiply </b>and reshape the result to a matrix form:</div><div><img src=""paste-ae52c1596456c76958442b674a663e7559354a54.jpg""><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   <div></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A function \(f\) is <b>invariant</b> if <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A function \(f\) is <b>invariant</b> if <span class=cloze>\(f(T(x))=f(x)\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A function \(f\) is <span class=cloze>[]</span> if \(f(T(x))=f(x)\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A function \(f\) is <span class=cloze><b>invariant</b></span> if \(f(T(x))=f(x)\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A function \(f\) is <b>equivariant</b> if <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A function \(f\) is <b>equivariant</b> if <span class=cloze>\(f(T(x))=T(f(x))\)</span><br><br> <div style=""font-style: italic; font-size: 14px""><u>Convolution</u> is equivariant (but not invariant) with respect to <u>translation</u></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A function \(f\) is <span class=cloze>[]</span> if \(f(T(x))=T(f(x))\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A function \(f\) is <span class=cloze><b>equivariant</b></span> if \(f(T(x))=T(f(x))\)<br><br> <div style=""font-style: italic; font-size: 14px""><u>Convolution</u> is equivariant (but not invariant) with respect to <u>translation</u></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why do <b>mini-batches </b>must be sampled randomly?"	"Mini-batches are used to compute an <b>unbiased estimate of the batch gradient</b>, thus it requires <b>independent samples</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the intuition about the <b>inverse </b>of a matrix usage?"	"We can use it to <b>revert the linear transformation</b>&nbsp;applied by the matrix  <div class=""details1"">The inverse of a rotation matrix of 90° would be a rotation matrix of -90°</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>Injective </b>function"	"Map each element of the target domain to at <b>most one source domain element</b>  <div class=""details1"">Tip: <u>in</u>jective -&gt; <u>in</u>put</div>  <div class=""details2""></div>   <img src=""paste-95d53b4c9182b0e4582029a24604334d7148e770.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>Bijective&nbsp;</b>function"	"Each element of one set is <b>paired with exactly one</b> element of the other set  <div class=""details1"">Injective + surjective</div>  <div class=""details2""></div>   <img src=""paste-db29c4d0181c501ff6022b05bfd2352d09ff6d52.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <img src=""paste-a8dcb820820d6568b3affbdda7c4c9426c7ad256.jpg""><div>What kind of function?</div>"	"Injective  <div class=""details1"">Each target element maps to at most one element of the source set</div>  <div class=""details2"">Tip:&nbsp;<u>in</u>jective -&gt;&nbsp;<u>in</u>put</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div><img src=""paste-121b5cbf41db726e60da884ef8588315dfb555d8.jpg""><br></div><div>What kind of function?</div>"	"Bijective  <div class=""details1"">All elements are paired to exactly one element from the target set</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div><img src=""paste-cca9316e38fb39554886763c70f04454e9f4b21e.jpg""><br></div><div>What kind of function?</div>"	"Surjective  <div class=""details1""><div>Each target element is mapped by <b>at least one element</b> from the source</div></div>  <div class=""details2"">Tip: <u>sur</u>jective -&gt;&nbsp;<u>sor</u>tie</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <b>Surjective</b> function"	"Each target element has <b>at least one </b>source element  <div class=""details1"">Tip:&nbsp;<u>sur</u>jective -&gt;&nbsp;<u>sor</u>tie</div>  <div class=""details2""></div>   <img src=""paste-141aeb0c0a950788fa0615a5968b4feca63ad132.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <img src=""paste-8f7e643aec828c481bcf8583a17dc716fde228d4.jpg""><br>What kind of function?"	"non-injective, non-surjective  <div class=""details1""><u>not injective:</u>&nbsp;C is mapped twice by 3 and 4</div>  <div class=""details2""><u>not surjective:</u> A is not mapped</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Condition for a function to have an <b>inverse</b>?"	"It must be <b>bijective</b>  <div class=""details1"">Each element of one set is mapped to a unique element of the other set</div>  <div class=""details2""></div>   <img src=""paste-c88a582e4519d29f96f0175acac8d0219b7f6ffa.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why <b>rectangular matrix</b> cannot have an <b>inverse</b>?"	"Because they are not <b>bijective</b>  <div class=""details1"">Intuitively a rectangular matrix <u>change the target space dimension</u>, thus no bijection is possible</div>  <div class=""details2""></div>   <img src=""paste-c88a582e4519d29f96f0175acac8d0219b7f6ffa.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a matrix with <b>full column rank</b>?"	"All its columns are <b>linearly independent</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a matrix with <b>full row rank</b>?"	"All its rows are <b>linearly independent</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a matrix with <b>full rank</b>?"	"All its rows and columns are <b>linearly independent</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <span class=cloze>[injective / surjective / bijective]</span> if the matrix has <b>full column rank</b>.<br><br>This implies that \(m \le n\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <span class=cloze><b>injective</b></span> if the matrix has <b>full column rank</b>.<br><br>This implies that \(m \le n\)<br><br> <div style=""font-style: italic; font-size: 14px"">Nb: Rank(A) = min(m,n)</div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>injective</b> if the matrix has <span class=cloze>[...rank]</span>.<br><br>This implies that \(m \le n\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>injective</b> if the matrix has <span class=cloze><b>full column rank</b></span>.<br><br>This implies that \(m \le n\)<br><br> <div style=""font-style: italic; font-size: 14px"">Nb: Rank(A) = min(m,n)</div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>injective</b> if the matrix has <b>full column rank</b>.<br><br>This implies that <span class=cloze>[\(m\) ? \(n\)]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>injective</b> if the matrix has <b>full column rank</b>.<br><br>This implies that <span class=cloze>\(m \le n\)</span><br><br> <div style=""font-style: italic; font-size: 14px"">Nb: Rank(A) = min(m,n)</div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <span class=cloze>[injective / surjective / bijective]</span> if the matrix has <b>full row rank</b>.<br><br>This implies that \(m \ge n\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <span class=cloze><b>surjective</b></span> if the matrix has <b>full row rank</b>.<br><br>This implies that \(m \ge n\)<br><br> <div style=""font-style: italic; font-size: 14px"">Nb: Rank(A) = min(m,n)</div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>surjective</b> if the matrix has <span class=cloze>[...rank]</span>.<br><br>This implies that \(m \ge n\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>surjective</b> if the matrix has <span class=cloze><b>full row rank</b></span>.<br><br>This implies that \(m \ge n\)<br><br> <div style=""font-style: italic; font-size: 14px"">Nb: Rank(A) = min(m,n)</div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>surjective</b> if the matrix has <b>full row rank</b>.<br><br>This implies that <span class=cloze>[\(m\) ? \(n\)]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>surjective</b> if the matrix has <b>full row rank</b>.<br><br>This implies that <span class=cloze>\(m \ge n\)</span><br><br> <div style=""font-style: italic; font-size: 14px"">Nb: Rank(A) = min(m,n)</div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <span class=cloze>[injective / surjective / bijective]</span> if the matrix has <b>full row rank </b>and <b>full column rank</b>.<br><br>This implies that \(m = n\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <span class=cloze><b>bijective</b></span> if the matrix has <b>full row rank </b>and <b>full column rank</b>.<br><br>This implies that \(m = n\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>bijective</b> if the matrix has <span class=cloze>[...rank]</span>.<br><br>This implies that \(m = n\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>bijective</b> if the matrix has <span class=cloze><b>full row rank </b>and <b>full column rank</b></span>.<br><br>This implies that \(m = n\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>bijective</b> if the matrix has <b>full row rank </b>and <b>full column rank</b>.<br><br>This implies that <span class=cloze>[\(m\) ? \(n\)]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  The transformation of a matrix \(n \times m\) is <b>bijective</b> if the matrix has <b>full row rank </b>and <b>full column rank</b>.<br><br>This implies that <span class=cloze>\(m = n\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is an <b>array capacity</b>?"	"The maximum number of elements that can be stored in the array  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is an <b>array length</b>?"	"The current number of elements that are stored in the array  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Complexity of <b>adding</b> an element at the <b>end</b> of a fixed array?"	"O(1)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-421d02f132a3b735edcce544583b421edccd9dc0.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Complexity of <b>adding</b> an element at the <b>start</b> of a fixed array?"	"O(N)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-50691bb6b8eaef842c271d4a493d2caf30de8d52.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Complexity of <b>adding</b> an element <b>anywhere</b> in a fixed array?"	"At worse O(N) (inserting at the start), at best O(1) (inserting at the end)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-c11d70014e0e4ef3dbe2d1db40ad42de921f84da.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Complexity of <b>deleting</b> an element at the <b>end</b>&nbsp;of a fixed array?"	"O(1)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-0e8188cd5a2226832445dd41877f43bbfd345047.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Complexity of <b>deleting</b> an element at the <b>start</b>&nbsp;of a fixed array?"	"O(N)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-e4a195d99a8c6553c6d097ad301f127947c8fa3c.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Complexity of <b>deleting</b> an element <b>anywhere</b>&nbsp;in a fixed array?"	"At worse O(N)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-383bb7dc9c260b9ef435c60dab4480a7f9be1e1e.jpg""> "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Intutively, what are the <b>eigenvectors </b>of a linear transformation matrix?"	"The <b>direction </b>in which it is stretched by the transformation&nbsp;  <div class=""details1"">The <u>eigenvalue</u>&nbsp;correspond to a stretch factor</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Intutively, what are the <b>eigenvalues&nbsp;</b>of a linear transformation matrix?"	"The <b>factor&nbsp;</b>in which each eigenvectors is stretched by the transformation&nbsp;  <div class=""details1"">The <u>eigenvector</u>&nbsp;correspond to the direction stretched by the transformation</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>shape </b>of a matrix \(A^TA\)?"	"<b>Square</b>&nbsp;matrix  <div class=""details1"">If \(A \in \mathbb{R}^{m \times n}\) then \(A^TA \in \mathbb{R}^{n \times n}\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What can we say about the <b>eigenvalues </b>of \(A^TA\)?"	"They are <b>positive</b>  <div class=""details1"">They are also the square of the <u>singular values</u>&nbsp;of \(A\)</div>  <div class=""details2""></div>   <img src=""paste-20aade6f51689fcfa6d44a9776edb0db4df94714.jpg"">"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Let \(x\) be an eigenvector of \(A\).<br><br>Then \(Ax = \) <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Let \(x\) be an eigenvector of \(A\).<br><br>Then \(Ax = \) <span class=cloze>\(\lambda x\) with \(\lambda\) an eigenvalue</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Let \(\lambda\) and \(x\) be resp. an eigenvalue and an eigenvector of \(A\)<br><br>Then \(\lambda x = \) <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  Let \(\lambda\) and \(x\) be resp. an eigenvalue and an eigenvector of \(A\)<br><br>Then \(\lambda x = \) <span class=cloze>\(A x\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  With the matrix \(A\), how to compute its&nbsp;<b>singular values</b>?"	"\(\sigma_i = \sqrt{\lambda_i}\) with \(\lambda_i\) eigenvalues of \(A^TA\)  <div class=""details1"">Singular values are always positive</div>  <div class=""details2"">The number of nonzero singular values of \(A\) equals \(A\)'s rank</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  With the matrix \(A\), what is:<br><br>\(\sigma_i = \sqrt{\lambda_i}\) with \(\lambda_i\) eigenvalues of \(A^TA\)<br>"	"Singular values  <div class=""details1"">Singular values are always positive</div>  <div class=""details2"">The number of nonzero singular values of \(A\) equals \(A\)'s rank</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What are the standard <b>basis vectors </b>in \(\mathbb{R}^3\)?"	"<img src=""paste-fc5025573d57e4203551862f8b9f80932dd7a1a6.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What could be those vectors?</div><img src=""paste-fc5025573d57e4203551862f8b9f80932dd7a1a6.jpg"">"	"The standard <b>basis vectors </b>in \(\mathbb{R}^3\)  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <div>A <span class=cloze>[...rank]</span> matrix creates linear dependencies between standard <b>basis vectors</b>, thus the projection will map a <b>higher</b>-dimensional vector space to a <b>lower</b>-dimensional vector space.</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <div>A <span class=cloze><b>low-rank</b></span> matrix creates linear dependencies between standard <b>basis vectors</b>, thus the projection will map a <b>higher</b>-dimensional vector space to a <b>lower</b>-dimensional vector space.</div><br><br> <div style=""font-style: italic; font-size: 14px"">A \(3 \times 3\) matrix with rank \(2\) will project 3d space into a plane.</div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <div>A <b>low-rank</b> matrix creates linear dependencies between <span class=cloze>[]</span>, thus the projection will map a <b>higher</b>-dimensional vector space to a <b>lower</b>-dimensional vector space.</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <div>A <b>low-rank</b> matrix creates linear dependencies between <span class=cloze>standard <b>basis vectors</b></span>, thus the projection will map a <b>higher</b>-dimensional vector space to a <b>lower</b>-dimensional vector space.</div><br><br> <div style=""font-style: italic; font-size: 14px"">A \(3 \times 3\) matrix with rank \(2\) will project 3d space into a plane.</div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <div>A <b>low-rank</b> matrix creates linear dependencies between standard <b>basis vectors</b>, thus the projection will map a <span class=cloze>[]</span>-dimensional vector space to a <span class=cloze>[]</span>-dimensional vector space.</div> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <div>A <b>low-rank</b> matrix creates linear dependencies between standard <b>basis vectors</b>, thus the projection will map a <span class=cloze><b>higher</b></span>-dimensional vector space to a <span class=cloze><b>lower</b></span>-dimensional vector space.</div><br><br> <div style=""font-style: italic; font-size: 14px"">A \(3 \times 3\) matrix with rank \(2\) will project 3d space into a plane.</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the <b>null-space</b>?</div>"	"The set of all vectors that land on the <b>origin </b>or become <b>null</b>&nbsp;after a transformation.  <div class=""details1"">A \(3 \times 3\) matrix with rank \(2\) will project 3d space into a plane. One of the 3 basis vector will be in the null space.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the set of all vectors that land on the&nbsp;<b>origin&nbsp;</b>or become&nbsp;<b>null</b>&nbsp;after a transformation.</div>"	"The <b>null-space</b>  <div class=""details1"">A \(3 \times 3\) matrix with rank \(2\) will project 3d space into a plane. One of the 3 basis vector will be in the null space.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the <b>spectral norm</b>?</div>"	"The <b>maximum singular value </b>of a matrix  <div class=""details1"">We can think of the spectral norm as measuring the <u>maximum amount that a matrix can “stretch”</u> a vector</div>  <div class=""details2""></div>   \(\sigma_{1}=\max _{\|\mathbf{x}\|_{2} \neq \mathbf{0}} \frac{\|A \mathbf{x}\|_{2}}{\|\mathbf{x}\|_{2}}\)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>To what correspond the&nbsp;<b>maximum singular value&nbsp;</b>of a matrix?</div>"	"To the <b>spectral norm</b>  <div class=""details1"">We can think of the spectral norm as measuring the&nbsp;<u>maximum amount that a matrix can “stretch”</u>&nbsp;a vector</div>  <div class=""details2""></div>   \(\sigma_{1}=\max _{\|\mathbf{x}\|_{2} \neq \mathbf{0}} \frac{\|A \mathbf{x}\|_{2}}{\|\mathbf{x}\|_{2}}\)"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\text{det}(AB) =\) <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \(\text{det}(AB) =\) <span class=cloze>\(\text{det}(A)\text{det}(B)\)</span><br><br> <div style=""font-style: italic; font-size: 14px"">Multiplicative property of the determinant<div><br></div><div><img src=""paste-4274b59efce102298d4be3adf15fc9681a6590dc.jpg""><br></div></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>chirality </b>of a space intuitively?"	"Whether we are looking at the space or its <b>mirror</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0061201fe88bf0185c79929eca8d4ccf969a44d7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Intuitively what can we say if a <b>determinant </b>is negative?"	"The <u>chirality</u>&nbsp;changes and we are looking at the <b>mirror </b>space  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0061201fe88bf0185c79929eca8d4ccf969a44d7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to interpret the <b>singular values </b>of a matrix?"	"They are the <b>width </b>and <b>length </b>of our new basis  <div class=""details1"">If a singular value equal 0, it means that one of the dimension has been collapsed</div>  <div class=""details2"">The larger singular value (the <u>spectral norm</u>) is the maximum ""action"" of the transformation</div>   <img src=""paste-8d95f2fb2089a8268aa09914dfde7fd6e39f6853.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What can we say if one <b>singular value </b>equals 0"	"One of the dimension has been collapsed  <div class=""details1"">Singular values are the&nbsp;<b>width&nbsp;</b>and&nbsp;<b>length&nbsp;</b>of our new basis</div>  <div class=""details2"">The matrix is not inversible</div>   <img src=""paste-8d95f2fb2089a8268aa09914dfde7fd6e39f6853.jpg"">"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  If a matrix has a <b>singular value </b>equal to 0, then it is <span class=cloze>[]</span>. "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  If a matrix has a <b>singular value </b>equal to 0, then it is <span class=cloze>not inversible</span>.<br><br> <div style=""font-style: italic; font-size: 14px"">One dimension has been collapsed, thus the transformation is not <u>bijective</u></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>[]</span>\(=\)\(B^T A^T\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>\((AB)^T\)</span>\(=\)\(B^T A^T\)<br><br> <div style=""font-style: italic; font-size: 14px""><img src=""paste-9bf22dc33c8ef499f0a3bf3f2013913549287e22.jpg""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \((AB)^T\)\(=\)<span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \((AB)^T\)\(=\)<span class=cloze>\(B^T A^T\)</span><br><br> <div style=""font-style: italic; font-size: 14px""><img src=""paste-9bf22dc33c8ef499f0a3bf3f2013913549287e22.jpg""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is this <b>decomposition</b>?<br><br>\(A=U \Sigma V^{T}\)"	"<b>S</b>ingular <b>V</b>alue <b>D</b>ecomposition (SVD)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0b2ce94926a6e70b7e262095fbe038623f7f9ed4.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does the acronyme SVD mean?"	"<b>S</b>ingular <b>V</b>alue <b>D</b>ecomposition  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0b2ce94926a6e70b7e262095fbe038623f7f9ed4.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In the SVD, what is \(U\)?<br><br>\(A=U \Sigma V^{T}\)"	"Unitary orthogonal matrix  <div class=""details1"">Also called the <u>left singular vectors</u></div>  <div class=""details2"">\(A \in \mathbb{R}^{m \times n}\) and \(U \in \mathbb{R}^{m \times m}\)</div>   <img src=""paste-0b2ce94926a6e70b7e262095fbe038623f7f9ed4.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In the SVD, what is \(V\)?<br><br>\(A=U \Sigma V^{T}\)"	"<b>Orthonormal</b> matrix that acts as a <u>rotation matrix</u>  <div class=""details1"">Also called the <u>right singular vectors</u>, it is the orthonormal eigenvectors of \(A^TA\)</div>  <div class=""details2"">\(A \in \mathbb{R}^{m \times n}\) and \(V \in \mathbb{R}^{n \times n}\)</div>   <img src=""paste-0b2ce94926a6e70b7e262095fbe038623f7f9ed4.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In the SVD, \(A=U \Sigma V^{T}\), how to find \(V\)?"	"They are the orthonormal <b>eigenvectors</b>&nbsp;of \(A^TA\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0b2ce94926a6e70b7e262095fbe038623f7f9ed4.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In the SVD, \(A=U \Sigma V^{T}\), how to find \(\Sigma\)?"	"This is a diagonal matrix made of the <b>singular values </b>of \(A\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0b2ce94926a6e70b7e262095fbe038623f7f9ed4.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>TruncatedSVD</b>?"	"An SVD where only <b>largest</b> \(t\) <b>singular values</b>&nbsp;are considered  <div class=""details1"">\(\tilde{\mathbf{A}}=\mathbf{U}_{t} \boldsymbol{\Sigma}_{t} \mathbf{V}_{t}^{T}\)</div>  <div class=""details2"">First \(t\) columns of \(U\), first \(t\) rows of \(V^T\) and first \(t\) columns &amp; rows of \(\Sigma\)</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the interest of <b>TruncatedSVD</b>&nbsp;over SVD?"	"<b>Faster</b> to compute and <b>less storage </b>used as we only consider the largest singular values  <div class=""details1"">\(\tilde{\mathbf{A}}=\mathbf{U}_{t} \boldsymbol{\Sigma}_{t} \mathbf{V}_{t}^{T}\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>semantic segmentation</b>?"	"Each pixel has its <b>class labelized</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-c2f111d528f9078045f2ee064d5eea7bef92afe0.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What kind of segmentation?<br><img src=""paste-c2f111d528f9078045f2ee064d5eea7bef92afe0.jpg""><br>"	"<b>Semantic </b>segmentation  <div class=""details1"">Each pixel has its class labelized</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What kind of segmentation?<br><img src=""paste-1a7da523259a7f100a2ef31c8f2442057c82a8e8.jpg""><br>"	"<b>Instance </b>segmentation  <div class=""details1"">Not all pixels are labelized, but different objects of the same class are labelized differently</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>instance</b>&nbsp;segmentation"	"Not all pixels are labelized, but different objects of the same class are labelized differently  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-1a7da523259a7f100a2ef31c8f2442057c82a8e8.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>panoptic segmentation</b>?"	"All pixels are labelized, and different objects of the same classes are distinguished  <div class=""details1"">This is <u>semantic</u>&nbsp;+ <u>instance</u>&nbsp;segmentation</div>  <div class=""details2""></div>   <img src=""paste-6f75afb2e6a96a3a2472f618c034388fa257c69f.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What kind of segmentation?<br><img src=""paste-6f75afb2e6a96a3a2472f618c034388fa257c69f.jpg""><br>"	"<b>Panoptic </b>segmentation  <div class=""details1"">This is <u>semantic</u>&nbsp;+ <u>instance</u>&nbsp;segmentation</div>  <div class=""details2"">All pixels are labelized, and different objects of the same classes are distinguished</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does it mean if \(A \in \mathbb{R}^{m \times n}\) is <b>full rank</b>?"	"\(\text{Rank}(A) = \text{min}\,\{m, n\}\)  <div class=""details1"">All standard basis vectors are <u>independent</u></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does it mean if \(A \in \mathbb{R}^{m \times n}\) is&nbsp;<b>low rank</b>?"	"\(\text{Rank}(A) \lt \text{min}\,\{m, n\}\)  <div class=""details1""><u>Not all</u>&nbsp;standard basis vectors are&nbsp;<u>independent</u></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the common method name to compute a <b>matrix rank</b>?"	"Gaussian Elimination  <div class=""details1"">It finds the number of independent basis</div>  <div class=""details2""></div>   <img src=""paste-0d4ea5940954e64d418b291c36055ecaac5c53a2.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to find the <b>matrix rank </b>with <u>Gaussian Elimination</u>?"	"Reduce the matrix by doing <b>linear combination of its rows</b> (or columns).<div><br></div><div>The <b>number of non-zeros rows is the rank</b>.</div>  <div class=""details1"">The rank correspond to the number of independent basis</div>  <div class=""details2""></div>   <img src=""paste-0d4ea5940954e64d418b291c36055ecaac5c53a2.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to find a <b>matrix inverse </b>using <u>Gaussian Elimination</u>?"	"Append an <b>identity matrix</b>, then do the <u>Gaussian Elimination</u> until the left part of the matrix is an identity.<br><br>The right part will be the inverse.  <div class=""details1"">If the left part cannot be reduced to identity, then the matrix is not inversible.</div>  <div class=""details2""></div>   <img src=""paste-d6526b00fd42215e8d40bc20d0266658e7ae8549.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Relation between <b>matrix rank </b>and <b>singular values</b>?"	"Rank = number of non-zero singular values  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>LU factorization</b>?<br>"	"Decomposition of a matrix into a <b>lower triangular </b>matrix \(L\) and <b>upper triangular </b>matrix \(U\)  <div class=""details1""></div>  <div class=""details2""></div>   \(\left[\begin{array}{ccc}<br>a_{11} &amp; a_{12} &amp; a_{13} \\<br>a_{21} &amp; a_{22} &amp; a_{23} \\<br>a_{31} &amp; a_{32} &amp; a_{33}<br>\end{array}\right]=\left[\begin{array}{ccc}<br>l_{11} &amp; 0 &amp; 0 \\<br>l_{21} &amp; l_{22} &amp; 0 \\<br>l_{31} &amp; l_{32} &amp; l_{33}<br>\end{array}\right]\left[\begin{array}{ccc}<br>u_{11} &amp; u_{12} &amp; u_{13} \\<br>0 &amp; u_{22} &amp; u_{23} \\<br>0 &amp; 0 &amp; u_{33}<br>\end{array}\right]\)"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>[...]</span> \(=\) \(B^{-1} A^{-1}\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  <span class=cloze>\((AB)^{-1}\)</span> \(=\) \(B^{-1} A^{-1}\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \((AB)^{-1}\) \(=\) <span class=cloze>[...]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  \((AB)^{-1}\) \(=\) <span class=cloze>\(B^{-1} A^{-1}\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  With column vector \(x \in \mathbb{R}^n\),<div><br></div><div>\(x^T x\) is a <b>scalar</b>&nbsp;or <b>matrix</b>?</div>"	"Scalar  <div class=""details1"">\((1 \times n)(n \times 1)\) -&gt; \((1 \times 1)\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  With column vector \(x \in \mathbb{R}^n\),<div><br></div><div>\(x x^T\) is a <b>scalar</b>&nbsp;or <b>matrix</b>?</div>"	"Matrix  <div class=""details1"">\((n \times 1)(1 \times n)\) -&gt; \((n \times n)\)</div>  <div class=""details2""></div>   "
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A set of \(n\) vectors in \(\mathbb{R}^m\) are not <span class=cloze>[]</span><b>&nbsp;</b>if \(n \gt m\) "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A set of \(n\) vectors in \(\mathbb{R}^m\) are not <span class=cloze><b>linearly independent</b></span><b>&nbsp;</b>if \(n \gt m\)<br><br> <div style=""font-style: italic; font-size: 14px""></div>"
"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A set of \(n\) vectors in \(\mathbb{R}^m\) are not <b>linearly independent</b><b>&nbsp;</b>if <span class=cloze>[]</span> "	"<h5>🤖 AI</h5>  <div style=""text-decoration: underline""></div>  A set of \(n\) vectors in \(\mathbb{R}^m\) are not <b>linearly independent</b><b>&nbsp;</b>if <span class=cloze>\(n \gt m\)</span><br><br> <div style=""font-style: italic; font-size: 14px""></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>nullspace</b>&nbsp;of matrix \(A \in \mathbb{R}^{m \times n}\)?"	"All \(x\): \(A x = 0\)  <div class=""details1"">Dimension : \(n - r\) with \(r\) the matrix's rank</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>reservoir sampling</b>?"	"Sampling online data with bounded memory  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In which case do we use <b>reservoir sampling</b>?"	"In an <b>online </b>setting where we cannot review previous data  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How do we usually select a new sample in the <b>reservoir sampling</b>?"	"Which a probability \(\frac{m}{n}\) which \(m\) the total memory size and \(n\) the number of seen datapoints  <div class=""details1"">Thus the probability goes towards 0 thru time</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When selecting with <b>reservoir sampling</b>, we use the probability \(\frac{m}{n}\) which \(m\) the total memory size and \(n\) the number of seen datapoints.<br><br>What is the probability behavior?"	"Probability goes <b>towards 0</b> the more we see samples  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When can we see <b>gradient dominance</b>?"	"When a mini-batch is made of a <b>majority class</b>  <div class=""details1"">It is said <u>imbalanced</u></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the problem of <b>gradient dominance </b>in a mini-batch?"	"The mini-batch is highly <b>biased towards the majority class</b>  <div class=""details1"">Thus minority class in the long tail brings few information</div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <b>linear search</b>&nbsp;in an array?"	"Going from the <b>start</b>&nbsp;to the <b>end </b>passing by all elements  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-882b45eedd094828c9a16aef6eab56cad9e907b6.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the worst complexity of&nbsp;<b>linear search</b>&nbsp;in an array?"	"\(O(N)\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-882b45eedd094828c9a16aef6eab56cad9e907b6.jpg""> "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Relation between the <b>outer product </b>and the <b>matrix multiplication</b>?"	"\(\mathbf{u} \otimes \mathbf{v}=\mathbf{u} \mathbf{v}^{\top}\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-52609aabc308fe175ab31b7f1f28c45107afae6e.jpg"">"
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 2"	"4  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 3"	"6  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 4"	"8  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 5"	"10  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 6"	"12  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 7"	"14  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 8"	"16  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 9"	"18  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 2 x 10"	"20  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 2"	"6  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 3"	"9  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 4"	"12  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 5"	"15  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 6"	"18  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 7"	"21  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 8"	"24  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 9"	"27  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 3 x 10"	"30  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 2"	"8  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 3"	"12  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 4"	"16  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 5"	"20  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 6"	"24  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 7"	"28  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 8"	"32  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 9"	"36  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 4 x 10"	"40  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 2"	"10  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 3"	"15  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 4"	"20  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 5"	"25  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 6"	"30  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 7"	"35  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 8"	"40  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 9"	"45  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 5 x 10"	"50  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 2"	"12  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 3"	"18  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 4"	"24  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 5"	"30  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 6"	"36  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 7"	"42  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 8"	"48  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 9"	"54  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 6 x 10"	"60  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 2"	"14  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 3"	"21  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 4"	"28  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 5"	"35  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 6"	"42  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 7"	"49  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 8"	"56  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 9"	"63  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 7 x 10"	"70  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 2"	"16  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 3"	"24  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 4"	"32  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 5"	"40  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 6"	"48  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 7"	"56  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 8"	"64  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 9"	"72  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 8 x 10"	"80  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 2"	"18  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 3"	"27  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 4"	"36  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 5"	"45  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 6"	"54  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 7"	"63  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 8"	"72  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 9"	"81  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 9 x 10"	"90  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 2"	"20  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 3"	"30  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 4"	"40  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 5"	"50  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 6"	"60  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 7"	"70  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 8"	"80  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 9"	"90  <div class=""details1""></div>  <div class=""details2""></div>  "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> 10 x 10"	"100  <div class=""details1""></div>  <div class=""details2""></div>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Behavior of <u>bitwise</u><b>&nbsp;NOT</b>?<div><br></div><div><pre>NOT <b>0</b>111  (decimal 7)<br></pre></div>"	"<b>Inverse </b>0 &amp; 1&nbsp;  <br/><br/> <span style=""font-size: 12px""><pre>NOT <b>0</b>111  (decimal 7)   = <b>1</b>000  (decimal 8)</pre></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Behavior of <u>bitwise</u>&nbsp;<b>AND</b>?<div><br></div><div><pre>    010<b>1</b> (decimal 5) AND 001<b>1</b> (decimal 3)</pre></div>"	"1 if the two bits are 1  <br/><br/> <span style=""font-size: 12px""><pre>    010<b>1</b> (decimal 5) AND 001<b>1</b> (decimal 3)   = 000<b>1</b> (decimal 1)</pre></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Behavior of <u>bitwise</u>&nbsp;<b>OR</b>?<div><br></div><div><pre>   0<b>101</b> (decimal 5) OR 0<b>011</b> (decimal 3)</pre></div>"	"1 if at least one the bit is 1  <br/><br/> <span style=""font-size: 12px""><pre>   0<b>101</b> (decimal 5) OR 0<b>011</b> (decimal 3)  = 0<b>111</b> (decimal 7)</pre></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Behavior of <u>bitwise</u>&nbsp;<b>XOR</b>?<div><br></div><div><pre>    0<b>10</b>1 (decimal 5) XOR 0<b>01</b>1 (decimal 3)</pre></div>"	"1 if only one bit is 1  <br/><br/> <span style=""font-size: 12px""><pre>    0<b>10</b>1 (decimal 5) XOR 0<b>01</b>1 (decimal 3)   = 0<b>11</b>0 (decimal 6)</pre></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Bitwise <b>AND </b>in Python?"	"x &amp; y  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Bitwise <b>OR&nbsp;</b>in Python?"	"x | y  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Bitwise <b>XOR&nbsp;</b>in Python?"	"x ^ y  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is<b> x &amp; y </b>in Python?"	"Bitwise AND  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is<b> x | y </b>in Python?"	"Bitwise OR  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is<b> x ^ y </b>in Python?"	"Bitwise XOR  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is<b> x &lt;&lt; y </b>in Python?"	"<b>Shifting</b> the bits to the left by y places  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is<b> x &gt;&gt; y </b>in Python?"	"<b>Shifting</b> the bits to the right by y places  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the effect of <b>shifting</b> a number \(x\), \(y\) places to the <b>left</b>?"	"\(x \times 2^y\)  <br/><br/> <span style=""font-size: 12px"">x &lt;&lt; y in Python</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the effect of <b>shifting</b> a number \(x\), \(y\) places to the <b>right</b>?"	"\(x \div 2^y\)  <br/><br/> <span style=""font-size: 12px"">x &gt;&gt; y in Python</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How to multiply a number \(x\) by \(2^y\) with <b>bitwise operations</b>?"	"<b>Shifting</b> to the <b>left</b> by \(y\) places: x &lt;&lt; y  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How to divide a number \(x\) by \(2^y\) with <b>bitwise operations</b>?"	"<b>Shifting</b> to the <b>right</b> by \(y\) places: x &gt;&gt; y  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What does the <b>endianess</b>&nbsp;determine?"	"Whether the <b>most significant byte </b>is stored at the lowest or highest memory adress of a storage location  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-80b31df761c2b3c5a8a52682a98110ebd104bc28.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <b>Big-endian</b>?"	"The most significant byte is stored at the lowest memory adress  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-80b31df761c2b3c5a8a52682a98110ebd104bc28.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <b>Little-endian</b>?"	"The most significant byte is stored at the highest memory adress  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-80b31df761c2b3c5a8a52682a98110ebd104bc28.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> The quasi-ubiquitous&nbsp;x86 processors use <b>Big-endian </b>or <b>Little-endian</b>?"	"Little-endian  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-80b31df761c2b3c5a8a52682a98110ebd104bc28.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How to check if a number is&nbsp;<b>odd or even</b>&nbsp;with <b>bitwise operations</b>?"	"x AND 1  <br/><br/> <span style=""font-size: 12px"">It check it the least significant byte is 1, thus an odd number, thus equal to 1</span> <br/><br/> <img src=""paste-d53c8894761c62034a077834ab832830c53dcd33.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How to check if two numbers are of <b>opposite signs</b> with <b>bitwise operations</b>?"	"x XOR y<div><br></div><div>If the resulting number is <b>positive</b>&nbsp;then they are of the <b>same sign</b></div>  <br/><br/> <span style=""font-size: 12px"">The most significant byte encode the sign (1 negative, 0 positive)</span> <br/><br/> <img src=""paste-7a0008a12d368aac0241454f63bb804d2fb046cf.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How to swap two numbers with <b>bitwise operations</b>?"	"x = x ^ y<br>y = x ^ y<br>x = x ^ y  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-f266a98acbdb98c5dd203a6260ff7c11f7a12ba5.jpg""> "
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> What is the goal of computer vision by&nbsp;<b>inverse graphics</b>?"	"Analyze an image by attempting to <b>synthesize</b> it  <div class=""details1"">Popularized by <u>David Marr</u>&nbsp;in its book Vision (1982)</div>  <div class=""details2""></div>  <img src=""paste-87e6a23f0b81817c5078594fdecf3ba58f787e84.jpg"">"
"<!-- <h5>💡 Culture</h5> -->  <div class=""subtitle""></div> <br> How do we call the strategy where to <b>analyze</b> an image, we attempt to&nbsp;<b>synthesize</b>&nbsp;it"	"<b>Inverse graphics</b>  <div class=""details1"">Popularized by&nbsp;<u>David Marr</u>&nbsp;in its book Vision (1982)</div>  <div class=""details2""></div>  <img src=""paste-87e6a23f0b81817c5078594fdecf3ba58f787e84.jpg"">"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is a <b>Higher-order function</b>?"	"A function that does at least:<div><br><div>- takes functions as <b>argument</b></div></div><div>- <b>returns</b> a function</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div>What is a function that does at least:</div><div><div>- takes functions as&nbsp;<b>argument</b></div></div><div>-&nbsp;<b>returns</b>&nbsp;a function</div><div>?</div>"	"<b>Higher-order </b>function  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div>What is (<i>simply</i>) a&nbsp;<b>homoiconic</b> language?&nbsp;</div>"	"A language whose <b>internal </b>and <b>external </b>representation are identical  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div>Is <b>LISP </b>a <b>homoiconic</b> language?</div>"	"Yes  <br/><br/> <span style=""font-size: 12px"">The data type 'list' in LIPS also represents the executed code</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div>Is <b>Python&nbsp;</b>a <b>homoiconic</b> language?</div>"	"No  <br/><br/> <span style=""font-size: 12px"">The internal AST is vastly different from the Python's types</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div>What is a programming language where the <b>internal </b>and <b>external </b>representations are identical?</div>"	"A <b>homoiconic </b>language  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>Covariate shift</b>?"	"A <b>changement</b> of the input features <b>distribution</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-ca1c711d86ed677fcd2dbc0f8578d41ab514cde1.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the name of the <b>shift </b>where there is a&nbsp;<b>changement</b>&nbsp;of the input features&nbsp;<b>distribution</b>?</div>"	"<b>Covariate </b>shift  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-ca1c711d86ed677fcd2dbc0f8578d41ab514cde1.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the name of the <b>shift </b>where there is a&nbsp;<b>changement</b>&nbsp;of the output targets&nbsp;<b>distribution</b>?</div>"	"<b>Prior&nbsp;</b>shift  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-b01469e6562c40a9e3f405f418b4d6a41a8fb206.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is <b>Prior shift</b>?</div>"	"A&nbsp;<b>changement</b>&nbsp;of the output targets&nbsp;<b>distribution</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-b01469e6562c40a9e3f405f418b4d6a41a8fb206.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is <b>Concept shift</b>?</div>"	"A&nbsp;changement&nbsp;of the <b>dependence</b> between the features and the target  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-5d769e850815afc4597a58d6489ba42d97e7328f.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the name of the <b>shift </b>where there is a changement&nbsp;of the&nbsp;<b>dependence</b>&nbsp;between the features and the target?<br></div>"	"<b>Concept </b>shift  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-5d769e850815afc4597a58d6489ba42d97e7328f.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the acronyme meaning of <b>ERM </b>in learning theory?</div>"	"<b>E</b>mpirical <b>R</b>isk <b>M</b>inimization  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the difference between the <b>empirical risk</b>&nbsp;and the <b>true error</b>?</div>"	"The former is <b>limited to our dataset</b> while the latter represents the whole domain  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>When does the <b>empirical error </b>is equal to the <b>true error</b>?</div>"	"When the dataset size grows to \(+\infty\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>Statistical-based AIs (SVM, NN, ...) are <b>inductive </b>or <b>deductive </b>learners?</div>"	"<b>Inductives</b>&nbsp;because they learn rules from examples  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>Rules-based AIs are <b>inductive </b>or <b>deductive </b>learners?</div>"	"<b>Deductives</b>&nbsp;because predict from predefined rules  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When <b>overfitting</b>, how are the <b>empirical risk </b>and the <b>true error</b>?&nbsp;"	"The <b>risk is low</b>&nbsp;and the <b>error is high</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the&nbsp;<strong>realizability assumption</strong>?"	"That we can find a model in our hypothesis space so that the <b>true error is 0</b>  <div class=""details1"">Thus the <u>empirical risk</u>&nbsp;will also be 0</div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When trying to improve performance, what does going <b>high</b>&nbsp;mean?"	"Using a <b>more powerful machine</b>  <br/><br/> <span style=""font-size: 12px"">According to the Bane's rule (cf HackerNews comment)</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When trying to improve performance, what does going <b>wide</b>&nbsp;mean?"	"Using several machines in <b>parallel</b>  <br/><br/> <span style=""font-size: 12px"">According to the Bane's rule (cf HackerNews comment)</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When trying to improve performance, what does going <b>deep</b>&nbsp;mean?"	"<b>Optimizing code</b> by removing abstractions  <br/><br/> <span style=""font-size: 12px"">According to the Bane's rule (cf HackerNews comment)</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the <b>Bane's rule</b>?"	"You don't understand a distributed computing problem <b>until you can get it to fit on a single machine first.</b>  <br/><br/> <span style=""font-size: 12px"">According to the Bane's rule (cf HackerNews comment)</span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>set family</b>?"	"A <b>set of sets</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When can we say that a model \(f\) <b>shatters</b>&nbsp;a set of data points \((x_1, x_2, ..., x_n)\)?"	"If for some parameters \(\theta\), \(f\) can <b>predicts without errors all the labels</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>VC-dimension </b>of a binary classification model \(f\)?"	"The maximum number of points that \(f\) can <b>shatter</b>  <div class=""details1"">Shatter means classifying w/o errors</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the VC dimension of a <b>linear classifier </b>in 2d?"	"3  <div class=""details1"">By <u>Radon</u>'s theorem, in 2d a line cannot <u>shatter</u>&nbsp;four or more points</div>  <div class=""details2""></div>   <img src=""paste-ac70e5204ee3f070484ab09df5cad397f0be1915.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does this formula means?<div><br></div><div>\(m \geq \frac{\log (|\mathcal{H}| / \delta)}{\epsilon}\)</div>"	"We need at least \(m\) samples to get an error lower than \(\epsilon\) given the cardinality of the hypothesis space \(|\mathcal{H}|\) and the confidence \(\delta\).  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is \(m\) here?</div><div><br></div>\(m \geq \frac{\log (|\mathcal{H}| / \delta)}{\epsilon}\)"	"The number of training samples  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is \(|\mathcal{H}|\) here?</div><div><br></div>\(m \geq \frac{\log (|\mathcal{H}| / \delta)}{\epsilon}\)"	"The cardinality of the <b>hypothesis space</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is \(\epsilon\) here?</div><div><br></div>\(m \geq \frac{\log (|\mathcal{H}| / \delta)}{\epsilon}\)"	"The maximum value of the <b>true error</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is \(\delta\) here?</div><div><br></div>\(m \geq \frac{\log (|\mathcal{H}| / \delta)}{\epsilon}\)"	"The <b>confidence</b> we set as hyperparameter  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>hypothesis space</b>?<div></div>"	"All the considered <b>functions</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Who introduced the notion of <b>PAC-learnability</b>?"	"Leslie Valiant  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-f2a6cbc286906ee8320084733045954a3bdc7b3e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does the acronyme <b>PAC</b>&nbsp;means in learning theory?"	"<b>P</b>robably <b>A</b>pproximately <b>C</b>orrect  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does the ""<u>probably</u>"" means in&nbsp;<b>P</b>robably&nbsp;<b>A</b>pproximately&nbsp;<b>C</b>orrect (PAC) learning?"	"Our hypothesis will be have low generalization error with a <b>high probability </b>\(1 - \delta\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-cbf4b8006295d0e3a351be5c1dfbd786550f09dc.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does the ""<u>approximately</u>"" means in&nbsp;<b>P</b>robably&nbsp;<b>A</b>pproximately&nbsp;<b>C</b>orrect (PAC) learning?"	"Our hypothesis will be have<b> low generalization error</b> \(\text{err}(h) \le \epsilon\) with a high probability<b>&nbsp;</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-cbf4b8006295d0e3a351be5c1dfbd786550f09dc.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does it mean, that an algorithm \(A\)&nbsp;PAC-learns a class of function?"	"It will have a <b>low generalization error </b>with a <b>high probability</b>  <div class=""details1"">More precisely an hypothesis \(h\) will validate this among a set \(H\)</div>  <div class=""details2""></div>   <img src=""paste-cbf4b8006295d0e3a351be5c1dfbd786550f09dc.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is it mean, that an algorithm \(A\) <b>efficiently</b> PAC-learns a class of functions?"	"If it can run in <b>time polynomial </b>of the amount of data \(|S|\)  <div class=""details1"">i.e. it doesn't need to use all the data to train efficiently, akin to lowshot models</div>  <div class=""details2""></div>   <img src=""paste-cbf4b8006295d0e3a351be5c1dfbd786550f09dc.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does it mean, that an algorithm \(A\)<b>&nbsp;</b>that can PAC-learn a class of functions, is a <b>proper learner</b>?"	"The output function \(h\) always lies in \(H\)  <div class=""details1"">This means we can learn without <u>overfitting</u></div>  <div class=""details2""></div>   <img src=""paste-cbf4b8006295d0e3a351be5c1dfbd786550f09dc.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  If an algorithm \(A\) is not a <b>proper learner</b>, how is the <u>empirical risk</u>?"	"Null  <div class=""details1"">We are <u>overfitting</u></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  If an algorithm \(A\) is not a <b>proper learner</b>, how is the <u>true error</u>?"	"High  <div class=""details1"">We are <u>overfitting</u></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why do we want to limit our <b>hypothesis space</b>?"	"If we minimize errors over all functions, we will easily reach <b>null empirical risk</b>, but maybe with a <b>high true error</b>  <div class=""details1"">This is overfitting</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What happens when we minimize over all functions and not solely \(H\)?"	"<div>We may overfit</div>  <div class=""details1"">If we minimize errors over all functions, we will easily reach&nbsp;<b>null empirical risk</b>, but maybe with a&nbsp;<b>high true error</b></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Under which sampling assumption does <b>PAC learning </b>rely?"	"<div>That samples are <b>Independent and Identically Distributed </b>(iid)</div>  <div class=""details1"">Which is not true in some scenarios as <u>Continual Learning</u></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>sample selection bias</b>?"	"We don't sample examples in an&nbsp;<b>Independent and Identically Distributed</b> (iid)&nbsp;manner  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>non-stationnary environment</b>?"	"When the data <b>distribution changes through time</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the setting <b>Deep Mutual Learning</b>?"	"Training the <b>teacher &amp; students</b>&nbsp;together in the same time  <div class=""details1"">Both models benefit</div>  <div class=""details2"">Introduced by <i>Zhang et al. 2018b</i></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>Cross-Modal Transfer</b>?"	"Transfering knowledge between a <b>teacher &amp; student</b>&nbsp;each with different modality  <div class=""details1"">e.g. RGB to depth</div>  <div class=""details2""></div>   <img src=""paste-8d75486a7b6c5bcb0e901e90ac40f5d75adb1953.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What is a <b>prosaic AI</b>?"	"An AI we can build&nbsp;<b>without learning anything substantially new</b> about <b>intelligence</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  How do we call an AI we can build&nbsp;<b>without learning anything substantially new</b>&nbsp;about&nbsp;<b>intelligence</b>?"	"A <b>prosaic </b>AI  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What does the acronyme <b>IDA </b>means?"	"<b>I</b>terate<b>d</b> <b>A</b>mplification  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  Which organization push&nbsp;<b>Iterated&nbsp;Amplification</b> (<b>IDA)</b>?"	"OpenAI  <div class=""details1"">From&nbsp;Paul Christiano</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What is&nbsp;<b>Iterated&nbsp;Amplification</b> (<b>IDA)</b>?"	"A <b>research agenda</b> that address the AI safety problem  <div class=""details1"">It's from OpenAI's&nbsp;Paul Christiano</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What is <b>a</b><strong>n intent aligned </strong>agent?"	"An agent that does what we want it to do  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  <div>How do we call an agent that does what we want it to do?</div>"	"An <strong>intent aligned&nbsp;</strong>agent  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  <div>What are the two main goal of Iterated Amplification (<b>IDA)</b>?</div>"	"1. To have a powerful AI<br>2. To have a safe AI  <div class=""details1"">If the AI isn't good, it may do catastrophic mistakes</div>  <div class=""details2"">If the AI isn't safe, it may harm us</div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What is a <b>corrigible </b>agent?"	"An AI that we can fix or at least <b>shut down</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What is the term for an AI that we can fix or at least&nbsp;<b>shut down</b>?"	"A <b>corrigible </b>agent  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What is&nbsp;<b><em style="""">approval-directed</em> AI</b>?"	"An AI that only takes actions that the <b>user would probably approve</b>  <div class=""details1"">This also include actions the AI could hide</div>  <div class=""details2"">It allow us to achieve <u>corrigibility</u></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  Does an&nbsp;<b><em style="""">approval-directed</em> AI </b>optimize for short-term or long-term preferences of the user?"	"Short-term  <div class=""details1"">We say it's <u>act-based</u></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What is an <b>act-based </b>AI?"	"An AI that takes in account the <b>short-term </b>preferences of the user  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">AI Safety</div>  What is <b>short-term preferences </b>of an user when considering AI safety?"	"Things that the user would still endorse in the short-term  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the name of the field where a model is trained <b>collaboratively</b> on many <b>personal devices</b>?"	"Federated Learning  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is <b>Federated Learning</b>?</div>"	"Learning a model <b>collaboratively </b>on many <b>personal devices</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>In&nbsp;<b>Federated Learning</b>, where is the data stored?</div>"	"On the&nbsp;<b>personal devices</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the main bottleneck that lead Google to develop <b>federated learning</b>?</div>"	"The network <b>bandwidth</b>  <div class=""details1"">Uploading data is very costly</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What kind of uncertainty does the <b>Monte Carlo Dropout </b>detect?"	"<b>Epistemic</b> uncertainty  <div class=""details1"">Uncertainty linked to the model bias</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What kind of uncertainty does the <b>MAP inference </b>by <b>predicting the variance&nbsp;</b>detect?"	"<b>Aleatoric</b> uncertainty  <div class=""details1"">Uncertainty linked to the data bias</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What kind of uncertainty is linked to the <b>data bias</b>?"	"<b>Aleatoric</b> uncertainty  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What kind of uncertainty is linked to the <b>model bias</b>?"	"<b>Epistemic</b> uncertainty  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>random forest</b>?"	"An ensembling of many <b>randomized decision trees</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In <u>random forest</u>, what is <b>bagging</b>?"	"Learning each <u>decision tree</u>&nbsp;on a <b>different data subset</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In <u>random forest</u>, what is <b>features randomness</b>?"	"Learning each <u>decision tree</u>&nbsp;on a <b>different subset of the features</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In <u>random forest</u>, what are two strategies to ensure <b>diversity</b>&nbsp;among decision trees?"	"<b>Bagging </b>and <b>features randomness</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How do we call in <u>random forest</u>&nbsp;the strategy of learning each&nbsp;<u>decision tree</u>&nbsp;on a&nbsp;<b>different data subset</b>?"	"<b>Bagging</b>  <div class=""details1"">Also bootstrap aggragation</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How do we merge the trees prediction in a <b>random forest</b>?"	"<b>Average </b>of likelihoods or <b>voting</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>Breiman-RF </b>strategy in random forest?"	"Producing diversity among trees with <b>bagging </b>and <b>features randomness</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does the <b>impurity </b>of a leaf mean in a decision tree?"	"How much <b>different classes are mixed</b>  <div class=""details1"">We wish to minimize it so classes are well discriminated</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the most common <b>impurity </b>metric in decision tree?"	"<b>Gini</b>&nbsp;impurity  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>Gini </b>impurity of a leaf?"	"The <b>probability</b> that two <b>randomly</b> selected training examples from the leaf have <b>different results</b>  <div class=""details1""><img src=""paste-f9011204f7908e57031e22a6f9d8d2cc607fedfa.jpg""><div><img src=""paste-e71e0274605fc7150519cb80936e92c864f1558b.jpg""><br></div></div>  <div class=""details2""></div>   <div><img src=""paste-09ba5162be5d5dd639cc0516c5db7393ee27deb6.jpg""><img src=""paste-66a87f9dab9cefaa9ed20c00be4a2866bb2f8326.jpg""><img src=""paste-6f7695505bcf10eb25ef82ca52e2f2f9cffa5bf5.jpg""><br></div><div><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Each split of a decision tree decreases or increases the <b>impurity</b>?"	"Decrease  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>Gini </b>impurity of a leaf for a <b>continuous </b>variable?"	"The <b>Mean Square Error </b>with the mean value of the leaf  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When can we stop growing the <b>decision tree</b>?"	"When the <b>impurity</b> is null, or if a defined <b>maximum depth </b>is reached  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What happens if we don't bound the depth of a <b>decision tree</b>?"	"We may <b>overfit</b>  <div class=""details1"">The decision tree grows until it can scatter the whole dataset</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What attribute of a <b>decision tree</b>&nbsp;is linked to its <b>VC dimension</b>?"	"Its <b>depth</b>  <div class=""details1"">With maximum depth, it can scatter the whole training set</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the effect of the <u>minimum number of samples per leaf</u> in a <b>decision tree</b>&nbsp;on <b>overfitting</b>?"	"The lower the minimum, the more overfitting  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why does <b>random forest</b>&nbsp;reduce overfitting &amp; variance?"	"Because it ensembles several <b>different</b> <b>decision trees</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When choosing a <u>random subset of the features</u> for each decision tree of the <b>random forest</b>, how to choose the number of features?&nbsp;"	"<img src=""paste-5d8a832f75a00886c91cf18d12e7b70724ccb509.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  To what <b>random forest </b>is very robust?"	"overfitting  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the idea of <b>gradient boosting</b>?"	"Using consecutive <b>weak learners</b>, each <b>correcting the errors</b> of the previous ones  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the initial model in <b>gradient boosting</b>?"	"A very simple model  <div class=""details1"">e.g. for a regression we could simply take the mean value</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the loss of a weak learner (not the first) in <b>gradient boosting</b>?"	"It tries to minimizes the <b>residual </b>between the ground-truth and the predictions of the previous ensemble&nbsp;  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>residual </b>in <b>gradient boosting</b>?"	"The difference between the ground-truth and the model output  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>gradient boosted decision trees</b>?"	"Gradient boosting where the weak learners are decision trees  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>learning rate</b>&nbsp;in <b>gradient boosting</b>?"	"We only takes a <b>fraction of the error correction</b>&nbsp;done by&nbsp;the following weak learner  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-5f27978f6374e50d2b7a305ddd23f79eb945bf65.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What happens if we reduce the <b>learning rate </b>in <b>gradient boosting</b>?"	"We reduce <b>overfitting</b>  <div class=""details1"">However too low and we are stuck with the initial very bad model</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What happens if we increase the <b>number of weak learners&nbsp;</b>in <b>gradient boosting</b>?"	"We reduce <b>bias </b>but we increase <b>variance </b>(and thus overfitting)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why is it called <b>gradient boosting</b>?"	"Because each weak learner models the <b>residual </b>aka the <b>gradient</b> between the ground-truth and the previous predicitions  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does a <b>weak learner </b>predict in <b>gradient boosting</b>?"	"The difference (<b>error</b>) between the ground-truth and the previous predictions  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to get a discrete class prediction in <b>decision trees</b>?"	"Given an arrival leaf, take the <b>most common class</b> that belonged to this leaf during training&nbsp;  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to get a probability &nbsp;class prediction in <b>decision trees</b>?"	"Given an arrival leaf, give the <b>proportion of the most common class</b> that belonged to this leaf during training compared to other classes also in the same leaf  <div class=""details1"">If a leaf has 6 blues and 4 reds, then the probability is 60%</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does the acronyme <b>LUPI </b>mean?"	"<b>L</b>earning <b>U</b>nder <b>P</b>rivileged <b>I</b>nformation  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>privileged information</b>?"	"Extra info about a sample only available in training  <div class=""details1"">Could be attributes, localization, etc.</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Can&nbsp;<b>privileged information </b>be available in training?"	"Yes  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Can&nbsp;<b>privileged information </b>be available in testing?"	"No  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>marginalization</b>?"	"Computing a marginal probability by calculating the <b>subset of a larger probability distribution</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-b571a8eb13785faa506357351a4198aa4563f6d7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What probability rule does&nbsp;<b>marginalization </b>involve?"	"The <b>sum </b>rule  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-b571a8eb13785faa506357351a4198aa4563f6d7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why <b>marginalization </b>is often complicated for NNs?"	"Because we would need to know <b>all possible weights</b>, thus we only sample a limited amount&nbsp;  <div class=""details1"">MC Dropout can approximate it</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When is a classifier perfectly <b>calibrated</b>?"	"When the confidence correspond to the actual accuracy  <div class=""details1"">\(\mathbb{P}(\hat{Y}=Y \mid \hat{P}=p)=p\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>power set </b>of \(\{x, y, z\}\)?"	"\(\{\{\},\{x\},\{y\},\{z\},\{x, y\},\{x, z\},\{y, z\},\{x, y, z\}\}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is \(\{\{\},\{x\},\{y\},\{z\},\{x, y\},\{x, z\},\{y, z\},\{x, y, z\}\}\)?</div>"	"The&nbsp;<b>power set&nbsp;</b>of \(\{x, y, z\}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What does <b>conformal prediction</b>&nbsp;produce?</div>"	"A set of classes with an associated confidence  <div class=""details1"">The set can be any set of the <u>power set</u></div>  <div class=""details2"">Ideally we want to have a set of one class with high confidence</div>   <img src=""paste-e70d63f41be991eb1503ea83854b2452c11d8ca5.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What does it mean if a <b>conformal prediction</b>&nbsp;produce a set of several classes?</div>"	"There is <b>uncertainty</b>&nbsp;in which class is the correct one  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-e70d63f41be991eb1503ea83854b2452c11d8ca5.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>With what kind of model can we use <b>conformal prediction</b>?</div>"	"Every kind of model  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-e70d63f41be991eb1503ea83854b2452c11d8ca5.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is a <b>non-conformity measure</b>?</div>"	"A function capturing the <b>strangeness </b>of the label \(y\) to the data \(x\)&nbsp;  <div class=""details1""></div>  <div class=""details2"">Some examples:</div>   <img src=""paste-0a87f4e6bf5bf58c9c7730143b25424426ab5721.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>How do <b>conformal prediction</b>&nbsp;choose which classes to place in the set prediction?</div>"	"All classes whose <b>non-conformity </b>measure has a <b>p-value </b>lower than some threshold  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-86a1a7af42f0a153e81e519c0deba7d0defb9e7e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What does the <b>credibility </b>mean in conformal prediction?</div>"	"How likely the sample is to come from the train set  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What does the <b>confidence&nbsp;</b>mean in conformal prediction?</div>"	"How certain the model is that the prediction is a <b>singleton</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  On what is based <b>credibility </b>for conformal prediction?"	"On the <b>p-value </b>of the <b>non-conformity </b>measure  <div class=""details1"">A low p-value means the sample is non-conforme</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to get the <b>credibility </b>based on the p-value of non-conformity measure?"	"Take the <b>highest p-value</b>&nbsp;such as the prediction is an empty set  <div class=""details1"">The higher it is, the less probable the non-conformity measure found something</div>  <div class=""details2""></div>   <img src=""paste-95954f4e6dd7e6cb081dbf970aa4e819aa413ce4.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to get the <b>confidence&nbsp;</b>based on the p-value of non-conformity measure?"	"The complement to 1 of the highest p-value from a singleton  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-9201daefc49ed74122c002045ac221abbf18e2f7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In most <b>uncertainty </b>methods, what do we do first? Uncertainty estimation or predictions?&nbsp;"	"Predictions  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In <b>conformal predictions</b>, what do we do first? Uncertainty estimation or predictions?&nbsp;"	"Uncertainty  <div class=""details1"">We first set the level of acceptable uncertainty</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the inverse of a <b>unitary </b>matrix \(U\)?"	"Its <b>conjugate transpose </b>\(U^{\dagger}\)  <div class=""details1"">\(U^{\dagger} U=U U^{\dagger}=I\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Given \(U\) a unitary matrix, \(U^{*} U=\)...?"	"\(U^{*} U=I\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Given \(U\) a unitary matrix, \(U U^{*}=\)...?"	"\(U U^{*}=I\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>real </b>analogue of <b>unitary </b>matrix?"	"<b>Orthogonal </b>matrix  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>complex&nbsp;</b>analogue of <b>orthogonal&nbsp;</b>matrix?"	"<b>Unitary&nbsp;</b>matrix  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does it means that a matrix \(M\) is <b>diagonalizable</b>&nbsp;by an <b>unitary </b>matrix?"	"\(M=U D U^{\dagger}\)  <div class=""details1"">With \(D\) a diagonal matrix</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does \(*\) denotes for a matrix \(A^*\)?"	"The <b>complex conjugate</b>  <div class=""details1""></div>  <div class=""details2""></div>   \(\boldsymbol{A}=\left[\begin{array}{cc}<br>1 &amp; 1+i \\<br>-2-i &amp; i \\<br>5 &amp; 4-2 i<br>\end{array}\right]\)<div><br></div><div>\(\boldsymbol{A}^{*}=\left[\begin{array}{cc}</div>1 &amp; 1-i \\<br>-2+i &amp; -i \\<br>5 &amp; 4+2 i<br>\end{array}\right]\)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>complex conjugate </b>of a matrix?"	"Inverse the sign of <b>imaginary parts</b>  <div class=""details1"">\(a + ib \rightarrow a - ib\)</div>  <div class=""details2""></div>   \(\boldsymbol{A}=\left[\begin{array}{cc}<br>1 &amp; 1+i \\<br>-2-i &amp; i \\<br>5 &amp; 4-2 i<br>\end{array}\right]\)<div><br></div><div>\(\boldsymbol{A}^{*}=\left[\begin{array}{cc}</div>1 &amp; 1-i \\<br>-2+i &amp; -i \\<br>5 &amp; 4+2 i<br>\end{array}\right]\)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What symbol do we use to denote <b>complex conjugate</b>?"	"\(A^*\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What symbol do we use to denote <b>transpose</b>?"	"\(A^T\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What symbol do we use to denote <b>conjugate transpose</b>?"	"\(A^\dagger\)  <div class=""details1"">Sometimes also \(A^*\)</div>  <div class=""details2"">Named <u>dagger</u></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the&nbsp;<b>conjugate transpose </b>of a matrix?"	"The <b>tranpose </b>of the <b>complex conjugate</b>  <div class=""details1"">\(A^\dagger = (A^*)^T\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In \(M=U D U^{\dagger}\), what kind of matrix is \(U\)?"	"A <b>unitary </b>matrix  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In \(M=U D U^{\dagger}\), what kind of matrix is \(U^\dagger\)?"	"The <b>complex transpose </b>of an&nbsp;<b>unitary </b>matrix  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In \(M=U D U^{\dagger}\), what kind of matrix is \(D\)?"	"A <b>diagonal </b>matrix  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>normal </b>matrix?"	"A matrix that <b>commutes </b>with its <b>conjugate transpose</b>  <div class=""details1"">\(A^{\dagger} A=A A^{\dagger}\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How do we call a matrix that&nbsp;<b>commutes&nbsp;</b>with its&nbsp;<b>conjugate transpose</b>, like \(A^{\dagger} A=A A^{\dagger}\)?"	"A <b>normal </b>matrix  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  A <b>complex normal </b>matrix is always diagonalizable by what?"	"An&nbsp;<b>unitary </b>matrix  <div class=""details1"">\(M=U D U^{\dagger}\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does \(M=U D U^{\dagger}\) mean?"	"\(M\) is <b>diagonalizable</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does \(M M^{\dagger}=M^{\dagger} M\) mean about the length of the \(j^{th}\) row and \(j^{th}\) column"	"They are of the same <b>length</b>  <div class=""details1"">\(M M^{\dagger}=\left[\begin{array}{cc}<br>r r^{\dagger} &amp; \cdots \\<br>\cdots &amp; \cdots<br>\end{array}\right]\)</div>  <div class=""details2"">\(M^{\dagger} M=\left[\begin{array}{cc}<br>c^{\dagger} c &amp; \cdots \\<br>\cdots &amp; \cdots<br>\end{array}\right]\)</div>   "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Two Sum</div>  Solve <b>TwoSum </b>problem:<div><br></div><div>Find indexes of two different numbers adding up to a third number.</div>  <br/> <div class=""example"">Given nums = [2, 7, 11, 15], target = 9,<div>Because nums[<strong>0</strong>] + nums[<strong>1</strong>] = 2 + 7 = 9,</div><div>return [<strong>0</strong>, <strong>1</strong>].<br></div></div> <br/> Which data structure? "	"Hashmap  <br/> <div class=""algo""><center></center><div><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">twoSum</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>], target: <span style=""color: #008000"">int</span>):         hashmap <span style=""color: #666666"">=</span> {}         <span style=""color: #008000; font-weight: bold"">for</span> i, n <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">enumerate</span>(nums):             complement <span style=""color: #666666"">=</span> target <span style=""color: #666666"">-</span> n             j <span style=""color: #666666"">=</span> hashmap<span style=""color: #666666"">.</span>get(complement)             <span style=""color: #008000; font-weight: bold"">if</span> j <span style=""color: #AA22FF; font-weight: bold"">is</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> <span style=""color: #008000; font-weight: bold"">None</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> i <span style=""color: #666666"">!=</span> j:                 <span style=""color: #008000; font-weight: bold"">return</span> [i, j]             hashmap[n] <span style=""color: #666666"">=</span> i </pre></div> </td></tr></tbody></table></center></div><div><br></div></div> "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Contains Duplicates</div>  <b>Check if an array contains duplicates.</b><div><br></div><div>Search for the fastest algorithm.</div>  <br/> <div class=""example""><strong>Input:</strong>&nbsp;[1,2,3,1]&nbsp;<strong>Output:</strong>&nbsp;true<br><div><strong>Input:&nbsp;</strong>[1,2,3,4]&nbsp;<strong>Output:</strong>&nbsp;false</div></div> <br/> Which data structure? "	"Set  <br/> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">containsDuplicate</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         s <span style=""color: #666666"">=</span> <span style=""color: #008000"">set</span>()         <span style=""color: #008000; font-weight: bold"">for</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> nums:             <span style=""color: #008000; font-weight: bold"">if</span> n <span style=""color: #AA22FF; font-weight: bold"">in</span> s:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>             s<span style=""color: #666666"">.</span>add(n)         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center></div> "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div>  <div>Given two strings <em>s</em> and <em>t</em>, write a function to determine if <em>t</em> is an anagram of <em>s</em>.</div><div><br></div><div>Fastest algorithm.</div>  <br/> <div class=""example""><b>Input:</b> <em>s</em> = ""anagram"", <em>t</em> = ""nagaram"" <b>Output:</b> true<br><div><b>Input:</b> <em>s</em> = ""rat"", <em>t</em> = ""car"" <b>Output: </b>false<br></div></div> <br/> Which data structure? "	"Hashmap  <br/> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isAnagram</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>, t: <span style=""color: #008000"">str</span>):         <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #008000"">len</span>(s) <span style=""color: #666666"">!=</span> <span style=""color: #008000"">len</span>(t):             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>         hashmap <span style=""color: #666666"">=</span> collections<span style=""color: #666666"">.</span>defaultdict(<span style=""color: #008000"">int</span>)         <span style=""color: #008000; font-weight: bold"">for</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> s:             hashmap[ss] <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">for</span> tt <span style=""color: #AA22FF; font-weight: bold"">in</span> t:             <span style=""color: #008000; font-weight: bold"">if</span> hashmap<span style=""color: #666666"">.</span>get(tt, <span style=""color: #666666"">0</span>) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>             hashmap[tt] <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span> </pre></div> </td></tr></tbody></table></center></div> "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Valid Anagram</div>  Given a string containing just the characters '(', ')', '{', '}', '[' and ']', determine if the input string is valid.  <br/> <div class=""example""><strong>Input:</strong> ""()[]{}"" <strong>Output:</strong> true<br><div><strong>Input:</strong> ""(]"" <strong>Output:</strong> false<br></div></div> <br/> Which data structure? "	"Stack  <br/> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">isValid</span>(<span style=""color: #008000"">self</span>, s: <span style=""color: #008000"">str</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">bool</span>:         stack <span style=""color: #666666"">=</span> []         <span style=""color: #008000; font-weight: bold"">for</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> s:             <span style=""color: #008000; font-weight: bold"">if</span> ss <span style=""color: #AA22FF; font-weight: bold"">in</span> (<span style=""color: #BA2121"">""(""</span>, <span style=""color: #BA2121"">""{""</span>, <span style=""color: #BA2121"">""[""</span>):                 stack<span style=""color: #666666"">.</span>append(ss)             <span style=""color: #008000; font-weight: bold"">elif</span> <span style=""color: #008000"">len</span>(stack) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>:                 <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 elt <span style=""color: #666666"">=</span> stack<span style=""color: #666666"">.</span>pop()                 <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #AA22FF; font-weight: bold"">not</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>is_pair(elt, ss):                     <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000"">len</span>(stack) <span style=""color: #666666"">==</span> <span style=""color: #666666"">0</span>              <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">is_pair</span>(<span style=""color: #008000"">self</span>, opener, ender):         <span style=""color: #008000; font-weight: bold"">if</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""(""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">"")""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">elif</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""[""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""]""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">elif</span> opener <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""{""</span> <span style=""color: #AA22FF; font-weight: bold"">and</span> ender <span style=""color: #666666"">==</span> <span style=""color: #BA2121"">""}""</span>:             <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">True</span>         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000; font-weight: bold"">False</span> </pre></div> </td></tr></tbody></table></center></div> "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Product of Array Except Self</div></div>  Given an array nums of <em>n</em> integers where <em>n</em> &gt; 1, &nbsp;return an array output such that output[i] is equal to the product of all the elements of nums except nums[i].<br><div><br></div><div>Do it without division, and with space complexity \(O(n)\).</div>  <br/> <div class=""example""><b>Input:</b> [1,2,3,4]&nbsp;<div><b>Output:</b> [24,12,8,6]</div></div> <br/> Which data structure? "	"Array  <br/> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">productExceptSelf</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         left <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         right <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             left[i] <span style=""color: #666666"">=</span> left[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">2</span>, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):                 right[i] <span style=""color: #666666"">=</span> right[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums)):             nums[i] <span style=""color: #666666"">=</span> left[i] <span style=""color: #666666"">*</span> right[i]         <span style=""color: #008000; font-weight: bold"">return</span> nums </pre></div> </td></tr></tbody></table></center></div> "
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""><div>Product of Array Except Self</div></div>  Given an array nums of&nbsp;<em>n</em>&nbsp;integers where&nbsp;<em>n</em>&nbsp;&gt; 1, &nbsp;return an array output such that output[i] is equal to the product of all the elements of nums except nums[i].<br><div><br></div><div>Do it without division, and with space complexity \(O(1)\) (excluding output array).</div>  <br/> <div class=""example""><b>Input:</b> [1,2,3,4]<div><b>Output:</b> [24,12,8,6]</div></div> <br/> Which data structure? "	"Array  <br/> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">productExceptSelf</span>(<span style=""color: #008000"">self</span>, nums: List[<span style=""color: #008000"">int</span>]):         left <span style=""color: #666666"">=</span> [<span style=""color: #666666"">1</span> <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums))]         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             left[i] <span style=""color: #666666"">=</span> left[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>] <span style=""color: #666666"">*</span> nums[i <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>]          tmp <span style=""color: #666666"">=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):               left[i] <span style=""color: #666666"">=</span> left[i] <span style=""color: #666666"">*</span> tmp             tmp <span style=""color: #666666"">*=</span> nums[i]          <span style=""color: #008000; font-weight: bold"">return</span> left </pre></div> </td></tr></tbody></table></center></div> "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is a <b>state</b>?"	"A complete description of the world at some instant  <div class=""details1"">Nothing is hidden</div>  <div class=""details2""></div>   <img src=""paste-7c6426ddd2aa70485a8df0974f5318e7fb17eede.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is an&nbsp;<b>observation</b>?"	"A <b>partial </b>description of the world  <div class=""details1"">Some parts are hidden</div>  <div class=""details2""></div>   <img src=""paste-130be14f0ee8e9211f94586c25c96e592fcbf457.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is an&nbsp;<b>action space</b>?"	"The set of <b>all possible actions </b>in an environment  <div class=""details1""></div>  <div class=""details2"">In super mario, we have four directions &amp; jump</div>   <img src=""paste-130be14f0ee8e9211f94586c25c96e592fcbf457.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  The <b>discount rate</b> diminish values of what?"	"Rewards far in the future  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-04b0783b4bbd6121d81cfd84355208a54531a87e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why do we discount rewards far in the future?"	"Because they are less probable  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-04b0783b4bbd6121d81cfd84355208a54531a87e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the range of values for the <b>discount rate </b>\(\gamma\)?"	"Between 0 and 1  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-04b0783b4bbd6121d81cfd84355208a54531a87e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How does the <b>discount rate </b>is applied for a reward \(n\) steps in the future?"	"The discount rate is taken to the power of \(n\): \(\gamma^n\)&nbsp;  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-04b0783b4bbd6121d81cfd84355208a54531a87e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is an <b>episodic </b>task?"	"A task that has an <b>ending</b>  <div class=""details1"">A super mario party</div>  <div class=""details2""></div>   <img src=""paste-130be14f0ee8e9211f94586c25c96e592fcbf457.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is an <b>continuous&nbsp;</b>task?"	"A task without ending  <div class=""details1"">Predicting the market</div>  <div class=""details2""></div>   <img src=""paste-6065ed445771f37b1eb1bda0266863184bed42e3.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the goal of the <b>exploration </b>phase?"	"To find <b>more information </b>about the environment  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the goal of the <b>exploitation&nbsp;</b>phase?"	"To use the known information to increase <b>rewards</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How can we do an <b>exploration phase</b>?"	"By taking <b>random </b>actions  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does an agent try to maximize?"	"The expected <b>cumulative </b>rewards  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-04b0783b4bbd6121d81cfd84355208a54531a87e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is a <b>policy</b>?"	"A function that givens a <b>state</b>&nbsp;returns an <b>action</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-87fa983289badc21c55c14c3308522e62abac4f2.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the commonly used letter for <b>policy</b>?"	"\(\pi\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-87fa983289badc21c55c14c3308522e62abac4f2.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the commonly used letter for <b>discount rate</b>?"	"\(\gamma\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-04b0783b4bbd6121d81cfd84355208a54531a87e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does \(\pi\) denote?"	"A <b>policy</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-87fa983289badc21c55c14c3308522e62abac4f2.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does \(\pi^*\) denote?"	"The <b>optimal </b>policy  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-87fa983289badc21c55c14c3308522e62abac4f2.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does <b>Policy-based methods </b>learn?"	"Learn which <b>actions </b>to take given a <b>state</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-faf063748a950da94b1261857313e917e0173813.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How do we call the strategy to learn which&nbsp;<b>actions&nbsp;</b>to take given a&nbsp;<b>state</b>?"	"<b>Policy-based </b>methods  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-faf063748a950da94b1261857313e917e0173813.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is <b>deterministic </b>policy?"	"Given a state, always returns the same <b>action</b>  <div class=""details1""></div>  <div class=""details2""><img src=""paste-4d0fdd69054d829eea115743259e058dd06d2715.jpg""></div>   <img src=""paste-2db93ff690e0da7bf6cac1ae484ecce6caaf2e61.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is <b>stochastic&nbsp;</b>policy?"	"Given a state, returns a probability distribution over several actions  <div class=""details1""></div>  <div class=""details2""><img src=""paste-0d5b6577ca13a7aca837a25b92e5c6559182ff62.jpg""></div>   <img src=""paste-8e0c482cd1cfb6c86ceb7ad8f8f12b10d3372e45.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does <b>Value-based methods </b>learn?"	"Learn which <b>state </b>is valuable  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-1c2e7ab795dae6bbed8fc5260600d65e8188802b.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How do we call the strategy to learn which&nbsp;<b>state </b>is valuable?"	"<b>Value-based </b>methods  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-1c2e7ab795dae6bbed8fc5260600d65e8188802b.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the <b>value of a state</b> in value-based methods?"	"The expected <b>discounted return</b>&nbsp;given our starting state and the action the policy will take  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-9b0ef1d0a96cf44592637087d0210f8586c8cd13.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In Value-based methods, what action does our policy choose?"	"The action leading to the state with <b>highest value</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-1c2e7ab795dae6bbed8fc5260600d65e8188802b.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the difference between <b>Q-learning </b>and <b>Deep-Q-learning</b>?"	"The former uses a q-<b>table</b>, while the latter learns to<b> approximate</b> the q value  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-d5688fc5e2be0935b01c4bdc764f3775082ce7b0.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the <b>reward hypothesis</b>?"	"All goals can be described as the <b>maximization of the expected cumulative reward</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the name of the hypothesis that states:<div><br></div><div>""All goals can be described as the&nbsp;<b>maximization of the expected cumulative reward</b>""<br></div>"	"Reward hypothesis  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Do we need to specify the behavior in <b>Policy-based </b>or <b>Value-based </b>methods?"	"In <b>Value-based </b>methods  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How do we call a <b>policy </b>that always take the actions leading to the <b>biggest value</b>?"	"<b>Greedy </b>policy  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-b9bd5427c6902ebe7da790de2e98cfe09adcc45b.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>Value</b>-based methods, to have an <b>optimal policy </b>function, what other function needs to be optimal?"	"The <b>value</b>&nbsp;function  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-c20a16ebe732c4acaf7cf08e4244fbaaf6434233.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the problem with <b>greedy policy</b>?"	"It doesn't <b>explore </b>much  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is a <b>greedy policy</b>?"	"Policy that always chooses the action that <b>maximizes value</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-b9bd5427c6902ebe7da790de2e98cfe09adcc45b.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>value</b>-based methods, what does the <b>value of a state&nbsp;</b>represent?"	"The <b>expected return </b>for all future timesteps, starting from a particular state  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-91a6712bb532aaff799eaae1d324357b1d9b832c.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>value</b>-based methods, theorically, how many timesteps in the future do we use to estimate the <b>state value</b>?"	"All future timesteps  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-91a6712bb532aaff799eaae1d324357b1d9b832c.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is an <b>action-value </b>function?"	"Function estimating the <b>value </b>given a pair of (state, action)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-151f70c7dee7db85ae9f0d39f3c729471d0a6d3d.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How does <b>action-value </b>function differ from <b>value-state</b>&nbsp;function?"	"Action-value also takes in account the <b>action</b>  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-0190cf5b7c9b65c2bc629efbdb70387d7763099d.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In the <b>Bellman </b>equation, what are the two parts of the <b>reward</b>?"	"The <b>immediate </b>reward, and the discounted values of <b>future </b>rewards  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-2e9cd7f0adad8a78a8d667c8ce2041fdf5ddaba8.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Is the <b>Bellman </b>equation iterative or recursive?"	"Recursive  <div class=""details1"">We call the value of the following state, which itself call the next value, etc.</div>  <div class=""details2""></div>   <div><img src=""paste-2e9cd7f0adad8a78a8d667c8ce2041fdf5ddaba8.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Where is the <b>recursion </b>in the <b>Bellman </b>equation?"	"When fetching the discounted <b>value of the future state</b>  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-2e9cd7f0adad8a78a8d667c8ce2041fdf5ddaba8.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why can we say <b>Bellman </b>recursion is more efficient than compute the value of all states?"	"Because we could <b>cache</b> previously computed <b>state values</b>  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-2e9cd7f0adad8a78a8d667c8ce2041fdf5ddaba8.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  With <b>Monte-Carlo</b>, when do we learn?"	"A the <b>episode end</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  With <b>Monte-Carlo</b>, how do we update the value of a state?"	"By adding a fraction of the <b>residual </b>(<i>difference between actual return and predicted return</i>)<b>&nbsp;</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-28a2b1a90fc8b49b19b3e8071c97950b4b3c80e1.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>Monte-Carlo</b>, what is the <b>residual </b>used to update the value state?"	"Difference between <b>actual</b> return and <b>predicted</b> return<b>&nbsp;</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-28a2b1a90fc8b49b19b3e8071c97950b4b3c80e1.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the role of the <b>learning rate </b>in the <b>Monte-Carlo </b>update rule?"	"How much update our state value estimation based on our previous error  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-28a2b1a90fc8b49b19b3e8071c97950b4b3c80e1.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the <b>Monte-Carlo </b>update rule formula?"	"<img src=""paste-28a2b1a90fc8b49b19b3e8071c97950b4b3c80e1.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  With no discount, what is the value of the return at timestep t \(G_t\)?<div><img src=""paste-486a0c0acc3676e296adad541d99c67748678986.jpg""><br></div>"	"3  <div class=""details1"">It's the number of cheese we eat in an episode of 10 steps</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  <div>What can be the initial value of a <b>state</b>&nbsp;before any learning?</div>"	"0  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  With <b>Temporal-Difference</b>, when do we learn?"	"After each <b>timestep</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  When can we have the true value of \(G_t\) in <b>Monte-Carlo</b>?"	"Only once an episode is finished  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What update learning rule learns once an <b>episode is finished</b>?"	"Monte-Carlo  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What update learning rule learns after <b>each timestep</b>?"	"Temporal-Difference  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why can't we have access to \(G_t\) in <b>temporal-difference</b>?"	"Because we update each timestemps and \(G_t\) is only available after a whole episode  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Do we have access to \(G_t\) in <b>Monte-Carlo</b>?"	"Yes  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Do we have access to \(G_t\) in <b>Temporal-Difference</b>?"	"No  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How do we estimate \(G_t\) in <b>Temporal-Difference</b>?"	"It's the <b>next step reward</b> + discounted <b>next state value</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-51ef446a5bf140993112dde543a0a89e60b6b38f.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the <b>Temporal-Difference </b>update rule?"	"<img src=""paste-51ef446a5bf140993112dde543a0a89e60b6b38f.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the update rule name of this?<div><img src=""paste-51ef446a5bf140993112dde543a0a89e60b6b38f.jpg""><br></div>"	"<b>Temporal-Difference</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the update rule name of this?<div><img src=""paste-28a2b1a90fc8b49b19b3e8071c97950b4b3c80e1.jpg""><br></div>"	"<b>Monte-Carlo</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  <div>Why <b>value state </b>are more meaningfull than <b>rewards</b>?</div>"	"Because rewards are <b>immediate </b>but could lead to sub-optimal results while <b>value state </b>estimate into the future  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  <div>What is the difference in the update rule formula of <b>Monte-Carlo </b>and <b>Temporal-Difference</b>?</div>"	"Monte-Carlo has the <b>exact</b> \(G_t\) while Temporal-Difference <b>estimates</b>&nbsp;it  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-c1d23912b0696f74d84ce6d14b1b591100bcd59e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  <div>What is the two inputs of a <b>Q-table</b>?</div>"	"The <b>state </b>and an <b>action</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0d26bea2844b9d4a335bebfcb1a6b47f01df7fad.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  <div>What is the output of a <b>Q-table</b>?</div>"	"The <b>Q-value</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0d26bea2844b9d4a335bebfcb1a6b47f01df7fad.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How is the <b>Q-table </b>initialized?"	"With <b>zeros</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-fb38ec61aa7aca95eacd8bf60fb2fc33a620b03e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the goal of <b>epsilon-greedy </b>policy vs <b>greedy</b>&nbsp;policy?"	"To allow sometimes some <b>exploration</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-a7c01b58a87000afc9c1ca11d29f05f9e5f34305.jpg"">(inverted)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does <b>epsilon-greedy </b>do in its <b>exploration </b>phase?"	"Select a <b>random </b>action  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-a7c01b58a87000afc9c1ca11d29f05f9e5f34305.jpg"">(inverted)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does <b>epsilon-greedy </b>do in its <b>exploitation&nbsp;</b>phase?"	"Select the most <b>valuable&nbsp;</b>action  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-a7c01b58a87000afc9c1ca11d29f05f9e5f34305.jpg"">(inverted)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>epsilon-greedy</b>, if we do the <b>exploration</b> with a probability \(\epsilon\), what is the probability to do <b>exploitation</b>?"	"\(1-\epsilon\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-a7c01b58a87000afc9c1ca11d29f05f9e5f34305.jpg"">(inverted)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>epsilon-greedy</b>, why do we set \(epsilon\) to a high value at first?"	"To do a lot of <b>exploration</b>  <div class=""details1"">Exploitation only have a probability of \(1-\epsilon\)</div>  <div class=""details2""></div>   <img src=""paste-f0756eccbec605e764b7f9601c564416f805dbed.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>epsilon-greedy</b>, how do \(\epsilon\) evolve through time?"	"It gets <b>reduced</b>  <div class=""details1"">So that we do less <u>exploration</u>&nbsp;and more <u>exploitation</u></div>  <div class=""details2""></div>   <img src=""paste-f0756eccbec605e764b7f9601c564416f805dbed.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>epsilon-greedy</b>, why is \(\epsilon\) reduced through time?"	"So that we do less&nbsp;<b>exploration</b>&nbsp;and more&nbsp;<b>exploitation</b><br>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-f0756eccbec605e764b7f9601c564416f805dbed.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>off-policy</b>, given that we choose action with <u>epsilon-greedy</u>, what action do we use to update the <b>Q-value</b>?"	"The action leading to the <b>maximum Q-value </b>(greedy)  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-3d92222b6427653b6fba618685f8653a56238ced.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>on-policy</b>, given that we choose action with <u>epsilon-greedy</u>, what action do we use to update the <b>Q-value</b>?"	"We gain sample the action with <b>epsilon-greedy</b>  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-3d92222b6427653b6fba618685f8653a56238ced.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>Q-learning</b>, what is the residual of the update rule?&nbsp;"	"Difference between the <b>immediate&nbsp;reward</b> and <b>maximum Q-value</b> vs former estimation  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-01b39c02471f0b82266ecf3f3f2cf61ef6cbcf36.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Where is the difference between <b>off-policy </b>and <b>on-policy</b>?"	"How to compute the <b>residual </b>when updating the <b>Q-value</b>  <div class=""details1"">Off: behavior policy != target policy</div>  <div class=""details2""></div>   <div><img src=""paste-3d92222b6427653b6fba618685f8653a56238ced.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Is <b>Q-learning </b>with&nbsp;<b>off-policy </b>or&nbsp;<b>on-policy</b>?"	"Off-policy  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-3d92222b6427653b6fba618685f8653a56238ced.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Is <b>Sarsa&nbsp;</b>with&nbsp;<b>off-policy </b>or&nbsp;<b>on-policy</b>?"	"On-policy  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-3d92222b6427653b6fba618685f8653a56238ced.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the difference between <b>Q-learning </b>and <b>Sarsa</b>?"	"The first is <b>off-policy</b>&nbsp;while the latter <b>on-policy</b>  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-7e1f2f4b46ee212be8bc1485ec65c2bd2a059bfc.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Do we learn the policy <b>directly </b>in off-policy or on-policy?"	"Off-policy  <div class=""details1"">Because in testing we don't use epsilon-greedy anymore but greedy</div>  <div class=""details2""></div>   <div><img src=""paste-3d92222b6427653b6fba618685f8653a56238ced.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the most stable between <b>off-policy </b>and <b>on-policy</b>?"	"On-policy  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-3d92222b6427653b6fba618685f8653a56238ced.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the data efficient between <b>off-policy </b>and <b>on-policy</b>?"	"Off-policy  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-3d92222b6427653b6fba618685f8653a56238ced.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>off-policy</b>, how do we call the policy used to select the initial action?"	"<b>Behavior </b>policy  <div class=""details1"">(first action in the image)</div>  <div class=""details2""></div>   <div><img src=""paste-2b14aae675da5a8308c4397c0f07d4bc6591dc41.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>off-policy</b>, how do we call the policy used to select the action to update the Q-value?"	"<b>Target&nbsp;</b>policy  <div class=""details1"">(second action in the image)</div>  <div class=""details2""></div>   <div><img src=""paste-2b14aae675da5a8308c4397c0f07d4bc6591dc41.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does the acronyme <b>DQN</b>&nbsp;mean?"	"<b>D</b>eep <b>Q</b>-<b>N</b>etwork  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Does <b>DQN </b>use <b>Temporal-Difference </b>or <b>Monte-Carlo</b>?"	"Temporal-Difference  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  On what kind of environment was <b>DQN </b>used at first?"	"Atari games  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What was the input of the <b>DQN </b>in Atari games?"	"Raw pixels  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What are the two parts of the architecture of the DQN?"	"A convnet and a fully-connected classifier  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How many <b>Q-values </b>does <b>DQN</b>&nbsp;predict?"	"The number of actions  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why can&nbsp;<b>Experience Replay&nbsp;</b>reduce forgetting?"	"Because we review previously seen actions/states  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-0604cfd83d65698c2d53ee86a7c035231fff0300.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why can&nbsp;<b>Experience Replay&nbsp;</b>reduce correlation between experiences?"	"Because each experience is sampled randomly  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why can latest sampling in DQN can induce experience correlation"	"Because all consecutive experiences may be very correlated and we would overfit on them  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-bce626b724fd6c37134a0014f5bf4d85019a9b71.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How is&nbsp;<b>Experience Replay </b>buffer initialized?"	"By doing a certain number of <b>random </b>actions  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>DQN</b>, when do we sample experiences from <b>Experience Replay </b>memory?"	"After taking an action and storing results in memory  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-634715fdb037983b2524867527333dece96e9e47.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In <b>DQN</b>, what do we do with samples experiences from <b>Experience Replay </b>memory?"	"We learn the DQN with the <b>temporal-difference </b>rule on the <b>samples</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-634715fdb037983b2524867527333dece96e9e47.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the loss in DQN training?"	"A <b>Mean Squared Error</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-634715fdb037983b2524867527333dece96e9e47.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What are the input of the <b>Mean Squared Error </b>in the <b>DQN</b>'s loss?"	"The current state predicted Q-value  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-634715fdb037983b2524867527333dece96e9e47.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What are the target of the <b>Mean Squared Error </b>in the <b>DQN</b>'s loss?"	"The reward + the next state predicted Q-value  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-634715fdb037983b2524867527333dece96e9e47.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the <b>reward scaling </b>in <b>DQN</b>?"	"Reward are either \(\{-1, 0, +1\}\)  <div class=""details1"">It's done to have a common set of hyperparameters across many different games</div>  <div class=""details2""></div>   <img src=""paste-634715fdb037983b2524867527333dece96e9e47.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does <b>LogSumExp </b>approximates?<div>\(\operatorname{LSE}\left(x_{1}, \ldots, x_{n}\right)=\log \left(\exp \left(x_{1}\right)+\cdots+\exp \left(x_{n}\right)\right)\)</div>"	"The <b>maximum </b>function  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to approximate with a <b>differentiable </b>function, the <b>maximum</b>?"	"LogSumExp  <div class=""details1"">\(\operatorname{LSE}\left(x_{1}, \ldots, x_{n}\right)=\log \left(\exp \left(x_{1}\right)+\cdots+\exp \left(x_{n}\right)\right)\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\log (X+a) \approx \)..."	"\(\log (X+a) \approx \log X+a / X\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why can we say the DQN learning rule is not fully&nbsp;<b>supervised</b>?"	"Because it tries to approximate the reward + the <b>predicted next state Q-value</b>&nbsp;  <div class=""details1"">Thus it's akin to self-distillation</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is <b>Fixed Q-targets</b>&nbsp;in <b>DQN</b>?"	"A <b>checkpoint</b> of the DQN, used to <b>predict the target</b> of the TD rule  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-73902065adf8fdfef420ee9e66147d4fed89b384.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why can we say DQN is doing <b>self-distillation</b>?"	"Because it tries to approximate the reward + the&nbsp;<b>predicted next state Q-value</b>&nbsp;predicted by itself  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  How can we avoid doing only <b>self-distillation </b>in DQN?"	"By using a separate <b>target network </b>to predict the next state Q-value  <div class=""details1"">In general, the target network is a checkpoint of the DQN</div>  <div class=""details2""></div>   <img src=""paste-73902065adf8fdfef420ee9e66147d4fed89b384.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In DQN, do we <b>underestimate </b>or <b>overestimate </b>Q-values?"	"We <b>overestimate </b>the Q-values  <div class=""details1"">Because of the maximum in the target policy</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  In DQN, why do we tend to <b>overestimate </b>Q-values?"	"Because being <u>off-policy</u>, our target policy is to take the <b>maximum </b>Q-value for the next state target  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does <b>Double DQN</b>&nbsp;fix compared to DQN?"	"It <b>overestimates less</b> the Q-values  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-876d0e00183e7c98d021fe90440ebd94b634e945.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  Why does <b>Double DQN</b>&nbsp;overestimate less the Q-values?"	"Because instead of taking the maximum Q-value of the target network, we pick the target's Q-value of the action chosen by the main network  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-876d0e00183e7c98d021fe90440ebd94b634e945.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is <b>Double DQN</b>?"	"Instead of taking the maximum Q-value of the target network, we pick the <b>target's Q-value of the action chosen by the main network</b>  <div class=""details1""></div>  <div class=""details2""></div>   <div><img src=""paste-876d0e00183e7c98d021fe90440ebd94b634e945.jpg""><br></div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What are the two parts (intuitively) of a <b>Q-value</b>?"	"The <b>value </b>of being at that <u>state</u>, and the <b>advantage </b>of taking a particular <u>action</u> at that state  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does&nbsp;<b>Dueling DQN </b>do?"	"It estimates separately the <b>state value </b>and the <b>action advantage</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-30730cc88a4209f59b12d126b12cf5d9880c9001.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  When does <b>Dueling Network </b>have an interest?"	"When <b>action choice making is not relevant</b>  <div class=""details1"">In some situation, only modeling the state value is sufficient</div>  <div class=""details2""></div>   <img src=""paste-0fd207958673208a1bfa272a05122472fd42c892.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the sampling in <b>Experience Replay </b>of DQN?"	"Uniform random  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why does the <b>rank of the features</b> need to decrease in order to <b>generalize</b>?"	"So that slightly different images can be <b>aliased </b>into common features  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What does ""<i>the network <b>aliases </b>state-actions pair by mapping them to a smaller subspace</i>"" mean?"	"That the <b>rank is reduced</b> so that sligthly different state-actions are <b>mapped to a common value</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle"">Reinforcement Learning</div>  What is the potential reason that <b>features matrix rank decreases </b>during DQN training?"	"Because the ""<b>self-distillation</b>"" where the target \(R + Q_{s+1}\) is computed us  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-6b8ae05bb74f739545741c1cdef21a7238d53624.jpg"">"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What tree family does <b>SumTree </b>belong to?"	"<b>Binary </b>tree  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-486b1384265ec98af1d554a10199f14efd225200.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> In a <b>SumTree</b>, what is the value of a <b>parent node</b>?"	"The <b>sum </b>of both children  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-486b1384265ec98af1d554a10199f14efd225200.jpg""> "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>Lasso penalty</b>?"	"A <b>L1 </b>regularization on the weights  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>Ridge penalty</b>?"	"A <b>L2&nbsp;</b>regularization on the weights  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>Elastic Net penalty</b>?"	"A <b>L1 </b>+<b>&nbsp;</b><b>L2&nbsp;</b>regularization on the weights  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Property of <b>linear function</b>?"	"\(f(\lambda x + \mu y) = \lambda f(x) + \mu f(y)\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is this property of \(f\)?<div>\(f(\lambda x + \mu y) = \lambda f(x) + \mu f(y)\)<br></div>"	"<b>Linear </b>function  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In <b>complex</b>, what is \(i\) value?"	"\(i^2 = -1\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In \(\mathbb{R}\), does every <b>polynomial </b>function have a root?"	"No  <div class=""details1"">\(x^2 + 1 = 0\) has not root in \(\mathbb{R}\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In \(\mathbb{C}\), does every <b>polynomial </b>function have a root?"	"Yes  <div class=""details1"">\(x^2 + 1 = 0\) has for root in \(x = i\) and \(x = -i\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the form of a <b>complex </b>number?"	"\(z = a + ib\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>complex conjugate </b>of \(z = a + ib\)?"	"\(z^* = a - ib\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is an <b>Argand diagram</b>?"	"A plot with the <b>real </b>on x, and the <b>imaginary </b>part on y  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-8e56461f8be951f2cd7e72ddfb3952b055c04892.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the effect of the <b>complex conjugate </b>on a <b>Argand </b>diagram?"	"Reflection in the x-axis  <div class=""details1"">\(z = a + ib\) --&gt; \(z^* = a - ib\)</div>  <div class=""details2""></div>   <img src=""paste-8e56461f8be951f2cd7e72ddfb3952b055c04892.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the name of this diagram?<div><img src=""paste-8e56461f8be951f2cd7e72ddfb3952b055c04892.jpg""><br></div>"	"<b>Argand </b>diagram  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the <b>modulus </b>of \(z = a + ib\)?</div>"	"\(r = |z| = \sqrt{a^2 + b^2}\)  <div class=""details1"">It is the <b>length </b>of the vector</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the <b>argument&nbsp;</b>of \(z = a + ib\)?</div>"	"\(\theta = \text{arg} z = \text{tan}^{-1}(\frac{b}{a})\)  <div class=""details1"">It is the <b>angle </b>between z and the real x-axis</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>For a <b>complex number </b>\(z\), what is \(r\):</div><div>\(z = r(\cos \theta + i\sin \theta)\)</div>"	"The <b>modulus&nbsp;</b>\(r = |z| = \sqrt{a^2 + b^2}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>For a <b>complex number </b>\(z\), what is \(\theta\):</div><div>\(z = r(\cos \theta + i\sin \theta)\)</div>"	"The <b>argument&nbsp;</b>\(\theta = \text{arg} z = \text{tan}^{-1}(\frac{b}{a})\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>How do you express a <b>complex </b>number with <b>polar </b>coordinates?&nbsp;</div>"	"\(z = r(\cos \theta + i\sin \theta)\)  <div class=""details1"">\(r\) is the <u>modulus</u></div>  <div class=""details2"">\(\theta\) is the <u>argument</u></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>\(e^a e^b =\)...</div>"	"\(e^a e^b = e^{a+b}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>\(e^{iz}=\)...</div>"	"\(e^{iz}= \cos z + i \sin z\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>\(\cos (-\theta) =\) ...</div>"	"\(\cos (-\theta) = \cos (\theta)\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>\(\sin (-\theta) =\) ...</div>"	"\(\sin (-\theta) = -\sin (\theta)\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How do you express a&nbsp;<b>complex&nbsp;</b>number with&nbsp;<b>exponent</b>?<br>"	"\(z = re^{i\theta}\)  <div class=""details1"">\(r\) is the&nbsp;<u>modulus</u></div>  <div class=""details2"">\(\theta\) is the&nbsp;<u>argument</u></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>root </b>of a function?"	"Values of \(x\) where \(f(x)=0\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-1ad83b9b95b19831d0b51ceea731d39a7e4c600f.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How do we call \(z\) in \(z^n = 1\) for \(n = 1, 2, 3, ...\)?"	"The <b>nth root of unity</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>nth root of unity</b>?"	"<div>A number \(z\) such that \(z^n = 1\) for \(n = 1, 2, 3, ...\)?<b><br></b></div>  <div class=""details1"">Usually \(z\) is complex</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  In the <b>nth root of unity</b>, \(z^n = 1\), what is the condition on \(n\)?"	"<div>Positive integer \(n = 1, 2, 3, ...\)<b><br></b></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is this theorem?<div><br></div><div>\(\cos n\theta + i\sin n\theta = (\cos \theta + i\sin \theta)^n\)</div>"	"<b>De Moivre</b>’s theorem  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-4bc66b3ac9a4e8c0bbc2437069ee479082922cc7.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  ""Proof by <b>induction</b>"" in french?"	"Preuve par <b>récurrence</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>""Preuve par&nbsp;<b>récurrence</b>"" in English?<br></div>"	"Proof by&nbsp;<b>induction</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is <b>De Moivre </b>theorem?</div>"	"\(\cos n\theta + i\sin n\theta = (\cos \theta + i\sin \theta)^n\)<br>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>How can we prove <b>De Moire </b>theorem?</div><div>\(\cos n\theta + i\sin n\theta = (\cos \theta + i\sin \theta)^n\)<br></div>"	"With <b>induction</b>  <div class=""details1""></div>  <div class=""details2""></div>   <div>Proving for \(n=0\) is trivial.</div><div><br></div><img src=""paste-7a08ac5c3da8331b693e9eeae99d4b265458e006.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  For a binary operator \(*\), what is <b>commutativity</b>?"	"\(a * b = b * a\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  For a binary operator \(*\), what is <b>associativity</b>?"	"\((a * b) * c = a * (b * c)\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  For a binary operator \(*\), what is <b>identity</b>?"	"Given \(e\), \(a * e = 0\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  For a binary operator \(*\), what is this property \(a * b = b * a\)?"	"Commutativity  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  For a binary operator \(*\), what is this property \((a*b)*c = a * (b*c)\)?"	"Associativity  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>unit vector</b>?"	"A vector of length \(|\mathbf{v}|\) 1  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Given \(\mathbf{a}\) and \(\mathbf{b}\) two vectors, what is this operator \(\mathbf{a} \cdot \mathbf{b}\)?"	"<b>dot</b> product  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the relation between the <b>dot product </b>and <b>cos</b>?"	"\(\mathbf{a} \cdot \mathbf{b} = |\mathbf{a}| |\mathbf{b}| \cos \theta\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does \(\mathbf{a} \cdot \mathbf{b} = 0\) mean?"	"\(\mathbf{a}\) and \(\mathbf{b}\) are <b>perpendicular</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> In which domain is the <b>Cheeger </b>constant used?"	"Graph theory  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Graph theory</div> What does the <b>Cheeger </b>constant measure?"	"Whether or not a graph has a ""<b>bottleneck</b>""  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Graph theory</div> What is the <b>Cheeger </b>constant?"	"The <b>number of edges</b> going <b>outside</b> a <b>partition</b> of the graph  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Graph theory</div> What is the <b>Cheeger </b>constant of a graph where some vertices don't have edges?"	"0  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Graph theory</div> What is the <b>Cheeger </b>constant of a strongly connected graph?"	"The total number of vertices  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Graph theory</div> How do we call the minimum number of edges going outside a partition of a graph?"	"<b>Cheeger </b>constant  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to normalize a vector to have values between 0 and 1?"	"<img src=""paste-40d511e0b7214843a41090f63aab5b068073ac38.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What did happen to this image?<div><img src=""paste-bad9f834aec180471901f1f6db28b539ba516c23.jpg""><br></div>"	"<b>Constrast </b>normalization  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is <b>contrast normalization</b>?</div>"	"<b>Subtracting by mean</b> pixel, and <b>dividing by standard deviation</b> pixel  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-bad9f834aec180471901f1f6db28b539ba516c23.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the name of the <b>normalization</b> usually done for <b>ImageNet</b> when subtracting by mean and dividing by std?</div>"	"<b>Contrast </b>normalization  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-bad9f834aec180471901f1f6db28b539ba516c23.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>Why do we do <b>contrast normalization </b>before feeding images to a CNN?</div>"	"To avoid learning <b>spurrious correlations</b> about the contrast&nbsp;  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-bad9f834aec180471901f1f6db28b539ba516c23.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the other name of <b>PCA Sphering</b>?"	"Whitening  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which variant of BatchNormalization is more appropriate for non-iid data?"	"Batch <b>Re</b>normalization  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What are the \(\mu\) and \(\sigma\) in training for the BatchNorm?"	"The mean and std deviation of the current&nbsp;<b>mini-batch</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What are the \(\mu\) and \(\sigma\) in inference for the BatchNorm?"	"The <b>moving</b> mean and <b>moving</b> std deviation <b>all seen batch</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How does&nbsp;<b>BatchRenorm </b>differ from BatchNorm?"	"The normalization is <b>corrected by the running mean and std</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-32e68e735efda6520e92eb8b454ae0ac88866915.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How is this approximation called?<div><img src=""paste-af4052fd524c3175d98784f6726c100656d823e5.jpg""><br></div>"	"Finite differences  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>How do we approximate <b>derivative </b>with <b>finite differences</b>?</div>"	"Compute the difference with a slight modification \(h\) very small&nbsp;  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-af4052fd524c3175d98784f6726c100656d823e5.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Order matter in a <b>combination </b>or a <b>permutation</b>?"	"Permutation  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>combination</b>?"	"A selection of items from a collection where the <b>order</b> <b>doesn't mattter</b>  <div class=""details1""></div>  <div class=""details2""></div>   S = {x, y, z}<div><br></div><div>Combination(S, k=2) -&gt; {x, y}, {x, z}, {y, z}</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a <b>permutation</b>?"	"A selection of items from a collection where the <b>order</b><b>&nbsp;mattters</b>  <div class=""details1""></div>  <div class=""details2""></div>   S = {x, y, z}<div><br></div><div>Permutation(S, k=2) -&gt; {x, y}, {y, x}, {x, z}, {z, x}, {y, z}, {z, y}</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the&nbsp;<b>permutation </b>formula?"	"\(\frac{n!}{(n-k)!}\)  <div class=""details1""></div>  <div class=""details2""></div>   S = {x, y, z}<div><br></div><div>Permutation(S, k=2) -&gt; {x, y}, {y, x}, {x, z}, {z, x}, {y, z}, {z, y}</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the&nbsp;<b>combination&nbsp;</b>formula?"	"\(\frac{n!}{k!(n-k)!}\)  <div class=""details1""></div>  <div class=""details2""></div>   S = {x, y, z}<div><br></div><div>Combination(S, k=2) -&gt; {x, y}, {x, z}, {y, z}</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How do we call this?<div><br><div>\(\left(\begin{array}{l}n \\k\end{array}\right)\)</div></div>"	"Binomial coefficients  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What do the <b>binomial coefficients </b>represent?<div><br><div>\(\left(\begin{array}{l}n \\k\end{array}\right)\)</div></div>"	"The number of possible <b>combinations</b>  <div class=""details1""></div>  <div class=""details2""></div>   \(\left(\begin{array}{l}n \\k\end{array}\right) = \frac{n!}{k!(n-k)!}\)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \((\mathbf{a}+\mathbf{b})^{\mathbf{n}}=\) ...<br>"	"\((\mathbf{a}+\mathbf{b})^{\mathbf{n}}=\sum_{\mathbf{k}=0}^{\mathbf{n}}\left(\begin{array}{l}<br>\mathbf{n} \\<br>\mathbf{k}<br>\end{array}\right) \cdot \mathbf{a}^{\mathbf{n}-\mathbf{k}} \cdot \mathbf{b}^{\mathbf{k}}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\sum_{\mathbf{k}=0}^{\mathbf{n}}\left(\begin{array}{l}<br>\mathbf{n} \\<br>\mathbf{k}<br>\end{array}\right) \cdot \mathbf{a}^{\mathbf{n}-\mathbf{k}} \cdot \mathbf{b}^{\mathbf{k}}=\) ...<br>"	"\(\sum_{\mathbf{k}=0}^{\mathbf{n}}\left(\begin{array}{l}<br>\mathbf{n} \\<br>\mathbf{k}<br>\end{array}\right) \cdot \mathbf{a}^{\mathbf{n}-\mathbf{k}} \cdot \mathbf{b}^{\mathbf{k}}=(\mathbf{a}+\mathbf{b})^{\mathbf{n}}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is \(\left(\begin{array}{l}n \\k\end{array}\right)\) in \((\mathbf{a}+\mathbf{b})^{\mathbf{n}}=\sum_{\mathbf{k}=0}^{\mathbf{n}}\left(\begin{array}{l}<br>\mathbf{n} \\<br>\mathbf{k}<br>\end{array}\right) \cdot \mathbf{a}^{\mathbf{n}-\mathbf{k}} \cdot \mathbf{b}^{\mathbf{k}}\)?"	"The number of possible <b>combinations</b>  <div class=""details1""></div>  <div class=""details2""></div>   \(\frac{n!}{k!(n-k)!}\)"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to fuse <b>bias </b>into the <b>weights </b>for linear regression?"	"Add bias into weights, add 1 to the features.  <div class=""details1""></div>  <div class=""details2""></div>   \(\mathbf{w} = (b, w_1, ..., w_N)^T\)<div><br></div><div>\(\mathbf{x} = (1, x_1, ..., x_N)^T\)</div><div><br></div><div>\(\mathbf{w}^T \mathbf{x}&nbsp; = \sum_{i=2}^N w_i \mathbf{x}_i + w_1\)</div>"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What are <b>basis functions </b>for linear regression?"	"Non-linear functions  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>pseudo-inverse </b>of \(\mathbf{X}\)?"	"\(\mathbf{X}^+ = (X^T X)^{-1} X^T\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is this \(\mathbf{X}^+ = (X^T X)^{-1} X^T\)?"	"The <b>pseudo-inverse of&nbsp;</b>\(\mathbf{X}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>least squares</b>&nbsp;method?"	"Minimizing the <b>square of the errors</b>  <div class=""details1"">It's the Mean Squared Error (MSE)</div>  <div class=""details2""></div>   <img src=""paste-5372b9bd4361d6ddb4c511307f4dee4ea3f63393.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When does the <b>least squares</b>&nbsp;method have a <b>closed-form solution</b>?"	"When the problem is <b>linear</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-5372b9bd4361d6ddb4c511307f4dee4ea3f63393.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does <b>Ordinary Least Squares </b>(OLS) method minimizes?"	"The sum of <b>squared errors</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-5372b9bd4361d6ddb4c511307f4dee4ea3f63393.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the closed-form solution of <b>Ordinary Least Squares</b>?"	"\(W = (X^T X)^{-1} X^T y\)  <div class=""details1"">It's the <u>pseudo-inverse</u>&nbsp;multiplied with the targets</div>  <div class=""details2""></div>   <img src=""paste-5372b9bd4361d6ddb4c511307f4dee4ea3f63393.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  For a&nbsp;<b>convex </b>function \(f\), how to find the <b>minimum</b>?"	"When its <b>derivative is equal to 0</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  For a <b>convex </b>function, why does the point \(x^*\) where \(\nabla f(x^*) = 0\) is the <b>minimum </b>and cannot be the <b>maximum</b>?"	"Because the maximum are not reachable  <div class=""details1"">ie. on all points \(x \in \mathcal{R}^N, \, \nabla^2 f(x) \ge 0\)</div>  <div class=""details2""></div>   <img src=""paste-690b0164f815a92c940351b3f91a731d01b8b234.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  On which conditions, a point \(x^* \in \mathcal{R^N}\) is the minimum of \(f\)?"	"\(\nabla f(x^*) = 0\) and \(\nabla^2 f(x^*) \ge 0\)  <div class=""details1"">First condition means either a <u>max</u>, <u>min</u>, or <u>saddle</u> point</div>  <div class=""details2"">Second condition means that slopes are upward in all directions</div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Linked List Random Node</div> Given a <b>Linked List </b>with each node storing an integer, sample uniform randomly a node value<div><br></div><div><i>Solution with minimal space complexity</i>.</div>  <br/><br/> <div class=""example""></div>"	"<img src=""paste-bfca38f36310155083881c8dd042d3f533c00941.jpg""> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">class</span> <span style=""color: #0000FF; font-weight: bold"">ReservoirR</span>:     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">__init__</span>(<span style=""color: #008000"">self</span>, head: ListNode):         <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>head <span style=""color: #666666"">=</span> head      <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">getRandom</span>(<span style=""color: #008000"">self</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">int</span>:         kept, c <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>, <span style=""color: #666666"">0</span>                  node <span style=""color: #666666"">=</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>head         <span style=""color: #008000; font-weight: bold"">while</span> node:             c <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>             <span style=""color: #008000; font-weight: bold"">if</span> random<span style=""color: #666666"">.</span>random() <span style=""color: #666666"">*</span> c <span style=""color: #666666"">&lt;</span> <span style=""color: #666666"">1</span>:                 kept <span style=""color: #666666"">=</span> node<span style=""color: #666666"">.</span>val             node <span style=""color: #666666"">=</span> node<span style=""color: #666666"">.</span>next                      <span style=""color: #008000; font-weight: bold"">return</span> kept </pre></div> </td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Linked List Random Node</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">class</span> <span style=""color: #0000FF; font-weight: bold"">ReservoirR</span>:     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">__init__</span>(<span style=""color: #008000"">self</span>, head: ListNode):         <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>head <span style=""color: #666666"">=</span> head      <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">getRandom</span>(<span style=""color: #008000"">self</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">int</span>:         kept, c <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>, <span style=""color: #666666"">0</span>                  node <span style=""color: #666666"">=</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>head         <span style=""color: #008000; font-weight: bold"">while</span> node:             c <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>             <span style=""color: #008000; font-weight: bold"">if</span> random<span style=""color: #666666"">.</span>random() <span style=""color: #666666"">*</span> c <span style=""color: #666666"">&lt;</span> <span style=""color: #666666"">1</span>:                 kept <span style=""color: #666666"">=</span> node<span style=""color: #666666"">.</span>val             node <span style=""color: #666666"">=</span> node<span style=""color: #666666"">.</span>next                      <span style=""color: #008000; font-weight: bold"">return</span> kept </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Linked List Random Node</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">class</span> <span style=""color: #0000FF; font-weight: bold"">ReservoirR</span>:     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">__init__</span>(<span style=""color: #008000"">self</span>, head: ListNode):         <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>head <span style=""color: #666666"">=</span> head      <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">getRandom</span>(<span style=""color: #008000"">self</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">int</span>:         kept, c <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>, <span style=""color: #666666"">0</span>                  node <span style=""color: #666666"">=</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>head         <span style=""color: #008000; font-weight: bold"">while</span> node:             c <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>             <span style=""color: #008000; font-weight: bold"">if</span> random<span style=""color: #666666"">.</span>random() <span style=""color: #666666"">*</span> c <span style=""color: #666666"">&lt;</span> <span style=""color: #666666"">1</span>:                 kept <span style=""color: #666666"">=</span> node<span style=""color: #666666"">.</span>val             node <span style=""color: #666666"">=</span> node<span style=""color: #666666"">.</span>next                      <span style=""color: #008000; font-weight: bold"">return</span> kept </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Linked List Random Node</div>  Given a <b>Linked List </b>with each node storing an integer, sample uniform randomly a node value<div><br></div><div><i>Solution with minimal space complexity</i>.</div>  <br/> <div class=""example""></div> <br/> Which data structure? "	"Reservoir Sampling  <br/> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">class</span> <span style=""color: #0000FF; font-weight: bold"">ReservoirR</span>:     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">__init__</span>(<span style=""color: #008000"">self</span>, head: ListNode):         <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>head <span style=""color: #666666"">=</span> head      <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">getRandom</span>(<span style=""color: #008000"">self</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">int</span>:         kept, c <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>, <span style=""color: #666666"">0</span>                  node <span style=""color: #666666"">=</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>head         <span style=""color: #008000; font-weight: bold"">while</span> node:             c <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>             <span style=""color: #008000; font-weight: bold"">if</span> random<span style=""color: #666666"">.</span>random() <span style=""color: #666666"">*</span> c <span style=""color: #666666"">&lt;</span> <span style=""color: #666666"">1</span>:                 kept <span style=""color: #666666"">=</span> node<span style=""color: #666666"">.</span>val             node <span style=""color: #666666"">=</span> node<span style=""color: #666666"">.</span>next                      <span style=""color: #008000; font-weight: bold"">return</span> kept </pre></div> </td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Linked List Random Node</div> Given a&nbsp;<b>Linked List&nbsp;</b>with each node storing an integer, sample uniform randomly a node value<div><br></div><div><i>Solution with minimal time complexity</i>.</div>  <br/><br/> <div class=""example""></div>"	"<br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">class</span> <span style=""color: #0000FF; font-weight: bold"">Solution</span>:     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">__init__</span>(<span style=""color: #008000"">self</span>, head: ListNode):         <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values <span style=""color: #666666"">=</span> []         <span style=""color: #008000; font-weight: bold"">while</span> head:             <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values<span style=""color: #666666"">.</span>append(head<span style=""color: #666666"">.</span>val)             head <span style=""color: #666666"">=</span> head<span style=""color: #666666"">.</span>next      <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">getRandom</span>(<span style=""color: #008000"">self</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">int</span>:         index <span style=""color: #666666"">=</span> <span style=""color: #008000"">int</span>(random<span style=""color: #666666"">.</span>random() <span style=""color: #666666"">*</span> <span style=""color: #008000"">len</span>(<span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values))         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values[index] </pre></div> </td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Linked List Random Node</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">class</span> <span style=""color: #0000FF; font-weight: bold"">Solution</span>:     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">__init__</span>(<span style=""color: #008000"">self</span>, head: ListNode):         <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values <span style=""color: #666666"">=</span> []         <span style=""color: #008000; font-weight: bold"">while</span> head:             <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values<span style=""color: #666666"">.</span>append(head<span style=""color: #666666"">.</span>val)             head <span style=""color: #666666"">=</span> head<span style=""color: #666666"">.</span>next      <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">getRandom</span>(<span style=""color: #008000"">self</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">int</span>:         index <span style=""color: #666666"">=</span> <span style=""color: #008000"">int</span>(random<span style=""color: #666666"">.</span>random() <span style=""color: #666666"">*</span> <span style=""color: #008000"">len</span>(<span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values))         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values[index] </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(1)\) although \(O(N)\) for initialization
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Linked List Random Node</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">class</span> <span style=""color: #0000FF; font-weight: bold"">Solution</span>:     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">__init__</span>(<span style=""color: #008000"">self</span>, head: ListNode):         <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values <span style=""color: #666666"">=</span> []         <span style=""color: #008000; font-weight: bold"">while</span> head:             <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values<span style=""color: #666666"">.</span>append(head<span style=""color: #666666"">.</span>val)             head <span style=""color: #666666"">=</span> head<span style=""color: #666666"">.</span>next      <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">getRandom</span>(<span style=""color: #008000"">self</span>) <span style=""color: #666666"">-&gt;</span> <span style=""color: #008000"">int</span>:         index <span style=""color: #666666"">=</span> <span style=""color: #008000"">int</span>(random<span style=""color: #666666"">.</span>random() <span style=""color: #666666"">*</span> <span style=""color: #008000"">len</span>(<span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values))         <span style=""color: #008000; font-weight: bold"">return</span> <span style=""color: #008000"">self</span><span style=""color: #666666"">.</span>values[index] </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(N)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Bubble Sort</div> Implement Bubble Sort  <br/><br/> <div class=""example""></div>"	"Swapping each pair together.<br><br>Worst case is when the list is reversed: each element has to be swap \(N\) times.<div><img src=""paste-e397eb1e61cc502e5dfa2acc2e0769556ec9acdf.jpg""><br></div> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">bubble_sort</span>(nums):     swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span>     <span style=""color: #008000; font-weight: bold"">while</span> swapped:         swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">False</span>         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>):             <span style=""color: #008000; font-weight: bold"">if</span> nums[i] <span style=""color: #666666"">&gt;</span> nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>]:                 nums[i], nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>] <span style=""color: #666666"">=</span> nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>], nums[i]                 swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span> </pre></div> </td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Bubble Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">bubble_sort</span>(nums):     swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span>     <span style=""color: #008000; font-weight: bold"">while</span> swapped:         swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">False</span>         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>):             <span style=""color: #008000; font-weight: bold"">if</span> nums[i] <span style=""color: #666666"">&gt;</span> nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>]:                 nums[i], nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>] <span style=""color: #666666"">=</span> nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>], nums[i]                 swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span> </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n^2)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Bubble Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">bubble_sort</span>(nums):     swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span>     <span style=""color: #008000; font-weight: bold"">while</span> swapped:         swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">False</span>         <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>):             <span style=""color: #008000; font-weight: bold"">if</span> nums[i] <span style=""color: #666666"">&gt;</span> nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>]:                 nums[i], nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>] <span style=""color: #666666"">=</span> nums[i<span style=""color: #666666"">+</span><span style=""color: #666666"">1</span>], nums[i]                 swapped <span style=""color: #666666"">=</span> <span style=""color: #008000; font-weight: bold"">True</span> </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Selection Sort</div> Implement Selection Sort  <br/><br/> <div class=""example""></div>"	"We select the smallest element of the unsorted list and append it to the sorted list.<div><img src=""paste-68afd6d1941c67cb16d89aaa548beac438ee45a9.jpg""><br></div> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">selection_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># This value of i corresponds to how many values were sorted</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums)):         <span style=""color: #408080; font-style: italic""># We assume that the first item of the unsorted segment is the smallest</span>         low <span style=""color: #666666"">=</span> i         <span style=""color: #408080; font-style: italic""># This loop iterates over the unsorted items</span>         <span style=""color: #008000; font-weight: bold"">for</span> j <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             <span style=""color: #008000; font-weight: bold"">if</span> nums[j] <span style=""color: #666666"">&lt;</span> nums[low]:                 low <span style=""color: #666666"">=</span> j         <span style=""color: #408080; font-style: italic""># Swap values of the lowest unsorted element with the first unsorted</span>         <span style=""color: #408080; font-style: italic""># element</span>         nums[i], nums[low] <span style=""color: #666666"">=</span> nums[low], nums[i] </pre></div> </td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Selection Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">selection_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># This value of i corresponds to how many values were sorted</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums)):         <span style=""color: #408080; font-style: italic""># We assume that the first item of the unsorted segment is the smallest</span>         low <span style=""color: #666666"">=</span> i         <span style=""color: #408080; font-style: italic""># This loop iterates over the unsorted items</span>         <span style=""color: #008000; font-weight: bold"">for</span> j <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             <span style=""color: #008000; font-weight: bold"">if</span> nums[j] <span style=""color: #666666"">&lt;</span> nums[low]:                 low <span style=""color: #666666"">=</span> j         <span style=""color: #408080; font-style: italic""># Swap values of the lowest unsorted element with the first unsorted</span>         <span style=""color: #408080; font-style: italic""># element</span>         nums[i], nums[low] <span style=""color: #666666"">=</span> nums[low], nums[i] </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n^2)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Selection Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">selection_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># This value of i corresponds to how many values were sorted</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(<span style=""color: #008000"">len</span>(nums)):         <span style=""color: #408080; font-style: italic""># We assume that the first item of the unsorted segment is the smallest</span>         low <span style=""color: #666666"">=</span> i         <span style=""color: #408080; font-style: italic""># This loop iterates over the unsorted items</span>         <span style=""color: #008000; font-weight: bold"">for</span> j <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(i <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>, <span style=""color: #008000"">len</span>(nums)):             <span style=""color: #008000; font-weight: bold"">if</span> nums[j] <span style=""color: #666666"">&lt;</span> nums[low]:                 low <span style=""color: #666666"">=</span> j         <span style=""color: #408080; font-style: italic""># Swap values of the lowest unsorted element with the first unsorted</span>         <span style=""color: #408080; font-style: italic""># element</span>         nums[i], nums[low] <span style=""color: #666666"">=</span> nums[low], nums[i] </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Difference between <b>Insertion </b>sort and <b>Selection </b>sort?"	"<b>Selection</b> sort selects the <b>smallest one</b>  and append it to the sorted list<div><br></div><div><b>Insertion</b> sort inserts the <b>current  one</b> in the correct position of the sorted list</div>  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-68afd6d1941c67cb16d89aaa548beac438ee45a9.jpg""><img src=""paste-68afd6d1941c67cb16d89aaa548beac438ee45a9.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the main property of a <b>max heap</b>?"	"Parent value is <b>higher</b> or equal to children  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-04980cd63469d5b92ddf4013ca8d81f849ea56d8.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the main property of a <b>min heap</b>?"	"Parent value is <b>lower</b> or equal to children  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-aebc8e1c650b3e24371878a3006e50dbe727ab72.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How to find the <b>maximum value </b>of a <u>max heap</u>?"	"Returns the <b>root </b>node  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-04980cd63469d5b92ddf4013ca8d81f849ea56d8.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When representing a <b>binary tree </b>with an array, what is the position of the <b>root</b>?"	"1  <br/><br/> <span style=""font-size: 12px"">In our case we assume the array starts at 1 and not at 0</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When representing a <b>binary tree </b>with an array, what is the position of a <b>left child</b>?"	"\(2n\) with \(n\) being the parent index in the array  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When representing a <b>binary tree </b>with an array, what is the position of a <b>right child</b>?"	"\(2n + 1\) with \(n\) being the parent index in the array  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When representing a <b>binary tree </b>with an array, what is the position of a <b>parent</b>?"	"\(\frac{n}{2}\) with \(n\) being the child index in the array  <br/><br/> <span style=""font-size: 12px"">Take the floor in case of floating result which happen for the right child</span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> <div>What is the difference between <b>stable </b>and <b>unstable sorts</b>?</div>"	"Stable respects the <b>initial ordering</b> when equal values are encountred  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-59b543b9e7a2cb277fb9490b9b09e20439e75a8d.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Heap Sort</div> Implement Heap Sort  <br/><br/> <div class=""example""></div>"	"Put MaxHeap's root at the end of the array then rebuild the heap excluding last elements<div><img src=""paste-c6da357684c7be58db9b4d7bb8aa1c2ee35c13f9.jpg""><br></div> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">heap_sort</span>(nums):     n <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(nums)      <span style=""color: #408080; font-style: italic""># Create a Max Heap from the list</span>     <span style=""color: #408080; font-style: italic""># The 2nd argument of range means we stop at the element before -1 i.e.</span>     <span style=""color: #408080; font-style: italic""># the first element of the list.</span>     <span style=""color: #408080; font-style: italic""># The 3rd argument of range means we iterate backwards, reducing the count</span>     <span style=""color: #408080; font-style: italic""># of i by 1</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(n, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):         heapify(nums, n, i)      <span style=""color: #408080; font-style: italic""># Move the root of the max heap to the end of</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(n <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>, <span style=""color: #666666"">0</span>, <span style=""color: #666666"">-1</span>):         nums[i], nums[<span style=""color: #666666"">0</span>] <span style=""color: #666666"">=</span> nums[<span style=""color: #666666"">0</span>], nums[i]         heapify(nums, i, <span style=""color: #666666"">0</span>)</pre></div></td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Heap Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">heap_sort</span>(nums):     n <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(nums)      <span style=""color: #408080; font-style: italic""># Create a Max Heap from the list</span>     <span style=""color: #408080; font-style: italic""># The 2nd argument of range means we stop at the element before -1 i.e.</span>     <span style=""color: #408080; font-style: italic""># the first element of the list.</span>     <span style=""color: #408080; font-style: italic""># The 3rd argument of range means we iterate backwards, reducing the count</span>     <span style=""color: #408080; font-style: italic""># of i by 1</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(n, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):         heapify(nums, n, i)      <span style=""color: #408080; font-style: italic""># Move the root of the max heap to the end of</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(n <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>, <span style=""color: #666666"">0</span>, <span style=""color: #666666"">-1</span>):         nums[i], nums[<span style=""color: #666666"">0</span>] <span style=""color: #666666"">=</span> nums[<span style=""color: #666666"">0</span>], nums[i]         heapify(nums, i, <span style=""color: #666666"">0</span>)</pre></div></td></tr></tbody></table></center><br>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n \log n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Heap Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">heap_sort</span>(nums):     n <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(nums)      <span style=""color: #408080; font-style: italic""># Create a Max Heap from the list</span>     <span style=""color: #408080; font-style: italic""># The 2nd argument of range means we stop at the element before -1 i.e.</span>     <span style=""color: #408080; font-style: italic""># the first element of the list.</span>     <span style=""color: #408080; font-style: italic""># The 3rd argument of range means we iterate backwards, reducing the count</span>     <span style=""color: #408080; font-style: italic""># of i by 1</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(n, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):         heapify(nums, n, i)      <span style=""color: #408080; font-style: italic""># Move the root of the max heap to the end of</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(n <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>, <span style=""color: #666666"">0</span>, <span style=""color: #666666"">-1</span>):         nums[i], nums[<span style=""color: #666666"">0</span>] <span style=""color: #666666"">=</span> nums[<span style=""color: #666666"">0</span>], nums[i]         heapify(nums, i, <span style=""color: #666666"">0</span>)</pre></div></td></tr></tbody></table></center><br>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
" <h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Heap Sort</div>  Implement Heap Sort  <br/> <div class=""example""></div> <br/> Which data structure? "	"Max Heap  <br/> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">heap_sort</span>(nums):     n <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(nums)      <span style=""color: #408080; font-style: italic""># Create a Max Heap from the list</span>     <span style=""color: #408080; font-style: italic""># The 2nd argument of range means we stop at the element before -1 i.e.</span>     <span style=""color: #408080; font-style: italic""># the first element of the list.</span>     <span style=""color: #408080; font-style: italic""># The 3rd argument of range means we iterate backwards, reducing the count</span>     <span style=""color: #408080; font-style: italic""># of i by 1</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(n, <span style=""color: #666666"">-1</span>, <span style=""color: #666666"">-1</span>):         heapify(nums, n, i)      <span style=""color: #408080; font-style: italic""># Move the root of the max heap to the end of</span>     <span style=""color: #008000; font-weight: bold"">for</span> i <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(n <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>, <span style=""color: #666666"">0</span>, <span style=""color: #666666"">-1</span>):         nums[i], nums[<span style=""color: #666666"">0</span>] <span style=""color: #666666"">=</span> nums[<span style=""color: #666666"">0</span>], nums[i]         heapify(nums, i, <span style=""color: #666666"">0</span>)</pre></div></td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Complexity of building a <b>max heap </b>from an array?"	"\(O(n \log n)\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Complexity of <b>heapifying </b>a node?"	"\(O(\log n)\)  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is the <b>heapify </b>procedure?"	"Recursively<b> push down</b> a node until it respects the heap condition  <br/><br/> <span style=""font-size: 12px"">A max heap would impose that the node value is higher than its children's values</span> <br/><br/> <img src=""paste-2c513b48fea97307fa73a06c08974095c913ec46.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> How to build a <b>heap </b>from an array?"	"Call <b>heapify </b>on all indexes starting from the last  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-2c513b48fea97307fa73a06c08974095c913ec46.jpg""><div><img src=""paste-7ac79d90218cab842f207339846fa32a6e88b765.jpg""><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Merge Sort</div> Implement Merge Sort  <br/><br/> <div class=""example""></div>"	"Divide recursively array into two lists until reaching lists of 1 element, then merge them with sorting by adding elements to a second array<div><img src=""paste-3d52a595dca065c94454a874c52edf2bb1de405d.jpg""></div> <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">merge</span>(left_list, right_list):     sorted_list <span style=""color: #666666"">=</span> []     left_list_index <span style=""color: #666666"">=</span> right_list_index <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>      <span style=""color: #408080; font-style: italic""># We use the list lengths often, so its handy to make variables</span>     left_list_length, right_list_length <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(left_list), <span style=""color: #008000"">len</span>(right_list)      <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(left_list_length <span style=""color: #666666"">+</span> right_list_length):         <span style=""color: #008000; font-weight: bold"">if</span> left_list_index <span style=""color: #666666"">&lt;</span> left_list_length <span style=""color: #AA22FF; font-weight: bold"">and</span> right_list_index <span style=""color: #666666"">&lt;</span> right_list_length:             <span style=""color: #408080; font-style: italic""># We check which value from the start of each list is smaller</span>             <span style=""color: #408080; font-style: italic""># If the item at the beginning of the left list is smaller, add it</span>             <span style=""color: #408080; font-style: italic""># to the sorted list</span>             <span style=""color: #008000; font-weight: bold"">if</span> left_list[left_list_index] <span style=""color: #666666"">&lt;=</span> right_list[right_list_index]:                 sorted_list<span style=""color: #666666"">.</span>append(left_list[left_list_index])                 left_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>             <span style=""color: #408080; font-style: italic""># If the item at the beginning of the right list is smaller, add it</span>             <span style=""color: #408080; font-style: italic""># to the sorted list</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 sorted_list<span style=""color: #666666"">.</span>append(right_list[right_list_index])                 right_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>          <span style=""color: #408080; font-style: italic""># If we've reached the end of the of the left list, add the elements</span>         <span style=""color: #408080; font-style: italic""># from the right list</span>         <span style=""color: #008000; font-weight: bold"">elif</span> left_list_index <span style=""color: #666666"">==</span> left_list_length:             sorted_list<span style=""color: #666666"">.</span>append(right_list[right_list_index])             right_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #408080; font-style: italic""># If we've reached the end of the of the right list, add the elements</span>         <span style=""color: #408080; font-style: italic""># from the left list</span>         <span style=""color: #008000; font-weight: bold"">elif</span> right_list_index <span style=""color: #666666"">==</span> right_list_length:             sorted_list<span style=""color: #666666"">.</span>append(left_list[left_list_index])             left_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>      <span style=""color: #008000; font-weight: bold"">return</span> sorted_list   <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">merge_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># If the list is a single element, return it</span>     <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">&lt;=</span> <span style=""color: #666666"">1</span>:         <span style=""color: #008000; font-weight: bold"">return</span> nums      <span style=""color: #408080; font-style: italic""># Use floor division to get midpoint, indices must be integers</span>     mid <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">//</span> <span style=""color: #666666"">2</span>      <span style=""color: #408080; font-style: italic""># Sort and merge each half</span>     left_list <span style=""color: #666666"">=</span> merge_sort(nums[:mid])     right_list <span style=""color: #666666"">=</span> merge_sort(nums[mid:])      <span style=""color: #408080; font-style: italic""># Merge the sorted lists into a new one</span>     <span style=""color: #008000; font-weight: bold"">return</span> merge(left_list, right_list) </pre></div> </td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Merge Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">merge</span>(left_list, right_list):     sorted_list <span style=""color: #666666"">=</span> []     left_list_index <span style=""color: #666666"">=</span> right_list_index <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>      <span style=""color: #408080; font-style: italic""># We use the list lengths often, so its handy to make variables</span>     left_list_length, right_list_length <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(left_list), <span style=""color: #008000"">len</span>(right_list)      <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(left_list_length <span style=""color: #666666"">+</span> right_list_length):         <span style=""color: #008000; font-weight: bold"">if</span> left_list_index <span style=""color: #666666"">&lt;</span> left_list_length <span style=""color: #AA22FF; font-weight: bold"">and</span> right_list_index <span style=""color: #666666"">&lt;</span> right_list_length:             <span style=""color: #408080; font-style: italic""># We check which value from the start of each list is smaller</span>             <span style=""color: #408080; font-style: italic""># If the item at the beginning of the left list is smaller, add it</span>             <span style=""color: #408080; font-style: italic""># to the sorted list</span>             <span style=""color: #008000; font-weight: bold"">if</span> left_list[left_list_index] <span style=""color: #666666"">&lt;=</span> right_list[right_list_index]:                 sorted_list<span style=""color: #666666"">.</span>append(left_list[left_list_index])                 left_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>             <span style=""color: #408080; font-style: italic""># If the item at the beginning of the right list is smaller, add it</span>             <span style=""color: #408080; font-style: italic""># to the sorted list</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 sorted_list<span style=""color: #666666"">.</span>append(right_list[right_list_index])                 right_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>          <span style=""color: #408080; font-style: italic""># If we've reached the end of the of the left list, add the elements</span>         <span style=""color: #408080; font-style: italic""># from the right list</span>         <span style=""color: #008000; font-weight: bold"">elif</span> left_list_index <span style=""color: #666666"">==</span> left_list_length:             sorted_list<span style=""color: #666666"">.</span>append(right_list[right_list_index])             right_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #408080; font-style: italic""># If we've reached the end of the of the right list, add the elements</span>         <span style=""color: #408080; font-style: italic""># from the left list</span>         <span style=""color: #008000; font-weight: bold"">elif</span> right_list_index <span style=""color: #666666"">==</span> right_list_length:             sorted_list<span style=""color: #666666"">.</span>append(left_list[left_list_index])             left_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>      <span style=""color: #008000; font-weight: bold"">return</span> sorted_list   <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">merge_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># If the list is a single element, return it</span>     <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">&lt;=</span> <span style=""color: #666666"">1</span>:         <span style=""color: #008000; font-weight: bold"">return</span> nums      <span style=""color: #408080; font-style: italic""># Use floor division to get midpoint, indices must be integers</span>     mid <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">//</span> <span style=""color: #666666"">2</span>      <span style=""color: #408080; font-style: italic""># Sort and merge each half</span>     left_list <span style=""color: #666666"">=</span> merge_sort(nums[:mid])     right_list <span style=""color: #666666"">=</span> merge_sort(nums[mid:])      <span style=""color: #408080; font-style: italic""># Merge the sorted lists into a new one</span>     <span style=""color: #008000; font-weight: bold"">return</span> merge(left_list, right_list) </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n \log n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Merge Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">merge</span>(left_list, right_list):     sorted_list <span style=""color: #666666"">=</span> []     left_list_index <span style=""color: #666666"">=</span> right_list_index <span style=""color: #666666"">=</span> <span style=""color: #666666"">0</span>      <span style=""color: #408080; font-style: italic""># We use the list lengths often, so its handy to make variables</span>     left_list_length, right_list_length <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(left_list), <span style=""color: #008000"">len</span>(right_list)      <span style=""color: #008000; font-weight: bold"">for</span> _ <span style=""color: #AA22FF; font-weight: bold"">in</span> <span style=""color: #008000"">range</span>(left_list_length <span style=""color: #666666"">+</span> right_list_length):         <span style=""color: #008000; font-weight: bold"">if</span> left_list_index <span style=""color: #666666"">&lt;</span> left_list_length <span style=""color: #AA22FF; font-weight: bold"">and</span> right_list_index <span style=""color: #666666"">&lt;</span> right_list_length:             <span style=""color: #408080; font-style: italic""># We check which value from the start of each list is smaller</span>             <span style=""color: #408080; font-style: italic""># If the item at the beginning of the left list is smaller, add it</span>             <span style=""color: #408080; font-style: italic""># to the sorted list</span>             <span style=""color: #008000; font-weight: bold"">if</span> left_list[left_list_index] <span style=""color: #666666"">&lt;=</span> right_list[right_list_index]:                 sorted_list<span style=""color: #666666"">.</span>append(left_list[left_list_index])                 left_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>             <span style=""color: #408080; font-style: italic""># If the item at the beginning of the right list is smaller, add it</span>             <span style=""color: #408080; font-style: italic""># to the sorted list</span>             <span style=""color: #008000; font-weight: bold"">else</span>:                 sorted_list<span style=""color: #666666"">.</span>append(right_list[right_list_index])                 right_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>          <span style=""color: #408080; font-style: italic""># If we've reached the end of the of the left list, add the elements</span>         <span style=""color: #408080; font-style: italic""># from the right list</span>         <span style=""color: #008000; font-weight: bold"">elif</span> left_list_index <span style=""color: #666666"">==</span> left_list_length:             sorted_list<span style=""color: #666666"">.</span>append(right_list[right_list_index])             right_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #408080; font-style: italic""># If we've reached the end of the of the right list, add the elements</span>         <span style=""color: #408080; font-style: italic""># from the left list</span>         <span style=""color: #008000; font-weight: bold"">elif</span> right_list_index <span style=""color: #666666"">==</span> right_list_length:             sorted_list<span style=""color: #666666"">.</span>append(left_list[left_list_index])             left_list_index <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>      <span style=""color: #008000; font-weight: bold"">return</span> sorted_list   <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">merge_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># If the list is a single element, return it</span>     <span style=""color: #008000; font-weight: bold"">if</span> <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">&lt;=</span> <span style=""color: #666666"">1</span>:         <span style=""color: #008000; font-weight: bold"">return</span> nums      <span style=""color: #408080; font-style: italic""># Use floor division to get midpoint, indices must be integers</span>     mid <span style=""color: #666666"">=</span> <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">//</span> <span style=""color: #666666"">2</span>      <span style=""color: #408080; font-style: italic""># Sort and merge each half</span>     left_list <span style=""color: #666666"">=</span> merge_sort(nums[:mid])     right_list <span style=""color: #666666"">=</span> merge_sort(nums[mid:])      <span style=""color: #408080; font-style: italic""># Merge the sorted lists into a new one</span>     <span style=""color: #008000; font-weight: bold"">return</span> merge(left_list, right_list) </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Quick Sort</div> Implement Quick Sort  <br/><br/> <div class=""example""></div>"	"Move elements to the left or to the right of a pivot recursively <br> <div class=""algo""><center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">partition</span>(nums, low, high):     <span style=""color: #408080; font-style: italic""># We select the middle element to be the pivot. Some implementations select</span>     <span style=""color: #408080; font-style: italic""># the first element or the last element. Sometimes the median value becomes</span>     <span style=""color: #408080; font-style: italic""># the pivot, or a random one. There are many more strategies that can be</span>     <span style=""color: #408080; font-style: italic""># chosen or created.</span>     pivot <span style=""color: #666666"">=</span> nums[(low <span style=""color: #666666"">+</span> high) <span style=""color: #666666"">//</span> <span style=""color: #666666"">2</span>]     i <span style=""color: #666666"">=</span> low <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>     j <span style=""color: #666666"">=</span> high <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>     <span style=""color: #008000; font-weight: bold"">while</span> <span style=""color: #008000; font-weight: bold"">True</span>:         i <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">while</span> nums[i] <span style=""color: #666666"">&lt;</span> pivot:             i <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>          j <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">while</span> nums[j] <span style=""color: #666666"">&gt;</span> pivot:             j <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>          <span style=""color: #008000; font-weight: bold"">if</span> i <span style=""color: #666666"">&gt;=</span> j:             <span style=""color: #008000; font-weight: bold"">return</span> j          <span style=""color: #408080; font-style: italic""># If an element at i (on the left of the pivot) is larger than the</span>         <span style=""color: #408080; font-style: italic""># element at j (on right right of the pivot), then swap them</span>         nums[i], nums[j] <span style=""color: #666666"">=</span> nums[j], nums[i]   <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">quick_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># Create a helper function that will be called recursively</span>     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">_quick_sort</span>(items, low, high):         <span style=""color: #008000; font-weight: bold"">if</span> low <span style=""color: #666666"">&lt;</span> high:             <span style=""color: #408080; font-style: italic""># This is the index after the pivot, where our lists are split</span>             split_index <span style=""color: #666666"">=</span> partition(items, low, high)             _quick_sort(items, low, split_index)             _quick_sort(items, split_index <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>, high)      _quick_sort(nums, <span style=""color: #666666"">0</span>, <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>) </pre></div> </td></tr></tbody></table></center><br></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Quick Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">partition</span>(nums, low, high):     <span style=""color: #408080; font-style: italic""># We select the middle element to be the pivot. Some implementations select</span>     <span style=""color: #408080; font-style: italic""># the first element or the last element. Sometimes the median value becomes</span>     <span style=""color: #408080; font-style: italic""># the pivot, or a random one. There are many more strategies that can be</span>     <span style=""color: #408080; font-style: italic""># chosen or created.</span>     pivot <span style=""color: #666666"">=</span> nums[(low <span style=""color: #666666"">+</span> high) <span style=""color: #666666"">//</span> <span style=""color: #666666"">2</span>]     i <span style=""color: #666666"">=</span> low <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>     j <span style=""color: #666666"">=</span> high <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>     <span style=""color: #008000; font-weight: bold"">while</span> <span style=""color: #008000; font-weight: bold"">True</span>:         i <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">while</span> nums[i] <span style=""color: #666666"">&lt;</span> pivot:             i <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>          j <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">while</span> nums[j] <span style=""color: #666666"">&gt;</span> pivot:             j <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>          <span style=""color: #008000; font-weight: bold"">if</span> i <span style=""color: #666666"">&gt;=</span> j:             <span style=""color: #008000; font-weight: bold"">return</span> j          <span style=""color: #408080; font-style: italic""># If an element at i (on the left of the pivot) is larger than the</span>         <span style=""color: #408080; font-style: italic""># element at j (on right right of the pivot), then swap them</span>         nums[i], nums[j] <span style=""color: #666666"">=</span> nums[j], nums[i]   <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">quick_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># Create a helper function that will be called recursively</span>     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">_quick_sort</span>(items, low, high):         <span style=""color: #008000; font-weight: bold"">if</span> low <span style=""color: #666666"">&lt;</span> high:             <span style=""color: #408080; font-style: italic""># This is the index after the pivot, where our lists are split</span>             split_index <span style=""color: #666666"">=</span> partition(items, low, high)             _quick_sort(items, low, split_index)             _quick_sort(items, split_index <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>, high)      _quick_sort(nums, <span style=""color: #666666"">0</span>, <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>) </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Time Complexity?</span>"	\(O(n \log n)\)
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline"">Quick Sort</div>  <center><table><tbody><tr><td><div class=""highlight"" style=""background: #f8f8f8""><pre style=""line-height: 125%""><span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">partition</span>(nums, low, high):     <span style=""color: #408080; font-style: italic""># We select the middle element to be the pivot. Some implementations select</span>     <span style=""color: #408080; font-style: italic""># the first element or the last element. Sometimes the median value becomes</span>     <span style=""color: #408080; font-style: italic""># the pivot, or a random one. There are many more strategies that can be</span>     <span style=""color: #408080; font-style: italic""># chosen or created.</span>     pivot <span style=""color: #666666"">=</span> nums[(low <span style=""color: #666666"">+</span> high) <span style=""color: #666666"">//</span> <span style=""color: #666666"">2</span>]     i <span style=""color: #666666"">=</span> low <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>     j <span style=""color: #666666"">=</span> high <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>     <span style=""color: #008000; font-weight: bold"">while</span> <span style=""color: #008000; font-weight: bold"">True</span>:         i <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">while</span> nums[i] <span style=""color: #666666"">&lt;</span> pivot:             i <span style=""color: #666666"">+=</span> <span style=""color: #666666"">1</span>          j <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>         <span style=""color: #008000; font-weight: bold"">while</span> nums[j] <span style=""color: #666666"">&gt;</span> pivot:             j <span style=""color: #666666"">-=</span> <span style=""color: #666666"">1</span>          <span style=""color: #008000; font-weight: bold"">if</span> i <span style=""color: #666666"">&gt;=</span> j:             <span style=""color: #008000; font-weight: bold"">return</span> j          <span style=""color: #408080; font-style: italic""># If an element at i (on the left of the pivot) is larger than the</span>         <span style=""color: #408080; font-style: italic""># element at j (on right right of the pivot), then swap them</span>         nums[i], nums[j] <span style=""color: #666666"">=</span> nums[j], nums[i]   <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">quick_sort</span>(nums):     <span style=""color: #408080; font-style: italic""># Create a helper function that will be called recursively</span>     <span style=""color: #008000; font-weight: bold"">def</span> <span style=""color: #0000FF"">_quick_sort</span>(items, low, high):         <span style=""color: #008000; font-weight: bold"">if</span> low <span style=""color: #666666"">&lt;</span> high:             <span style=""color: #408080; font-style: italic""># This is the index after the pivot, where our lists are split</span>             split_index <span style=""color: #666666"">=</span> partition(items, low, high)             _quick_sort(items, low, split_index)             _quick_sort(items, split_index <span style=""color: #666666"">+</span> <span style=""color: #666666"">1</span>, high)      _quick_sort(nums, <span style=""color: #666666"">0</span>, <span style=""color: #008000"">len</span>(nums) <span style=""color: #666666"">-</span> <span style=""color: #666666"">1</span>) </pre></div> </td></tr></tbody></table></center><br>  <br> <span class=""bold"">Space Complexity?</span>"	\(O(1)\)
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What does the acronym&nbsp;<b>ICA </b>mean?"	"<b>I</b>ndependent <b>C</b>omponent <b>A</b>nalysis  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the first <b>moment </b>in stats?"	"Mean  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the second&nbsp;<b>moment </b>in stats?"	"Variance  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the third&nbsp;<b>moment </b>in stats?"	"Skewness  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the fourth&nbsp;<b>moment </b>in stats?"	"Kurtosis  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which moment is the <b>mean</b>?"	"first  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which moment is the <b>variance</b>?"	"second  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which moment is the <b>skewness</b>?"	"third  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Which moment is the <b>kurtosis</b>?"	"fourth  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>skewness</b>?"	"Measure of the <b>asymmetry </b>of a distribution  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-f7f6d12e48c852a2291b15629ef9699875fb33ee.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is positive&nbsp;<b>skewness</b>?"	"When <b>tail in the right side</b> of the distribution  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-f7f6d12e48c852a2291b15629ef9699875fb33ee.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is negative&nbsp;<b>skewness</b>?"	"When <b>tail in the left side</b> of the distribution  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-f7f6d12e48c852a2291b15629ef9699875fb33ee.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the formula of the <b>skewness</b>?"	"<img src=""paste-57866b5fdc407befad944787c8cf24346ad3dbb3.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the formula of the <b>kurtosis</b>?"	"<img src=""paste-aa7ef1428ab2506306f518db44626a80cf2b1497.jpg"">  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>kurtosis</b>?"	"A measure of the <b>size of the tails </b>of a distribution  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-3ecff2a402eac27e339e890f71b6071876815e1e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>kurtosis </b>of a normal distribution?"	"3  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-9a4303242eecba70c4396637a9417868a582620a.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the difference between the <b>variance </b>and the <b>kurtosis</b>?"	"<span style=""background-color: rgba(0, 0, 0, 0);""><b>Variance</b>  describes the <b>scale</b> of the distribution.</span><div><span style=""background-color: rgba(0, 0, 0, 0);""><br></span></div><div><span style=""background-color: rgba(0, 0, 0, 0);""><b>Kurtosis</b> describes the  <b>spread</b> of the distribution in a scale-independent manner.</span><br></div>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>excess Kurtosis </b>meaning?"	"How much more <b>kurtosis </b>there is than a normal distribution  <div class=""details1"">Kurtosis of a normal distribution is 3</div>  <div class=""details2""></div>   <img src=""paste-9a4303242eecba70c4396637a9417868a582620a.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>excess Kurtosis </b>formula?"	"<img src=""paste-a3dfe302a19f288f874ab0e030c2de53a1c6eab5.jpg"">  <div class=""details1"">Kurtosis of a normal distribution is 3</div>  <div class=""details2""></div>   <img src=""paste-9a4303242eecba70c4396637a9417868a582620a.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a positive <b>excess kurtosis</b>?"	"Distribution <i>spikier&nbsp;</i>than a <b>gaussian</b>  <div class=""details1"">Remember that excess kurtosis is kurtosis - 3</div>  <div class=""details2""></div>   <img src=""paste-3ecff2a402eac27e339e890f71b6071876815e1e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is a negative&nbsp;<b>excess kurtosis</b>?"	"Distribution <i>flatter&nbsp;</i>than a <b>gaussian</b>  <div class=""details1"">Remember that excess kurtosis is kurtosis - 3</div>  <div class=""details2""></div>   <img src=""paste-3ecff2a402eac27e339e890f71b6071876815e1e.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How can we measure the <b>non-gaussianity </b>of a distribution with kurtosis?"	"With the <u>absolute value</u>&nbsp;of a <b>excess kurtosis</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-459889181fcbbda0fdf666e4119ecf64664b04af.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the<b> principle of maximum entropy</b>?"	"The probability distribution that <b>best represents the current knowledge</b> is the one with maximum entropy  <div class=""details1"">With no information at all, use uniform distribution</div>  <div class=""details2""></div>   <img src=""paste-5c14b02deacfd8d5c6bfc2b94a3a9f765b8abcaf.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why do we take the distribution with <b>maximum entropy </b>when we have few information about the real distribution?"	"It's the distribution that <b>generalize</b> the best given the <b>few given constraints</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  If we have no information about a real distribution, given the <b>maximum entropy </b>principle, which distribution should we choose?"	"Uniform  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  If we have only the <b>mean</b> about a real distribution, given the <b>maximum entropy </b>principle, which distribution should we choose?"	"Exponential  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-1ffe0aab1d8068ee9ac0f93970773300c1e94229.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  If we have only the <b>mean </b>and <b>variance</b>&nbsp;about a real distribution, given the <b>maximum entropy </b>principle, which distribution should we choose?"	"Gaussian  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-cc542907b7c8ad7db7e7d26f33f9a240190a84a3.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How does the principle of <b>max entropy </b>can be linked to <b>Occam's razor</b>?"	"They both choose the <b>easiest&nbsp;</b>solution, i.e. with <b>minimal assumption</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>negentropy </b>formula?"	"The <u>difference between the entropy</u> of a <b>Gaussian </b>and the given distribution  <div class=""details1"">Gaussian is the distribution with maximum entropy, so negentropy \(\ge 0\)</div>  <div class=""details2""></div>   <img src=""paste-3100ec64b0e1dd3cc9c6cbe5dcff09e7d8a2740c.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the usage of <b>negentropy</b>?"	"To measure the distance to <b>normality</b>  <div class=""details1"">Normality is defined by a Gaussian</div>  <div class=""details2"">The distance is on the resp. distributions entropies</div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <b>string interning </b>in Python?"	"A cache storing strings with equal value in the same data location  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-92d6a4e59f7f382716030a204832702a1b40fa0c.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> When does a call to a function can increase <b>space complexity</b>?"	"When doing <b>recursive calls</b>  <br/><br/> <span style=""font-size: 12px"">Each call is put on top of the <u>stack</u></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <b>hiearchical clustering</b>?"	"A method to cluster several clusters  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-d089b8301b85430a3c828ded8b9ae8592b133a1d.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What are the two main types of&nbsp;<b>hiearchical clustering</b>?"	"Agglomerative and Divisive  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-d089b8301b85430a3c828ded8b9ae8592b133a1d.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <u>agglomerative</u>&nbsp;<b>hiearchical clustering</b>?"	"<b>Start from clusters of one sample</b> each and then merge clusters recursively  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-d089b8301b85430a3c828ded8b9ae8592b133a1d.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <u>divisive</u>&nbsp;<b>hiearchical clustering</b>?"	"<b>Start from a single big cluster</b> that is then break into smaller clusters  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-d089b8301b85430a3c828ded8b9ae8592b133a1d.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What kind of <b>hiearchical clustering</b>&nbsp;do we refer to when saying ""<u>bottom-up</u>"""	"Agglomerative  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-d089b8301b85430a3c828ded8b9ae8592b133a1d.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What kind of <b>hiearchical clustering</b>&nbsp;do we refer to when saying ""<u>top-down</u>"""	"Divisive  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/> <img src=""paste-d089b8301b85430a3c828ded8b9ae8592b133a1d.jpg""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What is <b>single-linkage clustering</b>?"	"Merging the closest clusters together  <br/><br/> <span style=""font-size: 12px"">It's an <u>agglomerative</u>&nbsp;(bottom-up) method</span> <br/><br/> <img src=""clustering_animation_01.gif""> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> K-Means is intended for <b>convex </b>or non-convex data?"	"Convex  <br/><br/> <span style=""font-size: 12px""></span> <br/><br/>  "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> What trick can we do, to do K-means on non-convex data?<div><img src=""paste-9ed7147bd293509f7238434ded1b49a4ba7ea231.jpg""><br></div>"	"1. K-means data with a lot of clusters<div>2. Merge closest clusters recursively</div>  <br/><br/> <span style=""font-size: 12px"">Merging is done with <u>single-linkage</u></span> <br/><br/> <div><img src=""paste-5bf6678ea8efd7c5297bf9ad5fc7800d52b3d3ba.jpg""><br></div><div><img src=""clustering_animation_01.gif""><br></div> "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\frac{\left(\frac{a}{b}\right)}{c}=\) ..."	"\(\frac{\left(\frac{a}{b}\right)}{c}=\frac{a}{b c}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\frac{a}{\left(\frac{b}{c}\right)}=\) ..."	"\(\frac{a}{\left(\frac{b}{c}\right)}=\frac{a c}{b}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\log_b b = \)..."	"\(\log_b b = 1\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\log_b 1 = \)..."	"\(\log_b 1 = 0\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\log \frac{x}{y} = \)..."	"\(\log \frac{x}{y} = \log x - \log y\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\log xy = \)..."	"\(\log xy = \log x + \log y\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(\log x^y = \)..."	"\(\log x^y = y \log x\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the value of the Neperien \(e\)?"	"\(e = \mathbf{2.71}8281828...\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  \(y = \log_b x \iff x = \)...<br>"	"\(y = \log_b x \iff x = b^y\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>\(x = b^y \iff y = \)...<br></div>"	"\(x = b^y \iff y = \log_b x\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  <div>What is the definition of the <b>derivative</b>?<br></div>"	"\(f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}\)  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>smoothness assumption</b>?"	"Two samples <b>close to each other</b> are likely to have the <b>same labels</b>  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is the <b>cluster assumption</b>?"	"Each class is separated from the others by <b>low density regions </b>where the decision surfaces lie  <div class=""details1""></div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How is the loss landscape when <b>gradient explosion </b>usually happen?"	"There is a <b>cliff</b>  <div class=""details1"">A normal update can lead to a high error</div>  <div class=""details2""></div>   <img src=""paste-8fe9a837538db54bbec7cbad38d2f7bc5db0faee.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  When is <b>gradient clipping </b>useful?"	"When there are <b>gradient explosion</b>  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-8fe9a837538db54bbec7cbad38d2f7bc5db0faee.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What is <b>gradient clipping</b>?"	"Bound element-wise the values of the gradient between \([-x, +x]\)  <div class=""details1""></div>  <div class=""details2""></div>   <img src=""paste-8fe9a837538db54bbec7cbad38d2f7bc5db0faee.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  Why <b>gradient clipping </b>can produce to incorrect gradients?"	"Because the clipping is done element-wise so the <b>gradient direction can change</b>  <div class=""details1"">If clipping first dimension but not second, the direction has changed</div>  <div class=""details2""></div>   <img src=""paste-8fe9a837538db54bbec7cbad38d2f7bc5db0faee.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What kind of <b>incorrect gradients </b>can be produced by <b>gradient clipping</b>?"	"Gradients with <b>different directions</b>  <div class=""details1"">If clipping first dimension but not second, the direction has changed</div>  <div class=""details2""></div>   <img src=""paste-8fe9a837538db54bbec7cbad38d2f7bc5db0faee.jpg"">"
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  What alternative to <b>gradient clipping </b>is there, that don't alter the gradient direction?"	"Gradient <b>norm clipping</b>  <div class=""details1"">\(\text{if}\,\, \Vert g \Vert &gt; \nu,\,\, g \leftarrow \frac{g\nu}{\Vert g \Vert}\)</div>  <div class=""details2""></div>   "
" <h5>🤖 Deep Learning</h5>  <div class=""subtitle""></div>  How to do <b>gradient norm clipping</b>?"	"\(\text{if}\,\, \Vert g \Vert &gt; \nu,\,\, g \leftarrow \frac{g\nu}{\Vert g \Vert}\)<br>  <div class=""details1""></div>  <div class=""details2""></div>   "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div> Max triplet sum<div><img class=latex src=""latex-96f06b16097590fa80f50989e796c7bc95c3430c.png""><br></div>  <br/><br/> <div class=""example""><div><span style=""color: rgb(212, 212, 212);"">- Given \(</span><span style=""color: rgb(78, 201, 176);"">I = </span><span style=""color: rgb(215, 186, 125);"">\{</span><span style=""color: rgb(181, 206, 168);"">3</span><span style=""color: rgb(78, 201, 176);"">,</span><span style=""color: rgb(181, 206, 168);"">2</span><span style=""color: rgb(78, 201, 176);"">,</span><span style=""color: rgb(181, 206, 168);"">1</span><span style=""color: rgb(215, 186, 125);"">\}</span><font color=""#4ec9b0"">\)</font><span style=""color: rgb(212, 212, 212);"">&nbsp;the function returns </span><span style=""color: rgb(78, 201, 176);"">$</span><span style=""color: rgb(78, 201, 176);"">-</span><span style=""color: rgb(181, 206, 168);"">1</span><span style=""color: rgb(78, 201, 176);"">\)</span><span style=""color: rgb(212, 212, 212);"">&nbsp;as there is no valid triplet in&nbsp;</span><span style=""color: rgb(212, 212, 212);"">\(</span><span style=""color: rgb(78, 201, 176);"">I</span><span style=""color: rgb(78, 201, 176);"">\)</span><br></div><div><span style=""color: #d4d4d4;"">- Given \(</span><span style=""color: #4ec9b0;"">I = </span><span style=""color: #d7ba7d;"">\{</span><span style=""color: #b5cea8;"">1</span><span style=""color: #4ec9b0;"">,</span><span style=""color: #b5cea8;"">3</span><span style=""color: #4ec9b0;"">,</span><span style=""color: #b5cea8;"">2</span><span style=""color: #d7ba7d;"">\}</span><span style=""color: rgb(78, 201, 176);"">\)</span><span style=""color: rgb(212, 212, 212);"">&nbsp;the function returns </span><span style=""color: rgb(78, 201, 176);"">$</span><span style=""color: rgb(78, 201, 176);"">-</span><span style=""color: rgb(181, 206, 168);"">1</span><span style=""color: rgb(78, 201, 176);"">\)</span><span style=""color: rgb(212, 212, 212);"">&nbsp;as there is no valid triplet in&nbsp;</span><span style=""color: rgb(212, 212, 212);"">\(</span><span style=""color: rgb(78, 201, 176);"">I</span><span style=""color: rgb(78, 201, 176);"">\)</span><span style=""color: rgb(212, 212, 212);"">.</span></div><div><div><div><span style=""color: #d4d4d4;"">- Given&nbsp;</span>\(<span style=""color: #4ec9b0;"">I = </span><span style=""color: #d7ba7d;"">\{</span><span style=""color: #b5cea8;"">1</span><span style=""color: #4ec9b0;"">,</span><span style=""color: #b5cea8;"">2</span><span style=""color: #4ec9b0;"">,</span><span style=""color: #b5cea8;"">3</span><span style=""color: #d7ba7d;"">\}</span><span style=""color: #d4d4d4;"">&nbsp;</span>\)<span style=""color: #d4d4d4;"">the function returns </span><font color=""#4ec9b0"">\(</font><span style=""color: #b5cea8;"">6</span><font color=""#4ec9b0"">\)</font><span style=""color: #d4d4d4;"">. There is only one valid triplet in \(</span><span style=""color: #4ec9b0;"">I\)</span><span style=""color: #d4d4d4;"">.</span></div></div></div><div><br></div></div>"	"skdjfhskdjfskdf<div>sdkljfhskdjfhsd</div><div>skdjhfksjdhfksjdhfksjdhfks</div> <br> <div class=""algo""><pre>template <span style=""color: rgb(48, 128, 128);"">&lt;</span>class T<span style=""color: rgb(48, 128, 128);"">&gt;</span>  class Node <span style=""color: rgb(64, 96, 128);"">{</span> <span style=""color: rgb(227, 74, 220);"">&nbsp;&nbsp;&nbsp;&nbsp;public:</span>   T val<span style=""color: rgb(64, 96, 128);"">;</span>   Node <span style=""color: rgb(48, 128, 128);"">*</span>next<span style=""color: rgb(64, 96, 128);"">;</span>   Node <span style=""color: rgb(48, 128, 128);"">*</span>random<span style=""color: rgb(64, 96, 128);"">;</span>    Node<span style=""color: rgb(48, 128, 128);"">(</span><span style=""color: rgb(32, 0, 128); font-weight: bold;"">const</span> T <span style=""color: rgb(48, 128, 128);"">&amp;</span>_val<span style=""color: rgb(48, 128, 128);"">)</span>   <span style=""color: rgb(64, 96, 128);"">{</span>     val    <span style=""color: rgb(48, 128, 128);"">=</span> _val<span style=""color: rgb(64, 96, 128);"">;</span>     next   <span style=""color: rgb(48, 128, 128);"">=</span> nullptr<span style=""color: rgb(64, 96, 128);"">;</span>     random <span style=""color: rgb(48, 128, 128);"">=</span> nullptr<span style=""color: rgb(64, 96, 128);"">;</span>   <span style=""color: rgb(64, 96, 128);"">}</span> <span style=""color: rgb(64, 96, 128);"">}</span><span style=""color: rgb(64, 96, 128);"">;</span></pre></div> "
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  <pre>template <span style=""color: rgb(48, 128, 128);"">&lt;</span>class T<span style=""color: rgb(48, 128, 128);"">&gt;</span>  class Node <span style=""color: rgb(64, 96, 128);"">{</span> <span style=""color: rgb(227, 74, 220);"">&nbsp;&nbsp;&nbsp;&nbsp;public:</span>   T val<span style=""color: rgb(64, 96, 128);"">;</span>   Node <span style=""color: rgb(48, 128, 128);"">*</span>next<span style=""color: rgb(64, 96, 128);"">;</span>   Node <span style=""color: rgb(48, 128, 128);"">*</span>random<span style=""color: rgb(64, 96, 128);"">;</span>    Node<span style=""color: rgb(48, 128, 128);"">(</span><span style=""color: rgb(32, 0, 128); font-weight: bold;"">const</span> T <span style=""color: rgb(48, 128, 128);"">&amp;</span>_val<span style=""color: rgb(48, 128, 128);"">)</span>   <span style=""color: rgb(64, 96, 128);"">{</span>     val    <span style=""color: rgb(48, 128, 128);"">=</span> _val<span style=""color: rgb(64, 96, 128);"">;</span>     next   <span style=""color: rgb(48, 128, 128);"">=</span> nullptr<span style=""color: rgb(64, 96, 128);"">;</span>     random <span style=""color: rgb(48, 128, 128);"">=</span> nullptr<span style=""color: rgb(64, 96, 128);"">;</span>   <span style=""color: rgb(64, 96, 128);"">}</span> <span style=""color: rgb(64, 96, 128);"">}</span><span style=""color: rgb(64, 96, 128);"">;</span></pre>  <br> <span class=""bold"">Time Complexity?</span>"	"\(<span style=""color: rgb(78, 201, 176);"">I =&nbsp;</span><span style=""color: rgb(215, 186, 125);"">\{</span><span style=""color: rgb(181, 206, 168);"">1</span><span style=""color: rgb(78, 201, 176);"">,</span><span style=""color: rgb(181, 206, 168);"">2</span><span style=""color: rgb(78, 201, 176);"">,</span><span style=""color: rgb(181, 206, 168);"">3</span><span style=""color: rgb(215, 186, 125);"">\}</span><font color=""#4ec9b0"">\)</font>"
"<h5>💻 Computer Science</h5>  <div style=""text-decoration: underline""></div>  <pre>template <span style=""color: rgb(48, 128, 128);"">&lt;</span>class T<span style=""color: rgb(48, 128, 128);"">&gt;</span>  class Node <span style=""color: rgb(64, 96, 128);"">{</span> <span style=""color: rgb(227, 74, 220);"">&nbsp;&nbsp;&nbsp;&nbsp;public:</span>   T val<span style=""color: rgb(64, 96, 128);"">;</span>   Node <span style=""color: rgb(48, 128, 128);"">*</span>next<span style=""color: rgb(64, 96, 128);"">;</span>   Node <span style=""color: rgb(48, 128, 128);"">*</span>random<span style=""color: rgb(64, 96, 128);"">;</span>    Node<span style=""color: rgb(48, 128, 128);"">(</span><span style=""color: rgb(32, 0, 128); font-weight: bold;"">const</span> T <span style=""color: rgb(48, 128, 128);"">&amp;</span>_val<span style=""color: rgb(48, 128, 128);"">)</span>   <span style=""color: rgb(64, 96, 128);"">{</span>     val    <span style=""color: rgb(48, 128, 128);"">=</span> _val<span style=""color: rgb(64, 96, 128);"">;</span>     next   <span style=""color: rgb(48, 128, 128);"">=</span> nullptr<span style=""color: rgb(64, 96, 128);"">;</span>     random <span style=""color: rgb(48, 128, 128);"">=</span> nullptr<span style=""color: rgb(64, 96, 128);"">;</span>   <span style=""color: rgb(64, 96, 128);"">}</span> <span style=""color: rgb(64, 96, 128);"">}</span><span style=""color: rgb(64, 96, 128);"">;</span></pre>  <br> <span class=""bold"">Space Complexity?</span>"	"<span style=""color: rgb(212, 212, 212);"">(</span><span style=""color: rgb(78, 201, 176);"">I =&nbsp;</span><span style=""color: rgb(215, 186, 125);"">\{</span><span style=""color: rgb(181, 206, 168);"">1</span><span style=""color: rgb(78, 201, 176);"">,</span><span style=""color: rgb(181, 206, 168);"">2</span><span style=""color: rgb(78, 201, 176);"">,dfsdffdf</span><span style=""color: rgb(181, 206, 168);"">3</span><span style=""color: rgb(215, 186, 125);"">\}</span><font color=""#4ec9b0"">\)</font>"
